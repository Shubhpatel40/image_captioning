{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhpatel40/image_captioning/blob/main/image_captioning_using_custom_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwRVImNojLdI",
        "outputId": "52faabfc-14b9-4d22-f1c3-5c1f08198793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/flickr8k\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.6.0.163-1+cuda11.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xco_n10jgHF",
        "outputId": "a98ad34b-4168-439d-feb0-e4a369b3eb2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcudnn8\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 446 MB of archives.\n",
            "After this operation, 1,140 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8 8.6.0.163-1+cuda11.8 [446 MB]\n",
            "Fetched 446 MB in 6s (80.0 MB/s)\n",
            "Selecting previously unselected package libcudnn8.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.6.0.163-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.6.0.163-1+cuda11.8) ...\n",
            "Setting up libcudnn8 (8.6.0.163-1+cuda11.8) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow estimator keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "822h4FEJjmv0",
        "outputId": "2152d415-59ca-4787-f61f-b6ec0e5dd056"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[33mWARNING: Skipping estimator as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: keras 3.8.0\n",
            "Uninstalling keras-3.8.0:\n",
            "  Successfully uninstalled keras-3.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjpGG9vljpRZ",
        "outputId": "64524a18-142f-4499-87f8-cdae5fc722a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.11/dist-packages (2.18.1)\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.11/dist-packages (4.9.9)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.7.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (1.17.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.67.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.23.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_datasets) (25.3.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow_datasets) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.70.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, keras, tensorflow, tensorflow_text\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow_text\n",
            "    Found existing installation: tensorflow-text 2.18.1\n",
            "    Uninstalling tensorflow-text-2.18.1:\n",
            "      Successfully uninstalled tensorflow-text-2.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.10.0 ml-dtypes-0.5.1 tensorboard-2.19.0 tensorflow-2.19.0 tensorflow_text-2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZPU53bEjs6I",
        "outputId": "2fe6e814-f7bb-46a8-e221-b491df3a48c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import csv\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "zsSYUDkEsUD8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions_file = path + \"/captions.txt\"\n",
        "\n",
        "with open(captions_file, \"r\") as f:\n",
        "    captions = f.read().splitlines()"
      ],
      "metadata": {
        "id": "jozNHPT1js3i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset_dict = {}\n",
        "\n",
        "for caption in captions[1:-1]:\n",
        "    reader = csv.reader([caption])\n",
        "    image, caption = tuple(next(reader))\n",
        "    if image not in full_dataset_dict:\n",
        "        full_dataset_dict[image] = []\n",
        "    full_dataset_dict[image].append(caption)"
      ],
      "metadata": {
        "id": "AW_yMohekOSV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Load the model***"
      ],
      "metadata": {
        "id": "jpB7h87JsjIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (224, 224, 3)\n",
        "restnet = tf.keras.applications.ResNet50V2(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False)\n",
        "restnet.trainable=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8BaN6zNkQE-",
        "outputId": "5806309c-74c8-44c6-c6d4-baed77da05ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94668760/94668760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***function to load the model and resize for the model***"
      ],
      "metadata": {
        "id": "aFxQS6TutGb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    full_image_path = f\"{path}/Images/{image_path}\"\n",
        "    img = tf.io.read_file(full_image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img"
      ],
      "metadata": {
        "id": "oVWYCk1tmYA1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.newaxis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9APSXyquM9q",
        "outputId": "1d0c129c-2921-4fbb-d2ac-83963d7573da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.exists(f\"{path}/Images/1000268201_693b08cb0e.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhvTJGY8urtS",
        "outputId": "0ea57449-39e1-48c9-b039-f55f38e6de72"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_batch = load_image(list(full_dataset_dict.keys())[0])[tf.newaxis, :]\n",
        "\n",
        "print(test_img_batch.shape)\n",
        "print(restnet(test_img_batch).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQmmnZtFs7C2",
        "outputId": "f3ad4a75-9594-4f6d-da89-e86e071a81e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 224, 224, 3)\n",
            "(1, 7, 7, 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Preprocess Captions***"
      ],
      "metadata": {
        "id": "F6MEWebdvP7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Function for the remove punctuation, lower case, and add the special token before and after the captions.***"
      ],
      "metadata": {
        "id": "XoqoPCvavT8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "class preprocess_text:\n",
        "   def __init__(self):\n",
        "      pass\n",
        "\n",
        "   def standardize(self, s:List):\n",
        "      s = tf.constant(s, dtype=tf.string)\n",
        "      s = tf.strings.lower(s)\n",
        "      s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
        "      byte_arr = s.numpy()\n",
        "\n",
        "      # Decode each element safely\n",
        "      decoded = [\n",
        "          b.decode('utf-8') if isinstance(b, (bytes, bytearray)) else str(b)\n",
        "          for b in byte_arr\n",
        "      ]\n",
        "\n",
        "      return [f\"{t} [END]\" for t in decoded]\n",
        "\n",
        "   def make_vocabulary_dict(self, full_dataset_dict):\n",
        "      vocabulary_size = 1\n",
        "      vocab_dict = {'[START]': 0}\n",
        "      for image in full_dataset_dict:\n",
        "          for caption in full_dataset_dict[image]:\n",
        "              for word in caption.split():\n",
        "                  if word not in vocab_dict:\n",
        "                      vocab_dict[word] = vocabulary_size\n",
        "                      vocabulary_size += 1\n",
        "                  else:\n",
        "                      continue\n",
        "\n",
        "      return vocab_dict\n",
        "\n",
        "   def index_to_word(self, vocab_dict):\n",
        "      index_to_word = {index: word for word, index in vocab_dict.items()}\n",
        "      return index_to_word\n",
        "\n",
        "   def word_to_index(self, s):\n",
        "      return [vocab_dict[word] for word in s.split()]"
      ],
      "metadata": {
        "id": "XsuhGDJswti4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tokens_to_tensor:\n",
        "    def __init__(self, vocab_dict, vocabulary_size, max_len=50):\n",
        "      self.vocab_dict = vocab_dict\n",
        "      self.vocabulary_size = vocabulary_size\n",
        "      self.max_len = max_len\n",
        "      self.preprocess_text = preprocess_text()\n",
        "\n",
        "    def preprocessing(self, caption_batch):\n",
        "      all_captions_tokenIDs = []\n",
        "\n",
        "      for s_tokenID in caption_batch:\n",
        "        ## split the caption\n",
        "        s_tokenID = []\n",
        "\n",
        "        ## find the token ID for the each token from vocab_dict.\n",
        "        # for word in caption:\n",
        "        #   if word in self.vocab_dict:\n",
        "        #     s_tokenID.append(self.vocab_dict[word])\n",
        "\n",
        "        #   else:\n",
        "        #     s_tokenID.append(0)\n",
        "\n",
        "        ## padding the length\n",
        "        for _ in range(0, self.max_len-len(s_tokenID)):\n",
        "          s_tokenID.append(0)\n",
        "\n",
        "        all_captions_tokenIDs.append(s_tokenID)\n",
        "\n",
        "      tgt_ids = torch.tensor(all_captions_tokenIDs, dtype=torch.long)\n",
        "      return tgt_ids"
      ],
      "metadata": {
        "id": "FMT3Bk0RwcoC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_preprocess = preprocess_text(vocab_dict, vocabulary_size)\n",
        "# standardize([\"a cat on the sat, and i would like to tell with her.\"])"
      ],
      "metadata": {
        "id": "NwsuBWTuxSRb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temp_captions = list(standardize(full_dataset_dict[\"2083434441_a93bc6306b.jpg\"]))\n",
        "# print(temp_captions)"
      ],
      "metadata": {
        "id": "XEzLuhijvwdl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing = preprocess_text()"
      ],
      "metadata": {
        "id": "hmRflgXs80wD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in full_dataset_dict:\n",
        "    full_dataset_dict[image] = preprocessing.standardize(full_dataset_dict[image])"
      ],
      "metadata": {
        "id": "6tM46m664U6f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset_dict[\"2083434441_a93bc6306b.jpg\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKO7fU6P6yc1",
        "outputId": "3ba1879e-22b5-47bc-ba66-be9b2f057a95"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an elderly woman is riding a bicycle in the city as a yellow taxi is about to pass by  [END]',\n",
              " 'an elderly woman rides a bicycle along a city street  [END]',\n",
              " 'an older woman with blond hair rides a bicycle down the street  [END]',\n",
              " 'a woman in a grey overcoat rides her bicycle along a street  [END]',\n",
              " 'older woman wearing glassses riding a bicycle with a shopping bag on the handle  yellow car is in the background  [END]']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make the vocabulary size"
      ],
      "metadata": {
        "id": "kyqRrsS36nfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict = preprocessing.make_vocabulary_dict(full_dataset_dict)"
      ],
      "metadata": {
        "id": "SdeygPRt8KOW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Convert the dataset into the index***"
      ],
      "metadata": {
        "id": "7MS8OzrNFQW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = {}\n",
        "for image in full_dataset_dict:\n",
        "    for caption in full_dataset_dict[image]:\n",
        "        full_dataset[caption] = image"
      ],
      "metadata": {
        "id": "Zhe5clWL9E6g"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'length of the full dataset each caption with image: {len(list(full_dataset.keys()))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GDqKHeL-BU5",
        "outputId": "5b2c88da-fd0a-44d9-eff2-1263d75e18c2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the full dataset each caption with image: 40168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Make the index to word ***"
      ],
      "metadata": {
        "id": "65yG_z8vFZsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = preprocessing.index_to_word(vocab_dict)"
      ],
      "metadata": {
        "id": "WvvBDmzNFBqC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Make the dataframe from the dictionary***"
      ],
      "metadata": {
        "id": "bZK0OaQx-RR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(full_dataset, orient='index', columns=['image'])\n",
        "df = df.reset_index().rename(columns={'index': 'caption'})\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "TD4Fr4Xy-Qyt",
        "outputId": "4a5e2443-5217-4cc7-eee2-a9a2cce151d9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 caption  \\\n",
              "0      a child in a pink dress is climbing up a set o...   \n",
              "1             a girl going into a wooden building  [END]   \n",
              "2      a little girl climbing into a wooden playhouse...   \n",
              "3      a little girl climbing the stairs to her playh...   \n",
              "4      a little girl in a pink dress going into a woo...   \n",
              "...                                                  ...   \n",
              "40163  woman writing on a pad in room with gold  deco...   \n",
              "40164     a man in a pink shirt climbs a rock face [END]   \n",
              "40165      a man is rock climbing high in the air  [END]   \n",
              "40166  a person in a red shirt climbing up a rock fac...   \n",
              "40167               a rock climber in a red shirt  [END]   \n",
              "\n",
              "                           image  \n",
              "0      1000268201_693b08cb0e.jpg  \n",
              "1      1000268201_693b08cb0e.jpg  \n",
              "2      1000268201_693b08cb0e.jpg  \n",
              "3      1000268201_693b08cb0e.jpg  \n",
              "4      1000268201_693b08cb0e.jpg  \n",
              "...                          ...  \n",
              "40163   997338199_7343367d7f.jpg  \n",
              "40164   997722733_0cb5439472.jpg  \n",
              "40165   997722733_0cb5439472.jpg  \n",
              "40166   997722733_0cb5439472.jpg  \n",
              "40167   997722733_0cb5439472.jpg  \n",
              "\n",
              "[40168 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1bfe9168-836d-4648-be3c-bab160b59a11\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a child in a pink dress is climbing up a set o...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a girl going into a wooden building  [END]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a little girl climbing into a wooden playhouse...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a little girl climbing the stairs to her playh...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a little girl in a pink dress going into a woo...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40163</th>\n",
              "      <td>woman writing on a pad in room with gold  deco...</td>\n",
              "      <td>997338199_7343367d7f.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40164</th>\n",
              "      <td>a man in a pink shirt climbs a rock face [END]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40165</th>\n",
              "      <td>a man is rock climbing high in the air  [END]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40166</th>\n",
              "      <td>a person in a red shirt climbing up a rock fac...</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40167</th>\n",
              "      <td>a rock climber in a red shirt  [END]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40168 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1bfe9168-836d-4648-be3c-bab160b59a11')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1bfe9168-836d-4648-be3c-bab160b59a11 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1bfe9168-836d-4648-be3c-bab160b59a11');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-728492db-0fca-4203-ab64-4c17aba4346d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-728492db-0fca-4203-ab64-4c17aba4346d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-728492db-0fca-4203-ab64-4c17aba4346d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_cc3daa08-e892-47b8-b88a-6c5d64b0bcdd\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_cc3daa08-e892-47b8-b88a-6c5d64b0bcdd button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 40168,\n  \"fields\": [\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40168,\n        \"samples\": [\n          \"a boat with two fishermen sits peacefully on the lake  [END]\",\n          \"a black and white dog jumps to get the frisbee  [END]\",\n          \"a dog runs on the beach with a red dog toy with a yellow rope attached  [END]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3115174046_9e96b9ce47.jpg\",\n          \"3107592525_0bcd00777e.jpg\",\n          \"2426724282_237bca30b5.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['caption'] = df['caption'].apply(lambda x: x.replace(\"  \", \" \"))\n",
        "df['caption'] = df['caption'].apply(lambda x: preprocessing.word_to_index(x))"
      ],
      "metadata": {
        "id": "0-rOY_r9-ioG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "zm0m7gon-9Dz",
        "outputId": "4e5bcd5a-0423-4c26-877c-4eedf32ea4cd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 caption  \\\n",
              "0      [1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 1...   \n",
              "1                         [1, 16, 17, 18, 1, 19, 20, 15]   \n",
              "2                      [1, 21, 16, 7, 18, 1, 19, 22, 15]   \n",
              "3                 [1, 21, 16, 7, 23, 11, 24, 25, 22, 15]   \n",
              "4         [1, 21, 16, 3, 1, 4, 5, 17, 18, 1, 19, 26, 15]   \n",
              "...                                                  ...   \n",
              "40163  [234, 1258, 38, 1, 4025, 3, 561, 35, 1202, 306...   \n",
              "40164        [1, 75, 3, 1, 4, 161, 112, 1, 164, 202, 15]   \n",
              "40165            [1, 75, 6, 164, 7, 488, 3, 23, 227, 15]   \n",
              "40166  [1, 187, 3, 1, 110, 161, 7, 8, 1, 164, 202, 54...   \n",
              "40167                  [1, 164, 288, 3, 1, 110, 161, 15]   \n",
              "\n",
              "                           image  \n",
              "0      1000268201_693b08cb0e.jpg  \n",
              "1      1000268201_693b08cb0e.jpg  \n",
              "2      1000268201_693b08cb0e.jpg  \n",
              "3      1000268201_693b08cb0e.jpg  \n",
              "4      1000268201_693b08cb0e.jpg  \n",
              "...                          ...  \n",
              "40163   997338199_7343367d7f.jpg  \n",
              "40164   997722733_0cb5439472.jpg  \n",
              "40165   997722733_0cb5439472.jpg  \n",
              "40166   997722733_0cb5439472.jpg  \n",
              "40167   997722733_0cb5439472.jpg  \n",
              "\n",
              "[40168 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69974d2f-c79f-4b25-aba4-979a00740be0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 1...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 16, 17, 18, 1, 19, 20, 15]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 21, 16, 7, 18, 1, 19, 22, 15]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 21, 16, 7, 23, 11, 24, 25, 22, 15]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 21, 16, 3, 1, 4, 5, 17, 18, 1, 19, 26, 15]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40163</th>\n",
              "      <td>[234, 1258, 38, 1, 4025, 3, 561, 35, 1202, 306...</td>\n",
              "      <td>997338199_7343367d7f.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40164</th>\n",
              "      <td>[1, 75, 3, 1, 4, 161, 112, 1, 164, 202, 15]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40165</th>\n",
              "      <td>[1, 75, 6, 164, 7, 488, 3, 23, 227, 15]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40166</th>\n",
              "      <td>[1, 187, 3, 1, 110, 161, 7, 8, 1, 164, 202, 54...</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40167</th>\n",
              "      <td>[1, 164, 288, 3, 1, 110, 161, 15]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40168 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69974d2f-c79f-4b25-aba4-979a00740be0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-69974d2f-c79f-4b25-aba4-979a00740be0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-69974d2f-c79f-4b25-aba4-979a00740be0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-fc1f164b-073c-4e3b-810a-6a9c5154e338\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc1f164b-073c-4e3b-810a-6a9c5154e338')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-fc1f164b-073c-4e3b-810a-6a9c5154e338 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_b77714a5-6e17-41cb-ae90-84d6e6c7914f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b77714a5-6e17-41cb-ae90-84d6e6c7914f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 40168,\n  \"fields\": [\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3115174046_9e96b9ce47.jpg\",\n          \"3107592525_0bcd00777e.jpg\",\n          \"2426724282_237bca30b5.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert dict keys and values to lists\n",
        "# keys = list(full_dataset.keys())\n",
        "# values = list(full_dataset.values())\n",
        "\n",
        "# # Split keys and corresponding values together\n",
        "# keys_train, keys_test, vals_train, vals_test = sklearn.model_selection.train_test_split(\n",
        "#     keys, values,\n",
        "#     test_size=0.2,\n",
        "#     random_state=42,  # for reproducibility\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "# # Reconstruct train/test dicts\n",
        "# train_data = dict(zip(keys_train, vals_train))\n",
        "# test_data  = dict(zip(keys_test, vals_test))"
      ],
      "metadata": {
        "id": "9NwsZ3uxlTGU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Train data lengh: {len(list(train_data.keys()))}\")\n",
        "# print(f\"Test data length: {len(list(test_data.keys()))}\")"
      ],
      "metadata": {
        "id": "SX3a_yHSlS9y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Make the dataframe from the dictionary***"
      ],
      "metadata": {
        "id": "9PB1Z9LVkYJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train = pd.DataFrame.from_dict(train_data, orient='index', columns=['value'])\n",
        "# df_train = df_train.reset_index().rename(columns={'index': 'key'})\n",
        "# df_train"
      ],
      "metadata": {
        "id": "CWoXmmW_F0Yt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_test = pd.DataFrame.from_dict(test_data, orient='index', columns=['value'])\n",
        "# df_test = df_test.reset_index().rename(columns={'index': 'key'})\n",
        "# df_test"
      ],
      "metadata": {
        "id": "C3OQxRPPk2uW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Feature extractor that extract the feature from the image***"
      ],
      "metadata": {
        "id": "C3NS28irnDK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extractor(img):\n",
        "    resize_img = load_image(img)[tf.newaxis, :]\n",
        "    img_feature = restnet(resize_img)\n",
        "    return img_feature"
      ],
      "metadata": {
        "id": "RqiZGDP11XB4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Transformer Embedding part of token***"
      ],
      "metadata": {
        "id": "JS05ryJp24N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(vocab_dict.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWymKj7e38QO",
        "outputId": "09484459-b4e0-4c54-f9dc-09f8ea7768e4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8830"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(list(vocab_dict.keys()))\n",
        "vocabulary_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpys_FJKsocd",
        "outputId": "fc67a52b-bdd1-4f37-f8a5-e0b0cd19f082"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8830"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 50):\n",
        "        super().__init__()\n",
        "        # Create a long enough P matrix once\n",
        "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len,1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                             (-math.log(10000.0) / d_model))  # (d_model/2,)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            x + positional encoding, same shape\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        # only take the first seq_len positions\n",
        "        return x + self.pe[:, :seq_len]\n",
        "\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, max_length, depth: int):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, depth)\n",
        "        self.pos_enc     = PositionalEncoding(depth, max_length)\n",
        "        # this single layer can serve as demonstration; in practice you’d stack several of these\n",
        "        # self.self_attn   = nn.MultiheadAttention(d_model, nhead)\n",
        "        # self.ffn         = nn.Sequential(\n",
        "        #     nn.Linear(d_model, 4*d_model),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(4*d_model, d_model)\n",
        "        # )\n",
        "        # self.layernorm1  = nn.LayerNorm(d_model)\n",
        "        # self.layernorm2  = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tgt: target token IDs, shape (batch_size, tgt_len)\n",
        "        Returns:\n",
        "            output embeddings, shape (batch_size, tgt_len, d_model)\n",
        "        \"\"\"\n",
        "        bsz, tgt_len = tgt.size()\n",
        "\n",
        "        # 1) Token embedding\n",
        "        tok_emb = self.token_embed(tgt)                     # (bsz, tgt_len, d_model)\n",
        "\n",
        "        # 2) Positional embedding\n",
        "        x = self.pos_enc(tok_emb)                           # (bsz, tgt_len, d_model)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     B, L, V, D, H = 2, 10, 1000, 512, 8\n",
        "#     decoder = EmbeddingLayer(vocab_size=V, max_length=50, depth=D)\n",
        "#     test_tensor = convert_output_to_tensor(vocab_dict=vocab_dict, vocabulary_size=vocabulary_size)\n",
        "#     tgt_ids = test_tensor.preprocessing([[1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 12, 13, 14, 15], [1, 16, 17, 18, 1, 19, 20, 15]])\n",
        "#     out = decoder(tgt_ids)                                # (B, L, D)\n",
        "#     print(out)\n",
        "#     print(out.shape)  # -> torch.Size([2, 10, 512])\n"
      ],
      "metadata": {
        "id": "Zzy2o9C_1wqN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_length, depth):\n",
        "    super().__init__()\n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=depth,\n",
        "        mask_zero=True)\n",
        "\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __call__(self, seq):\n",
        "    # seq = convert_into_index(seq)\n",
        "    print(f\"Calling Sequence embedding: {seq}\")\n",
        "\n",
        "    for _ in range(0, self.max_length-len(seq)):\n",
        "        seq.append(0)\n",
        "\n",
        "    seq = np.array(seq, dtype=np.int32)\n",
        "    print(f\"Sequence for embedding: {seq}\")\n",
        "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "    print(f\"After token embedding: {seq.shape}\\n\")\n",
        "\n",
        "    token_emb = seq\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[0])  # (seq)\n",
        "    x = x[tf.newaxis, :]  # (1, seq)\n",
        "    print(x.shape)\n",
        "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "    print(f\"After positional embedding: {x.shape}\\n\")\n",
        "\n",
        "    return self.add([seq, token_emb[np.newaxis, :, :]])"
      ],
      "metadata": {
        "id": "NJ2r-3d40nTD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # Use Add instead of + so the keras mask propagates through.\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    attn = self.mha(query=x, value=x,\n",
        "                    use_causal_mask=True)\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "EoFF1KkRKJGv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def __call__(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(\n",
        "             query=x, value=y,\n",
        "             return_attention_scores=True)\n",
        "\n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "dgfIg6SDKX7b"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
        "    ])\n",
        "\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)\n"
      ],
      "metadata": {
        "id": "8r_sGDknKcIH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "\n",
        "\n",
        "  def __call__(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    print(\"Sucessfully entering into the decoder layer\")\n",
        "    # Text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    print(f\"After completing self attention layer: {out_seq.shape}\\n\")\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "\n",
        "    print(f'After completing cross attention layer: {out_seq}\\n')\n",
        "\n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    print(f\"After completing last attention score: {self.last_attention_scores}\\n\")\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    print(f'After feed forward neural network: {out_seq}\\n')\n",
        "\n",
        "    return out_seq"
      ],
      "metadata": {
        "id": "dOFVz8xjKcBi"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=vocabulary_size, activation='softmax', **kwargs)\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens)\n",
        "\n",
        "    print(f\"Counts in the Token Output: {counts}\\n\")\n",
        "\n",
        "    counts_arr = np.zeros(shape=(vocabulary_size,))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    print(f\"Counts array after removing banned tokens: {counts_arr}\\n\")\n",
        "    print(f'Shape of the counts array: {counts_arr.shape}\\n')\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    print(f\"Total: {total}\\n\")\n",
        "    p = counts_arr/total   ## get the empirical probability of each token. This is used to compute entropy and bias terms in probability space.\n",
        "    print(f\"Probability: {p}\\n\")\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "    print(f\"log_p: {log_p}\\n\")\n",
        "\n",
        "    entropy = -(log_p*p).sum()  ## Computes the marginal entropy. it describe how much uncertainty remains if you only knew the marginal token distribution.\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(vocabulary_size):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "    print(f\"Bias: {self.bias}\\n\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    print(f\"Successfully enter into the output layer: {x}\")\n",
        "    print(f\"input shape of the output layer: {x.shape}\")\n",
        "    x = self.dense(x)\n",
        "    # TODO(b/250038731): Fix this.\n",
        "    # An Add layer doesn't work because of the different shapes.\n",
        "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
        "    # the losses.\n",
        "    return x"
      ],
      "metadata": {
        "id": "y_Agoh--KXOc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_into_index('the dog is playing with the stick on the beach')"
      ],
      "metadata": {
        "id": "KeXeteI9-c9A"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Converting the caption into index***"
      ],
      "metadata": {
        "id": "FGsVP3mo_xUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for index, caption in enumerate(df_train['key'], start=0):\n",
        "#     caption = caption.replace(\"  \", \" \")\n",
        "#     caption = caption.replace(\"[END]\", \"\")\n",
        "#     df_train.at[index, 'key'] = convert_into_index(caption)"
      ],
      "metadata": {
        "id": "9ewYKbf79r02"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for index, caption in enumerate(df_test['key'], start=0):\n",
        "#     caption = caption.replace(\"  \", \" \")\n",
        "#     caption = caption.replace(\"[END]\", \"\")\n",
        "#     df_test.at[index, 'key'] = convert_into_index(caption)"
      ],
      "metadata": {
        "id": "DBOy7cOI_w5V"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train"
      ],
      "metadata": {
        "id": "FB2c8wsK_iEU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in vocab_dict.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "    if value == 2:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Z73KEeBP3n",
        "outputId": "f1d2b6c7-79d9-43ba-c554-31ae097e3697"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[START]: 0\n",
            "a: 1\n",
            "child: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = TokenOutput(banned_tokens=['[START]'])\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(df['caption'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqYXixcyrolb",
        "outputId": "b3c17cd0-524a-4afd-8ea1-2a4afdf1ecc3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40168/40168 [00:00<00:00, 716739.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts in the Token Output: Counter({1: 62683, 15: 40168, 3: 18876, 23: 18283, 38: 10707, 6: 9316, 29: 8840, 28: 8018, 35: 7756, 75: 7248, 10: 6707, 46: 5571, 40: 3927, 27: 3801, 146: 3575, 31: 3486, 234: 3400, 16: 3323, 24: 3172, 101: 3061, 44: 2913, 197: 2881, 141: 2752, 110: 2669, 73: 2629, 41: 2556, 12: 2430, 79: 2356, 162: 2268, 47: 2057, 118: 2035, 34: 1977, 123: 1976, 78: 1968, 386: 1821, 161: 1806, 142: 1787, 134: 1773, 21: 1766, 65: 1586, 2: 1543, 187: 1534, 174: 1465, 177: 1445, 166: 1402, 57: 1385, 346: 1373, 62: 1367, 91: 1324, 64: 1275, 8: 1259, 178: 1258, 80: 1247, 63: 1232, 127: 1225, 144: 1217, 196: 1216, 222: 1212, 25: 1178, 155: 1164, 438: 1151, 274: 1113, 18: 1070, 227: 1050, 136: 1041, 129: 1025, 223: 983, 230: 970, 185: 953, 192: 947, 45: 942, 210: 920, 131: 919, 646: 902, 128: 894, 138: 869, 294: 867, 548: 858, 435: 844, 74: 786, 37: 772, 374: 759, 86: 749, 140: 743, 164: 743, 50: 742, 94: 741, 4: 735, 1231: 731, 276: 725, 256: 693, 356: 691, 490: 690, 95: 680, 289: 677, 457: 665, 906: 652, 241: 645, 159: 633, 397: 624, 403: 595, 550: 586, 224: 581, 787: 578, 56: 577, 660: 563, 298: 562, 149: 554, 296: 552, 513: 551, 394: 529, 260: 525, 350: 518, 66: 515, 189: 512, 20: 510, 89: 503, 7: 497, 437: 491, 380: 485, 202: 482, 1337: 482, 207: 471, 119: 470, 483: 470, 280: 467, 160: 464, 436: 462, 317: 461, 477: 452, 347: 441, 240: 439, 418: 435, 297: 434, 423: 432, 36: 429, 539: 427, 948: 427, 255: 419, 345: 418, 171: 417, 400: 417, 551: 417, 470: 413, 281: 405, 81: 403, 510: 402, 69: 401, 417: 399, 485: 397, 621: 397, 282: 392, 2926: 392, 39: 388, 420: 378, 552: 378, 878: 378, 132: 377, 502: 377, 77: 375, 652: 375, 1173: 373, 216: 367, 954: 365, 254: 359, 547: 358, 93: 356, 482: 356, 5: 348, 97: 346, 244: 342, 443: 340, 122: 339, 249: 336, 522: 334, 237: 331, 674: 325, 150: 318, 1051: 317, 569: 316, 204: 314, 518: 312, 873: 310, 422: 306, 54: 305, 70: 304, 145: 302, 574: 302, 1008: 297, 399: 291, 1229: 287, 19: 284, 1126: 283, 1494: 283, 488: 281, 236: 279, 1210: 277, 206: 276, 432: 276, 1403: 270, 693: 268, 190: 261, 349: 260, 597: 259, 290: 258, 367: 258, 634: 256, 307: 254, 242: 251, 1272: 250, 558: 249, 108: 248, 168: 247, 231: 247, 504: 247, 962: 247, 60: 246, 247: 246, 258: 246, 481: 245, 961: 245, 739: 244, 820: 240, 428: 236, 937: 234, 599: 231, 852: 231, 384: 227, 877: 226, 163: 225, 1530: 224, 616: 223, 272: 221, 580: 218, 405: 217, 975: 216, 396: 215, 115: 211, 671: 209, 233: 208, 295: 208, 404: 208, 99: 206, 450: 204, 508: 204, 165: 203, 401: 202, 521: 201, 826: 201, 117: 200, 112: 198, 173: 198, 469: 196, 195: 195, 419: 195, 884: 195, 320: 194, 839: 193, 147: 192, 261: 190, 402: 190, 532: 190, 90: 189, 1114: 188, 228: 184, 537: 184, 761: 184, 387: 183, 323: 182, 728: 181, 2611: 180, 121: 178, 300: 178, 182: 177, 262: 175, 526: 174, 705: 174, 226: 172, 808: 172, 1178: 172, 965: 171, 133: 170, 135: 170, 583: 170, 214: 169, 416: 169, 585: 169, 804: 169, 2190: 168, 1084: 165, 232: 163, 353: 163, 500: 162, 411: 161, 664: 160, 336: 159, 655: 159, 525: 158, 542: 158, 1417: 158, 158: 157, 445: 156, 1434: 156, 218: 155, 2185: 155, 277: 154, 219: 153, 408: 153, 789: 153, 1048: 153, 203: 152, 221: 152, 17: 149, 1123: 149, 208: 148, 473: 148, 184: 147, 53: 146, 348: 146, 354: 146, 670: 146, 856: 146, 898: 145, 1487: 145, 368: 144, 3616: 144, 268: 142, 972: 142, 113: 141, 1739: 141, 243: 139, 424: 139, 169: 138, 462: 138, 492: 137, 777: 137, 850: 137, 1447: 136, 1097: 135, 733: 134, 1058: 134, 561: 132, 648: 132, 876: 132, 1704: 132, 92: 131, 1113: 131, 434: 130, 942: 130, 32: 129, 269: 128, 293: 128, 153: 127, 225: 125, 529: 125, 571: 124, 538: 123, 579: 123, 734: 123, 963: 123, 1009: 123, 288: 121, 905: 121, 1175: 121, 738: 119, 1049: 119, 1140: 119, 1598: 119, 2050: 119, 586: 118, 212: 117, 696: 116, 1534: 116, 98: 115, 665: 115, 718: 115, 1115: 115, 1359: 115, 395: 114, 157: 113, 251: 113, 460: 113, 391: 112, 283: 111, 593: 111, 721: 111, 943: 111, 1232: 111, 694: 110, 11: 109, 601: 109, 9: 108, 355: 108, 476: 108, 562: 108, 690: 108, 900: 108, 229: 107, 275: 106, 309: 106, 524: 106, 744: 106, 1082: 106, 1331: 106, 1356: 106, 1015: 105, 87: 104, 377: 104, 1245: 104, 414: 103, 1227: 103, 1941: 102, 666: 101, 747: 101, 760: 101, 1382: 101, 1936: 101, 533: 100, 656: 100, 1638: 100, 509: 99, 623: 99, 704: 98, 838: 98, 866: 98, 209: 97, 501: 97, 1437: 97, 1524: 97, 338: 96, 854: 96, 1172: 96, 252: 95, 325: 95, 503: 95, 1296: 95, 1446: 94, 1559: 94, 458: 93, 618: 93, 1016: 93, 1381: 93, 366: 92, 573: 92, 604: 92, 626: 92, 1160: 92, 1747: 92, 828: 91, 1055: 91, 1308: 91, 1622: 91, 649: 90, 1073: 90, 1117: 90, 301: 89, 564: 89, 964: 89, 1419: 89, 1451: 89, 855: 88, 1570: 88, 2375: 88, 137: 87, 617: 87, 762: 87, 1166: 87, 3488: 87, 302: 86, 640: 86, 689: 86, 857: 86, 1133: 86, 292: 85, 1046: 85, 1735: 85, 1787: 85, 375: 84, 322: 83, 360: 83, 364: 83, 584: 83, 3058: 83, 339: 82, 632: 82, 726: 82, 1567: 82, 238: 81, 341: 81, 931: 81, 1212: 81, 1274: 81, 1313: 81, 454: 80, 506: 80, 582: 80, 1416: 80, 643: 79, 716: 79, 795: 79, 1254: 79, 1736: 79, 1990: 79, 2245: 79, 183: 78, 600: 78, 620: 78, 1642: 78, 2002: 78, 329: 77, 383: 77, 415: 77, 479: 77, 1837: 77, 263: 76, 494: 76, 727: 76, 938: 76, 1233: 76, 2007: 76, 2239: 76, 442: 75, 780: 75, 1569: 75, 2028: 75, 440: 74, 595: 74, 1321: 74, 1650: 74, 2032: 74, 388: 73, 489: 73, 768: 73, 1392: 73, 2153: 73, 305: 72, 555: 72, 637: 72, 796: 72, 836: 72, 1132: 72, 1263: 72, 1476: 72, 337: 71, 1261: 71, 139: 70, 170: 70, 278: 70, 523: 70, 907: 70, 1491: 70, 107: 69, 635: 69, 920: 69, 303: 68, 557: 68, 1068: 68, 3208: 68, 181: 67, 267: 67, 578: 67, 612: 67, 732: 67, 1019: 67, 1037: 67, 1075: 67, 1224: 67, 1460: 67, 2480: 67, 2919: 67, 250: 66, 461: 66, 519: 66, 591: 66, 654: 66, 950: 66, 977: 66, 1678: 66, 2092: 66, 3512: 66, 179: 65, 220: 65, 1264: 65, 1426: 65, 2049: 65, 2191: 65, 2507: 65, 609: 64, 699: 64, 1453: 64, 1802: 64, 3219: 64, 58: 63, 335: 63, 946: 63, 981: 63, 1069: 63, 1950: 63, 2001: 63, 5195: 63, 592: 62, 702: 62, 1214: 62, 1531: 62, 2404: 62, 55: 61, 330: 61, 471: 61, 496: 61, 736: 61, 1027: 61, 1243: 61, 85: 60, 774: 60, 1137: 60, 1202: 60, 1721: 60, 924: 59, 978: 59, 991: 59, 1957: 59, 594: 58, 956: 58, 1065: 58, 1116: 58, 1439: 58, 1482: 58, 1668: 58, 1869: 58, 1948: 58, 2196: 58, 2556: 58, 43: 57, 109: 57, 421: 57, 517: 57, 1271: 57, 1594: 57, 1684: 57, 1879: 57, 3218: 57, 76: 56, 607: 56, 903: 56, 925: 56, 1066: 56, 1355: 56, 1953: 56, 2044: 56, 2595: 56, 167: 55, 245: 55, 393: 55, 575: 55, 848: 55, 882: 55, 1541: 55, 1624: 55, 1750: 55, 120: 54, 2954: 54, 211: 53, 514: 53, 596: 53, 745: 53, 822: 53, 831: 53, 1398: 53, 1571: 53, 1719: 53, 2346: 53, 4162: 53, 365: 52, 409: 52, 472: 52, 487: 52, 1422: 52, 1485: 52, 1538: 52, 2075: 52, 2555: 52, 2572: 52, 82: 51, 875: 51, 1252: 51, 1431: 51, 1436: 51, 1562: 51, 1636: 51, 1698: 51, 1898: 51, 14: 50, 215: 50, 455: 50, 776: 50, 1279: 50, 1489: 50, 2270: 50, 3090: 50, 3525: 50, 512: 49, 576: 49, 773: 49, 851: 49, 982: 49, 1063: 49, 1246: 49, 1277: 49, 1341: 49, 1813: 49, 1816: 49, 1840: 49, 2011: 49, 2162: 49, 2236: 49, 3290: 49, 3910: 49, 51: 48, 425: 48, 427: 48, 446: 48, 835: 48, 953: 48, 1004: 48, 1162: 48, 1653: 48, 1872: 48, 2123: 48, 468: 47, 1649: 47, 2269: 47, 331: 46, 1089: 46, 1560: 46, 1998: 46, 2125: 46, 48: 45, 103: 45, 213: 45, 217: 45, 511: 45, 765: 45, 1318: 45, 1497: 45, 1611: 45, 2057: 45, 2129: 45, 2140: 45, 2646: 45, 2807: 45, 3228: 45, 459: 44, 624: 44, 749: 44, 910: 44, 913: 44, 1281: 44, 1424: 44, 1475: 44, 1481: 44, 1608: 44, 1656: 44, 1746: 44, 72: 43, 88: 43, 156: 43, 319: 43, 464: 43, 544: 43, 633: 43, 677: 43, 885: 43, 955: 43, 1240: 43, 1293: 43, 1299: 43, 1389: 43, 1498: 43, 2474: 43, 3908: 43, 3918: 43, 4209: 43, 4321: 43, 989: 42, 1070: 42, 1319: 42, 1326: 42, 1386: 42, 1432: 42, 2146: 42, 2788: 42, 3685: 42, 4011: 42, 52: 41, 389: 41, 748: 41, 886: 41, 1473: 41, 1945: 41, 2483: 41, 2943: 41, 3392: 41, 429: 40, 433: 40, 881: 40, 895: 40, 918: 40, 1311: 40, 1542: 40, 1584: 40, 1934: 40, 2134: 40, 2166: 40, 2478: 40, 2521: 40, 2676: 40, 3317: 40, 4687: 40, 104: 39, 328: 39, 619: 39, 647: 39, 767: 39, 810: 39, 968: 39, 1161: 39, 1222: 39, 1275: 39, 1630: 39, 1631: 39, 1727: 39, 4627: 39, 116: 38, 332: 38, 505: 38, 577: 38, 818: 38, 1007: 38, 1024: 38, 1036: 38, 1136: 38, 1138: 38, 1205: 38, 1290: 38, 1605: 38, 2113: 38, 2441: 38, 2627: 38, 3060: 38, 3088: 38, 30: 37, 130: 37, 239: 37, 271: 37, 590: 37, 684: 37, 740: 37, 911: 37, 1042: 37, 1085: 37, 1127: 37, 1149: 37, 1579: 37, 1851: 37, 2025: 37, 2091: 37, 2329: 37, 2358: 37, 2470: 37, 2487: 37, 2980: 37, 3260: 37, 3515: 37, 3957: 37, 463: 36, 545: 36, 668: 36, 843: 36, 1612: 36, 1766: 36, 1871: 36, 1987: 36, 2006: 36, 2353: 36, 2461: 36, 304: 35, 363: 35, 534: 35, 672: 35, 755: 35, 995: 35, 1219: 35, 1327: 35, 1717: 35, 2167: 35, 2195: 35, 2216: 35, 2464: 35, 2612: 35, 2845: 35, 2990: 35, 4834: 35, 205: 34, 318: 34, 431: 34, 497: 34, 932: 34, 1071: 34, 1125: 34, 1143: 34, 1466: 34, 1467: 34, 1944: 34, 1963: 34, 2074: 34, 2416: 34, 3028: 34, 175: 33, 451: 33, 680: 33, 709: 33, 737: 33, 846: 33, 1064: 33, 1223: 33, 1545: 33, 1873: 33, 2066: 33, 2581: 33, 379: 32, 631: 32, 688: 32, 786: 32, 792: 32, 874: 32, 928: 32, 971: 32, 1206: 32, 1258: 32, 1486: 32, 1777: 32, 1841: 32, 1899: 32, 1976: 32, 2142: 32, 2250: 32, 2460: 32, 2791: 32, 3071: 32, 3161: 32, 4195: 32, 125: 31, 199: 31, 357: 31, 452: 31, 763: 31, 805: 31, 994: 31, 1456: 31, 2465: 31, 2950: 31, 3634: 31, 3645: 31, 3710: 31, 3746: 31, 4279: 31, 61: 30, 324: 30, 326: 30, 554: 30, 622: 30, 679: 30, 817: 30, 896: 30, 967: 30, 1045: 30, 1086: 30, 1230: 30, 1357: 30, 1484: 30, 1566: 30, 1629: 30, 1677: 30, 1748: 30, 1758: 30, 1823: 30, 2297: 30, 2533: 30, 2941: 30, 3065: 30, 3092: 30, 3288: 30, 3533: 30, 3769: 30, 42: 29, 235: 29, 430: 29, 872: 29, 933: 29, 1047: 29, 1098: 29, 1310: 29, 1710: 29, 1773: 29, 1916: 29, 1989: 29, 2051: 29, 2359: 29, 2520: 29, 2858: 29, 2891: 29, 2939: 29, 3704: 29, 3723: 29, 176: 28, 253: 28, 308: 28, 541: 28, 904: 28, 984: 28, 997: 28, 999: 28, 1141: 28, 1328: 28, 1373: 28, 1492: 28, 1500: 28, 1715: 28, 1749: 28, 1793: 28, 2021: 28, 2078: 28, 2165: 28, 2500: 28, 2531: 28, 2590: 28, 3077: 28, 3294: 28, 3298: 28, 3683: 28, 4276: 28, 200: 27, 520: 27, 543: 27, 746: 27, 860: 27, 930: 27, 966: 27, 1145: 27, 1152: 27, 1187: 27, 1259: 27, 1445: 27, 1601: 27, 1868: 27, 2336: 27, 2367: 27, 2730: 27, 2867: 27, 3117: 27, 3664: 27, 3905: 27, 3977: 27, 273: 26, 284: 26, 351: 26, 372: 26, 527: 26, 675: 26, 985: 26, 1011: 26, 1067: 26, 1072: 26, 1182: 26, 1371: 26, 1383: 26, 1483: 26, 1495: 26, 1546: 26, 1597: 26, 1625: 26, 1637: 26, 1737: 26, 1939: 26, 2026: 26, 2027: 26, 2038: 26, 2145: 26, 2154: 26, 2314: 26, 2468: 26, 2744: 26, 2793: 26, 2841: 26, 2885: 26, 3205: 26, 3861: 26, 4302: 26, 5736: 26, 151: 25, 516: 25, 730: 25, 947: 25, 1020: 25, 1039: 25, 1083: 25, 1351: 25, 1414: 25, 1585: 25, 1602: 25, 1728: 25, 1769: 25, 1839: 25, 1874: 25, 1971: 25, 2228: 25, 2257: 25, 2524: 25, 2689: 25, 2781: 25, 2931: 25, 3063: 25, 3156: 25, 3665: 25, 3701: 25, 3835: 25, 5192: 25, 5903: 25, 264: 24, 382: 24, 559: 24, 678: 24, 800: 24, 974: 24, 992: 24, 996: 24, 1225: 24, 1372: 24, 1644: 24, 1855: 24, 1889: 24, 1933: 24, 2076: 24, 2199: 24, 2279: 24, 2497: 24, 2527: 24, 2545: 24, 2637: 24, 2794: 24, 3101: 24, 3246: 24, 3860: 24, 3922: 24, 4216: 24, 4282: 24, 4301: 24, 6622: 24, 152: 23, 378: 23, 627: 23, 764: 23, 1159: 23, 1244: 23, 1364: 23, 1926: 23, 1952: 23, 2099: 23, 2121: 23, 2151: 23, 2242: 23, 2283: 23, 2301: 23, 2306: 23, 2825: 23, 2871: 23, 2920: 23, 3280: 23, 3389: 23, 3459: 23, 3562: 23, 6780: 23, 59: 22, 333: 22, 528: 22, 602: 22, 742: 22, 794: 22, 949: 22, 1093: 22, 1216: 22, 1340: 22, 1343: 22, 1362: 22, 1374: 22, 1763: 22, 1962: 22, 2016: 22, 2060: 22, 2065: 22, 2083: 22, 2182: 22, 2211: 22, 2275: 22, 2463: 22, 2548: 22, 2667: 22, 2673: 22, 2695: 22, 3033: 22, 3477: 22, 3586: 22, 3702: 22, 3822: 22, 4618: 22, 628: 21, 669: 21, 708: 21, 715: 21, 799: 21, 889: 21, 919: 21, 1001: 21, 1096: 21, 1101: 21, 1193: 21, 1376: 21, 1599: 21, 1641: 21, 1784: 21, 1893: 21, 2000: 21, 2174: 21, 2218: 21, 2402: 21, 2541: 21, 2894: 21, 2899: 21, 2940: 21, 2945: 21, 3463: 21, 3498: 21, 3588: 21, 3620: 21, 3652: 21, 3660: 21, 3976: 21, 4608: 21, 4922: 21, 5985: 21, 83: 20, 143: 20, 291: 20, 299: 20, 449: 20, 568: 20, 667: 20, 743: 20, 784: 20, 926: 20, 1018: 20, 1025: 20, 1163: 20, 1183: 20, 1346: 20, 1428: 20, 1429: 20, 1596: 20, 1646: 20, 1660: 20, 1792: 20, 1858: 20, 1905: 20, 1947: 20, 1991: 20, 2035: 20, 2039: 20, 2453: 20, 2493: 20, 2633: 20, 2711: 20, 2737: 20, 2783: 20, 2848: 20, 2856: 20, 3267: 20, 3377: 20, 3840: 20, 4357: 20, 4446: 20, 4540: 20, 5191: 20, 6034: 20, 614: 19, 644: 19, 695: 19, 1109: 19, 1154: 19, 1241: 19, 1298: 19, 1344: 19, 1349: 19, 1661: 19, 1685: 19, 1733: 19, 1797: 19, 1801: 19, 1882: 19, 1900: 19, 1967: 19, 2238: 19, 2323: 19, 2376: 19, 2418: 19, 2616: 19, 2683: 19, 2726: 19, 2762: 19, 3005: 19, 3010: 19, 3140: 19, 3177: 19, 3243: 19, 4133: 19, 4166: 19, 4303: 19, 6407: 19, 439: 18, 448: 18, 608: 18, 611: 18, 1054: 18, 1061: 18, 1207: 18, 1235: 18, 1239: 18, 1312: 18, 1379: 18, 1385: 18, 1444: 18, 1493: 18, 1533: 18, 1555: 18, 1798: 18, 1809: 18, 1850: 18, 2071: 18, 2080: 18, 2096: 18, 2108: 18, 2150: 18, 2262: 18, 2440: 18, 2479: 18, 2603: 18, 2712: 18, 2742: 18, 2872: 18, 2925: 18, 3097: 18, 3155: 18, 3284: 18, 3427: 18, 3499: 18, 3500: 18, 3535: 18, 3738: 18, 4030: 18, 4059: 18, 4060: 18, 4084: 18, 4186: 18, 4272: 18, 4275: 18, 4358: 18, 4564: 18, 4739: 18, 536: 17, 556: 17, 598: 17, 603: 17, 719: 17, 752: 17, 815: 17, 1091: 17, 1139: 17, 1201: 17, 1411: 17, 1517: 17, 1532: 17, 1548: 17, 1552: 17, 1659: 17, 1741: 17, 1780: 17, 1804: 17, 1846: 17, 1886: 17, 1901: 17, 1919: 17, 1946: 17, 2014: 17, 2040: 17, 2087: 17, 2105: 17, 2163: 17, 2212: 17, 2263: 17, 2267: 17, 2276: 17, 2342: 17, 2390: 17, 2405: 17, 2508: 17, 2684: 17, 2770: 17, 3105: 17, 3293: 17, 3305: 17, 3315: 17, 3330: 17, 3333: 17, 3454: 17, 3542: 17, 3561: 17, 3623: 17, 3649: 17, 4304: 17, 4394: 17, 4574: 17, 4615: 17, 5173: 17, 7137: 17, 358: 16, 359: 16, 588: 16, 698: 16, 858: 16, 897: 16, 952: 16, 990: 16, 1044: 16, 1056: 16, 1057: 16, 1121: 16, 1148: 16, 1200: 16, 1220: 16, 1402: 16, 1525: 16, 1554: 16, 1640: 16, 1681: 16, 1696: 16, 1713: 16, 1820: 16, 1958: 16, 2052: 16, 2068: 16, 2104: 16, 2120: 16, 2139: 16, 2233: 16, 2282: 16, 2335: 16, 2412: 16, 2537: 16, 2550: 16, 2690: 16, 2702: 16, 2801: 16, 2910: 16, 3052: 16, 3094: 16, 3163: 16, 3190: 16, 3255: 16, 3371: 16, 3508: 16, 3935: 16, 4076: 16, 4227: 16, 4663: 16, 4914: 16, 4973: 16, 5462: 16, 6302: 16, 84: 15, 246: 15, 248: 15, 312: 15, 316: 15, 441: 15, 770: 15, 771: 15, 887: 15, 1014: 15, 1128: 15, 1144: 15, 1238: 15, 1247: 15, 1332: 15, 1387: 15, 1388: 15, 1458: 15, 1504: 15, 1581: 15, 1664: 15, 1665: 15, 1667: 15, 1724: 15, 1731: 15, 1756: 15, 1866: 15, 1912: 15, 1924: 15, 1931: 15, 2128: 15, 2169: 15, 2253: 15, 2318: 15, 2386: 15, 2481: 15, 2491: 15, 2538: 15, 2575: 15, 2583: 15, 2622: 15, 2697: 15, 2716: 15, 2748: 15, 2815: 15, 2859: 15, 2905: 15, 2955: 15, 3042: 15, 3194: 15, 3252: 15, 3376: 15, 3453: 15, 3460: 15, 3516: 15, 3571: 15, 3684: 15, 3717: 15, 3731: 15, 3907: 15, 3923: 15, 3927: 15, 4001: 15, 4071: 15, 4139: 15, 4151: 15, 4369: 15, 4861: 15, 5368: 15, 5385: 15, 5585: 15, 6573: 15, 33: 14, 71: 14, 410: 14, 530: 14, 700: 14, 729: 14, 790: 14, 862: 14, 891: 14, 921: 14, 993: 14, 1013: 14, 1094: 14, 1111: 14, 1156: 14, 1184: 14, 1249: 14, 1276: 14, 1315: 14, 1368: 14, 1370: 14, 1394: 14, 1501: 14, 1537: 14, 1565: 14, 1647: 14, 1709: 14, 1779: 14, 1910: 14, 1920: 14, 1927: 14, 1955: 14, 2053: 14, 2249: 14, 2285: 14, 2348: 14, 2354: 14, 2442: 14, 2494: 14, 2540: 14, 2619: 14, 2660: 14, 2701: 14, 2878: 14, 3136: 14, 3153: 14, 3168: 14, 3234: 14, 3307: 14, 3357: 14, 3446: 14, 3556: 14, 3705: 14, 3728: 14, 3942: 14, 4032: 14, 4070: 14, 4078: 14, 4163: 14, 4238: 14, 4495: 14, 4496: 14, 4691: 14, 4764: 14, 4797: 14, 4977: 14, 6406: 14, 279: 13, 412: 13, 546: 13, 722: 13, 754: 13, 757: 13, 802: 13, 916: 13, 944: 13, 988: 13, 1108: 13, 1260: 13, 1266: 13, 1267: 13, 1270: 13, 1352: 13, 1380: 13, 1454: 13, 1478: 13, 1490: 13, 1572: 13, 1577: 13, 1613: 13, 1620: 13, 1635: 13, 1670: 13, 1722: 13, 1734: 13, 1753: 13, 1835: 13, 1902: 13, 2008: 13, 2009: 13, 2149: 13, 2194: 13, 2201: 13, 2229: 13, 2251: 13, 2334: 13, 2383: 13, 2393: 13, 2476: 13, 2477: 13, 2568: 13, 2605: 13, 2635: 13, 2644: 13, 2651: 13, 2671: 13, 2765: 13, 2798: 13, 3016: 13, 3035: 13, 3151: 13, 3178: 13, 3274: 13, 3291: 13, 3306: 13, 4234: 13, 4376: 13, 4450: 13, 4548: 13, 4777: 13, 5348: 13, 5405: 13, 5619: 13, 6468: 13, 6774: 13, 7493: 13, 285: 12, 334: 12, 342: 12, 540: 12, 645: 12, 651: 12, 673: 12, 682: 12, 741: 12, 791: 12, 844: 12, 879: 12, 940: 12, 959: 12, 1041: 12, 1095: 12, 1106: 12, 1179: 12, 1196: 12, 1197: 12, 1282: 12, 1399: 12, 1423: 12, 1470: 12, 1479: 12, 1536: 12, 1561: 12, 1694: 12, 1697: 12, 1703: 12, 1791: 12, 1794: 12, 1848: 12, 1891: 12, 1996: 12, 2036: 12, 2063: 12, 2292: 12, 2324: 12, 2328: 12, 2378: 12, 2437: 12, 2495: 12, 2543: 12, 2656: 12, 2700: 12, 2749: 12, 2766: 12, 2933: 12, 3006: 12, 3023: 12, 3038: 12, 3096: 12, 3199: 12, 3217: 12, 3263: 12, 3461: 12, 3490: 12, 3521: 12, 3708: 12, 3830: 12, 3946: 12, 3947: 12, 4008: 12, 4077: 12, 4108: 12, 4113: 12, 4172: 12, 4305: 12, 4379: 12, 4441: 12, 4501: 12, 4652: 12, 4674: 12, 4724: 12, 5052: 12, 5763: 12, 6379: 12, 371: 11, 426: 11, 486: 11, 491: 11, 587: 11, 703: 11, 713: 11, 724: 11, 751: 11, 833: 11, 869: 11, 888: 11, 914: 11, 980: 11, 1118: 11, 1147: 11, 1150: 11, 1169: 11, 1181: 11, 1248: 11, 1324: 11, 1367: 11, 1378: 11, 1412: 11, 1462: 11, 1477: 11, 1503: 11, 1543: 11, 1714: 11, 1718: 11, 1740: 11, 1754: 11, 1878: 11, 1923: 11, 2013: 11, 2045: 11, 2069: 11, 2109: 11, 2124: 11, 2157: 11, 2159: 11, 2175: 11, 2180: 11, 2313: 11, 2579: 11, 2753: 11, 2790: 11, 2875: 11, 3055: 11, 3130: 11, 3211: 11, 3474: 11, 3504: 11, 3592: 11, 3666: 11, 3681: 11, 3771: 11, 3821: 11, 3938: 11, 3962: 11, 4022: 11, 4057: 11, 4241: 11, 4289: 11, 4296: 11, 4318: 11, 4549: 11, 4804: 11, 4832: 11, 4900: 11, 5098: 11, 5601: 11, 6178: 11, 6363: 11, 7713: 11, 369: 10, 515: 10, 560: 10, 630: 10, 692: 10, 864: 10, 1110: 10, 1155: 10, 1177: 10, 1194: 10, 1204: 10, 1208: 10, 1228: 10, 1306: 10, 1390: 10, 1420: 10, 1443: 10, 1450: 10, 1499: 10, 1518: 10, 1583: 10, 1643: 10, 1663: 10, 1675: 10, 1679: 10, 1708: 10, 1818: 10, 1830: 10, 1856: 10, 1883: 10, 1965: 10, 1970: 10, 1992: 10, 2072: 10, 2117: 10, 2127: 10, 2235: 10, 2255: 10, 2259: 10, 2311: 10, 2321: 10, 2403: 10, 2511: 10, 2535: 10, 2604: 10, 2672: 10, 2678: 10, 2681: 10, 2728: 10, 2789: 10, 2802: 10, 2834: 10, 2853: 10, 2889: 10, 2922: 10, 3134: 10, 3197: 10, 3230: 10, 3231: 10, 3329: 10, 3337: 10, 3344: 10, 3381: 10, 3450: 10, 3485: 10, 3505: 10, 3573: 10, 3574: 10, 3603: 10, 3612: 10, 3749: 10, 3775: 10, 3779: 10, 3794: 10, 3881: 10, 4132: 10, 4230: 10, 4291: 10, 4300: 10, 4308: 10, 4324: 10, 4352: 10, 4395: 10, 4408: 10, 4428: 10, 4444: 10, 4449: 10, 4518: 10, 4597: 10, 4645: 10, 4795: 10, 4993: 10, 4994: 10, 5022: 10, 5164: 10, 5332: 10, 5403: 10, 5414: 10, 5433: 10, 5506: 10, 5681: 10, 5801: 10, 5905: 10, 6002: 10, 6196: 10, 6623: 10, 7675: 10, 124: 9, 321: 9, 407: 9, 484: 9, 606: 9, 625: 9, 641: 9, 681: 9, 816: 9, 827: 9, 870: 9, 894: 9, 929: 9, 934: 9, 1006: 9, 1092: 9, 1099: 9, 1100: 9, 1105: 9, 1198: 9, 1211: 9, 1234: 9, 1251: 9, 1280: 9, 1334: 9, 1336: 9, 1350: 9, 1369: 9, 1433: 9, 1508: 9, 1515: 9, 1535: 9, 1580: 9, 1632: 9, 1686: 9, 1726: 9, 1751: 9, 1761: 9, 1768: 9, 1776: 9, 1789: 9, 1814: 9, 1815: 9, 1826: 9, 1845: 9, 1895: 9, 1909: 9, 1956: 9, 1961: 9, 1980: 9, 1982: 9, 1984: 9, 2005: 9, 2047: 9, 2079: 9, 2093: 9, 2187: 9, 2210: 9, 2223: 9, 2227: 9, 2244: 9, 2264: 9, 2291: 9, 2364: 9, 2431: 9, 2457: 9, 2469: 9, 2471: 9, 2492: 9, 2526: 9, 2592: 9, 2594: 9, 2652: 9, 2674: 9, 2709: 9, 2735: 9, 2736: 9, 2738: 9, 2758: 9, 2817: 9, 2823: 9, 2844: 9, 2850: 9, 2876: 9, 2890: 9, 2903: 9, 2915: 9, 2966: 9, 2988: 9, 2989: 9, 2993: 9, 3022: 9, 3041: 9, 3113: 9, 3126: 9, 3172: 9, 3261: 9, 3362: 9, 3378: 9, 3382: 9, 3383: 9, 3395: 9, 3468: 9, 3489: 9, 3494: 9, 3545: 9, 3560: 9, 3580: 9, 3677: 9, 3743: 9, 3744: 9, 3802: 9, 3864: 9, 3979: 9, 3986: 9, 4025: 9, 4035: 9, 4058: 9, 4062: 9, 4066: 9, 4081: 9, 4092: 9, 4109: 9, 4248: 9, 4306: 9, 4397: 9, 4430: 9, 4451: 9, 4507: 9, 4530: 9, 4537: 9, 4626: 9, 4666: 9, 4679: 9, 4696: 9, 4799: 9, 4901: 9, 4920: 9, 5159: 9, 5387: 9, 5467: 9, 5477: 9, 5638: 9, 6026: 9, 6057: 9, 6104: 9, 6595: 9, 6607: 9, 6645: 9, 6870: 9, 7007: 9, 7065: 9, 7305: 9, 7647: 9, 8005: 9, 8398: 9, 96: 8, 126: 8, 201: 8, 257: 8, 313: 8, 340: 8, 344: 8, 390: 8, 661: 8, 663: 8, 758: 8, 806: 8, 912: 8, 945: 8, 951: 8, 969: 8, 976: 8, 1017: 8, 1112: 8, 1119: 8, 1131: 8, 1142: 8, 1151: 8, 1168: 8, 1185: 8, 1186: 8, 1215: 8, 1278: 8, 1395: 8, 1406: 8, 1407: 8, 1413: 8, 1463: 8, 1469: 8, 1507: 8, 1523: 8, 1556: 8, 1676: 8, 1682: 8, 1867: 8, 1921: 8, 1972: 8, 1986: 8, 2018: 8, 2046: 8, 2141: 8, 2206: 8, 2217: 8, 2243: 8, 2261: 8, 2271: 8, 2287: 8, 2298: 8, 2303: 8, 2330: 8, 2388: 8, 2438: 8, 2444: 8, 2559: 8, 2589: 8, 2618: 8, 2642: 8, 2665: 8, 2740: 8, 2779: 8, 2804: 8, 2808: 8, 2809: 8, 2827: 8, 2879: 8, 2887: 8, 2936: 8, 2951: 8, 3017: 8, 3087: 8, 3143: 8, 3193: 8, 3239: 8, 3262: 8, 3276: 8, 3286: 8, 3321: 8, 3343: 8, 3363: 8, 3367: 8, 3393: 8, 3532: 8, 3584: 8, 3589: 8, 3593: 8, 3613: 8, 3629: 8, 3635: 8, 3670: 8, 3699: 8, 3714: 8, 3730: 8, 3732: 8, 3751: 8, 3753: 8, 3782: 8, 3823: 8, 3878: 8, 3952: 8, 3972: 8, 3973: 8, 3978: 8, 4117: 8, 4193: 8, 4231: 8, 4244: 8, 4250: 8, 4258: 8, 4443: 8, 4471: 8, 4489: 8, 4523: 8, 4629: 8, 4685: 8, 4758: 8, 4762: 8, 4822: 8, 4980: 8, 5007: 8, 5072: 8, 5157: 8, 5158: 8, 5178: 8, 5193: 8, 5260: 8, 5345: 8, 5361: 8, 5461: 8, 5478: 8, 5503: 8, 5547: 8, 5575: 8, 5656: 8, 5660: 8, 5839: 8, 5874: 8, 6039: 8, 6347: 8, 6811: 8, 6932: 8, 188: 7, 193: 7, 265: 7, 362: 7, 413: 7, 498: 7, 567: 7, 589: 7, 638: 7, 657: 7, 731: 7, 756: 7, 847: 7, 861: 7, 941: 7, 1012: 7, 1028: 7, 1043: 7, 1088: 7, 1102: 7, 1107: 7, 1176: 7, 1291: 7, 1322: 7, 1361: 7, 1449: 7, 1457: 7, 1468: 7, 1471: 7, 1516: 7, 1550: 7, 1551: 7, 1575: 7, 1615: 7, 1687: 7, 1757: 7, 1765: 7, 1772: 7, 1877: 7, 1884: 7, 1906: 7, 1907: 7, 1937: 7, 1949: 7, 2022: 7, 2033: 7, 2054: 7, 2058: 7, 2082: 7, 2089: 7, 2094: 7, 2132: 7, 2172: 7, 2183: 7, 2188: 7, 2258: 7, 2327: 7, 2344: 7, 2436: 7, 2443: 7, 2450: 7, 2484: 7, 2542: 7, 2562: 7, 2571: 7, 2624: 7, 2640: 7, 2661: 7, 2685: 7, 2687: 7, 2799: 7, 2819: 7, 2820: 7, 2833: 7, 2838: 7, 2932: 7, 2946: 7, 2958: 7, 2992: 7, 3003: 7, 3013: 7, 3015: 7, 3027: 7, 3064: 7, 3086: 7, 3099: 7, 3100: 7, 3107: 7, 3114: 7, 3135: 7, 3162: 7, 3167: 7, 3202: 7, 3215: 7, 3297: 7, 3354: 7, 3359: 7, 3374: 7, 3449: 7, 3529: 7, 3552: 7, 3622: 7, 3706: 7, 3718: 7, 3740: 7, 3788: 7, 3865: 7, 3911: 7, 3975: 7, 3997: 7, 4010: 7, 4028: 7, 4064: 7, 4069: 7, 4072: 7, 4088: 7, 4096: 7, 4114: 7, 4118: 7, 4192: 7, 4226: 7, 4262: 7, 4266: 7, 4325: 7, 4354: 7, 4392: 7, 4421: 7, 4463: 7, 4468: 7, 4491: 7, 4521: 7, 4532: 7, 4557: 7, 4622: 7, 4658: 7, 4670: 7, 4701: 7, 4756: 7, 4765: 7, 4776: 7, 4819: 7, 4871: 7, 4875: 7, 4907: 7, 4910: 7, 4985: 7, 5054: 7, 5071: 7, 5073: 7, 5114: 7, 5143: 7, 5171: 7, 5199: 7, 5238: 7, 5312: 7, 5453: 7, 5482: 7, 5493: 7, 5497: 7, 5508: 7, 5560: 7, 5650: 7, 5872: 7, 5922: 7, 5946: 7, 6005: 7, 6207: 7, 6661: 7, 6675: 7, 6679: 7, 6714: 7, 7005: 7, 7607: 7, 22: 6, 68: 6, 106: 6, 311: 6, 352: 6, 373: 6, 381: 6, 392: 6, 613: 6, 717: 6, 720: 6, 779: 6, 781: 6, 793: 6, 809: 6, 814: 6, 824: 6, 840: 6, 957: 6, 960: 6, 1005: 6, 1030: 6, 1062: 6, 1120: 6, 1158: 6, 1167: 6, 1217: 6, 1273: 6, 1314: 6, 1335: 6, 1408: 6, 1465: 6, 1510: 6, 1513: 6, 1547: 6, 1563: 6, 1617: 6, 1619: 6, 1621: 6, 1634: 6, 1657: 6, 1689: 6, 1705: 6, 1752: 6, 1783: 6, 1795: 6, 1844: 6, 1860: 6, 1881: 6, 1892: 6, 1935: 6, 1943: 6, 2056: 6, 2090: 6, 2095: 6, 2126: 6, 2135: 6, 2203: 6, 2214: 6, 2230: 6, 2241: 6, 2273: 6, 2316: 6, 2347: 6, 2374: 6, 2414: 6, 2447: 6, 2452: 6, 2485: 6, 2489: 6, 2536: 6, 2539: 6, 2576: 6, 2577: 6, 2628: 6, 2630: 6, 2638: 6, 2653: 6, 2662: 6, 2666: 6, 2680: 6, 2682: 6, 2692: 6, 2698: 6, 2703: 6, 2729: 6, 2754: 6, 2769: 6, 2806: 6, 2846: 6, 2873: 6, 2960: 6, 2997: 6, 3020: 6, 3026: 6, 3034: 6, 3054: 6, 3061: 6, 3081: 6, 3191: 6, 3192: 6, 3232: 6, 3254: 6, 3259: 6, 3265: 6, 3272: 6, 3308: 6, 3326: 6, 3334: 6, 3414: 6, 3458: 6, 3471: 6, 3473: 6, 3644: 6, 3675: 6, 3680: 6, 3696: 6, 3700: 6, 3754: 6, 3758: 6, 3778: 6, 3803: 6, 3804: 6, 3812: 6, 3848: 6, 3853: 6, 3867: 6, 3871: 6, 3894: 6, 3895: 6, 3913: 6, 3921: 6, 3925: 6, 3958: 6, 3960: 6, 3971: 6, 4046: 6, 4068: 6, 4090: 6, 4105: 6, 4137: 6, 4153: 6, 4213: 6, 4257: 6, 4293: 6, 4295: 6, 4323: 6, 4334: 6, 4349: 6, 4416: 6, 4474: 6, 4499: 6, 4632: 6, 4688: 6, 4693: 6, 4786: 6, 4789: 6, 4811: 6, 4841: 6, 4870: 6, 4885: 6, 4919: 6, 4982: 6, 5025: 6, 5033: 6, 5097: 6, 5132: 6, 5154: 6, 5162: 6, 5188: 6, 5190: 6, 5208: 6, 5225: 6, 5259: 6, 5408: 6, 5439: 6, 5448: 6, 5481: 6, 5523: 6, 5528: 6, 5537: 6, 5568: 6, 5624: 6, 5659: 6, 5713: 6, 5722: 6, 5748: 6, 5802: 6, 5833: 6, 5873: 6, 5920: 6, 5925: 6, 5979: 6, 6036: 6, 6058: 6, 6059: 6, 6117: 6, 6134: 6, 6153: 6, 6322: 6, 6467: 6, 6498: 6, 6534: 6, 6610: 6, 6873: 6, 6938: 6, 6974: 6, 7059: 6, 7063: 6, 7269: 6, 7593: 6, 7608: 6, 7754: 6, 7827: 6, 7874: 6, 8028: 6, 8485: 6, 8486: 6, 49: 5, 180: 5, 186: 5, 270: 5, 327: 5, 499: 5, 507: 5, 549: 5, 629: 5, 639: 5, 750: 5, 823: 5, 834: 5, 842: 5, 853: 5, 859: 5, 868: 5, 927: 5, 936: 5, 973: 5, 986: 5, 998: 5, 1032: 5, 1038: 5, 1040: 5, 1074: 5, 1081: 5, 1104: 5, 1129: 5, 1153: 5, 1157: 5, 1226: 5, 1253: 5, 1268: 5, 1283: 5, 1289: 5, 1292: 5, 1297: 5, 1323: 5, 1339: 5, 1342: 5, 1375: 5, 1430: 5, 1452: 5, 1474: 5, 1512: 5, 1521: 5, 1540: 5, 1568: 5, 1588: 5, 1593: 5, 1606: 5, 1628: 5, 1633: 5, 1662: 5, 1691: 5, 1699: 5, 1702: 5, 1720: 5, 1810: 5, 1825: 5, 1831: 5, 1849: 5, 1852: 5, 1864: 5, 1887: 5, 1894: 5, 1897: 5, 1964: 5, 1966: 5, 1968: 5, 1973: 5, 1977: 5, 1999: 5, 2004: 5, 2031: 5, 2112: 5, 2122: 5, 2177: 5, 2181: 5, 2192: 5, 2215: 5, 2274: 5, 2277: 5, 2293: 5, 2300: 5, 2309: 5, 2315: 5, 2317: 5, 2332: 5, 2357: 5, 2362: 5, 2370: 5, 2399: 5, 2400: 5, 2426: 5, 2467: 5, 2557: 5, 2561: 5, 2563: 5, 2587: 5, 2600: 5, 2609: 5, 2610: 5, 2617: 5, 2621: 5, 2629: 5, 2631: 5, 2664: 5, 2752: 5, 2755: 5, 2756: 5, 2759: 5, 2760: 5, 2773: 5, 2777: 5, 2778: 5, 2818: 5, 2837: 5, 2840: 5, 2865: 5, 2870: 5, 2883: 5, 2897: 5, 2921: 5, 2949: 5, 2962: 5, 2967: 5, 2969: 5, 2981: 5, 2982: 5, 2994: 5, 3002: 5, 3014: 5, 3050: 5, 3066: 5, 3110: 5, 3124: 5, 3133: 5, 3137: 5, 3141: 5, 3142: 5, 3144: 5, 3145: 5, 3154: 5, 3157: 5, 3203: 5, 3226: 5, 3227: 5, 3240: 5, 3241: 5, 3245: 5, 3316: 5, 3364: 5, 3384: 5, 3436: 5, 3479: 5, 3502: 5, 3513: 5, 3520: 5, 3530: 5, 3534: 5, 3547: 5, 3567: 5, 3568: 5, 3585: 5, 3625: 5, 3637: 5, 3657: 5, 3658: 5, 3663: 5, 3668: 5, 3673: 5, 3720: 5, 3724: 5, 3770: 5, 3772: 5, 3795: 5, 3855: 5, 3858: 5, 3859: 5, 3902: 5, 3903: 5, 3945: 5, 3948: 5, 3964: 5, 3966: 5, 3992: 5, 3999: 5, 4074: 5, 4080: 5, 4093: 5, 4115: 5, 4119: 5, 4140: 5, 4142: 5, 4146: 5, 4228: 5, 4263: 5, 4322: 5, 4332: 5, 4333: 5, 4341: 5, 4356: 5, 4359: 5, 4360: 5, 4393: 5, 4399: 5, 4406: 5, 4431: 5, 4439: 5, 4453: 5, 4488: 5, 4494: 5, 4502: 5, 4515: 5, 4528: 5, 4556: 5, 4563: 5, 4568: 5, 4573: 5, 4653: 5, 4659: 5, 4667: 5, 4678: 5, 4683: 5, 4684: 5, 4725: 5, 4750: 5, 4767: 5, 4779: 5, 4783: 5, 4801: 5, 4810: 5, 4850: 5, 4856: 5, 4889: 5, 4902: 5, 4957: 5, 4976: 5, 4998: 5, 5020: 5, 5036: 5, 5066: 5, 5093: 5, 5099: 5, 5103: 5, 5117: 5, 5175: 5, 5210: 5, 5215: 5, 5221: 5, 5228: 5, 5244: 5, 5267: 5, 5272: 5, 5274: 5, 5280: 5, 5317: 5, 5321: 5, 5370: 5, 5375: 5, 5390: 5, 5392: 5, 5396: 5, 5410: 5, 5418: 5, 5425: 5, 5427: 5, 5434: 5, 5441: 5, 5511: 5, 5532: 5, 5546: 5, 5584: 5, 5591: 5, 5602: 5, 5621: 5, 5636: 5, 5655: 5, 5678: 5, 5712: 5, 5752: 5, 5809: 5, 5893: 5, 5980: 5, 6006: 5, 6037: 5, 6075: 5, 6087: 5, 6112: 5, 6129: 5, 6137: 5, 6203: 5, 6228: 5, 6259: 5, 6307: 5, 6328: 5, 6349: 5, 6359: 5, 6364: 5, 6369: 5, 6421: 5, 6483: 5, 6493: 5, 6537: 5, 6592: 5, 6620: 5, 6630: 5, 6659: 5, 6666: 5, 6690: 5, 6726: 5, 6783: 5, 6804: 5, 6813: 5, 6814: 5, 6843: 5, 6861: 5, 6862: 5, 6884: 5, 6919: 5, 6959: 5, 7023: 5, 7078: 5, 7131: 5, 7138: 5, 7177: 5, 7178: 5, 7335: 5, 7380: 5, 7403: 5, 7456: 5, 7513: 5, 7530: 5, 7648: 5, 7748: 5, 7861: 5, 8192: 5, 8328: 5, 8336: 5, 8763: 5, 8799: 5, 26: 4, 191: 4, 456: 4, 466: 4, 531: 4, 565: 4, 615: 4, 650: 4, 686: 4, 691: 4, 766: 4, 797: 4, 812: 4, 883: 4, 901: 4, 902: 4, 909: 4, 923: 4, 1026: 4, 1034: 4, 1050: 4, 1090: 4, 1165: 4, 1192: 4, 1209: 4, 1269: 4, 1303: 4, 1329: 4, 1347: 4, 1354: 4, 1377: 4, 1455: 4, 1459: 4, 1539: 4, 1574: 4, 1586: 4, 1592: 4, 1610: 4, 1614: 4, 1627: 4, 1654: 4, 1672: 4, 1738: 4, 1770: 4, 1771: 4, 1774: 4, 1778: 4, 1790: 4, 1817: 4, 1822: 4, 1833: 4, 1836: 4, 1861: 4, 1880: 4, 1885: 4, 1908: 4, 1918: 4, 1930: 4, 1993: 4, 1995: 4, 2003: 4, 2023: 4, 2043: 4, 2084: 4, 2097: 4, 2110: 4, 2130: 4, 2144: 4, 2148: 4, 2156: 4, 2193: 4, 2197: 4, 2204: 4, 2209: 4, 2222: 4, 2240: 4, 2248: 4, 2260: 4, 2284: 4, 2294: 4, 2312: 4, 2333: 4, 2351: 4, 2356: 4, 2377: 4, 2385: 4, 2401: 4, 2419: 4, 2430: 4, 2433: 4, 2434: 4, 2458: 4, 2473: 4, 2496: 4, 2509: 4, 2510: 4, 2513: 4, 2530: 4, 2544: 4, 2546: 4, 2549: 4, 2551: 4, 2584: 4, 2593: 4, 2596: 4, 2597: 4, 2599: 4, 2607: 4, 2632: 4, 2670: 4, 2686: 4, 2694: 4, 2699: 4, 2706: 4, 2722: 4, 2723: 4, 2733: 4, 2743: 4, 2776: 4, 2782: 4, 2796: 4, 2803: 4, 2813: 4, 2822: 4, 2831: 4, 2843: 4, 2892: 4, 2893: 4, 2895: 4, 2900: 4, 2907: 4, 2952: 4, 2964: 4, 2978: 4, 2979: 4, 2985: 4, 2995: 4, 3021: 4, 3025: 4, 3043: 4, 3069: 4, 3093: 4, 3109: 4, 3139: 4, 3146: 4, 3152: 4, 3160: 4, 3206: 4, 3207: 4, 3233: 4, 3236: 4, 3320: 4, 3323: 4, 3327: 4, 3370: 4, 3380: 4, 3396: 4, 3399: 4, 3400: 4, 3401: 4, 3404: 4, 3424: 4, 3435: 4, 3455: 4, 3484: 4, 3486: 4, 3492: 4, 3497: 4, 3518: 4, 3572: 4, 3599: 4, 3600: 4, 3624: 4, 3627: 4, 3628: 4, 3638: 4, 3643: 4, 3707: 4, 3737: 4, 3745: 4, 3759: 4, 3763: 4, 3774: 4, 3791: 4, 3792: 4, 3798: 4, 3831: 4, 3846: 4, 3874: 4, 3884: 4, 3890: 4, 3897: 4, 3909: 4, 3919: 4, 3920: 4, 3928: 4, 3993: 4, 4015: 4, 4017: 4, 4027: 4, 4038: 4, 4043: 4, 4047: 4, 4095: 4, 4106: 4, 4120: 4, 4123: 4, 4125: 4, 4148: 4, 4150: 4, 4152: 4, 4155: 4, 4170: 4, 4175: 4, 4189: 4, 4196: 4, 4208: 4, 4215: 4, 4222: 4, 4223: 4, 4224: 4, 4225: 4, 4249: 4, 4251: 4, 4252: 4, 4268: 4, 4298: 4, 4317: 4, 4374: 4, 4390: 4, 4400: 4, 4420: 4, 4423: 4, 4457: 4, 4482: 4, 4487: 4, 4503: 4, 4504: 4, 4508: 4, 4517: 4, 4545: 4, 4546: 4, 4559: 4, 4561: 4, 4572: 4, 4578: 4, 4580: 4, 4589: 4, 4611: 4, 4634: 4, 4649: 4, 4651: 4, 4660: 4, 4669: 4, 4690: 4, 4708: 4, 4712: 4, 4722: 4, 4729: 4, 4748: 4, 4772: 4, 4781: 4, 4785: 4, 4815: 4, 4826: 4, 4827: 4, 4831: 4, 4838: 4, 4849: 4, 4862: 4, 4873: 4, 4876: 4, 4879: 4, 4883: 4, 4903: 4, 4925: 4, 4930: 4, 4935: 4, 4945: 4, 4950: 4, 4956: 4, 4963: 4, 4968: 4, 4974: 4, 4981: 4, 4984: 4, 5000: 4, 5004: 4, 5005: 4, 5019: 4, 5063: 4, 5070: 4, 5076: 4, 5079: 4, 5088: 4, 5089: 4, 5102: 4, 5123: 4, 5139: 4, 5144: 4, 5163: 4, 5170: 4, 5176: 4, 5181: 4, 5194: 4, 5206: 4, 5217: 4, 5234: 4, 5242: 4, 5245: 4, 5264: 4, 5273: 4, 5275: 4, 5281: 4, 5295: 4, 5302: 4, 5303: 4, 5326: 4, 5328: 4, 5330: 4, 5344: 4, 5349: 4, 5386: 4, 5398: 4, 5435: 4, 5443: 4, 5476: 4, 5483: 4, 5485: 4, 5490: 4, 5512: 4, 5516: 4, 5570: 4, 5592: 4, 5610: 4, 5615: 4, 5664: 4, 5698: 4, 5706: 4, 5716: 4, 5734: 4, 5742: 4, 5758: 4, 5785: 4, 5806: 4, 5838: 4, 5847: 4, 5854: 4, 5855: 4, 5868: 4, 5886: 4, 5892: 4, 5906: 4, 5914: 4, 5948: 4, 5949: 4, 5952: 4, 5968: 4, 5977: 4, 5998: 4, 6011: 4, 6012: 4, 6018: 4, 6022: 4, 6023: 4, 6041: 4, 6043: 4, 6085: 4, 6094: 4, 6096: 4, 6121: 4, 6133: 4, 6143: 4, 6146: 4, 6158: 4, 6175: 4, 6184: 4, 6253: 4, 6269: 4, 6270: 4, 6386: 4, 6391: 4, 6402: 4, 6442: 4, 6443: 4, 6444: 4, 6466: 4, 6478: 4, 6481: 4, 6495: 4, 6510: 4, 6562: 4, 6576: 4, 6577: 4, 6596: 4, 6608: 4, 6609: 4, 6672: 4, 6689: 4, 6695: 4, 6766: 4, 6775: 4, 6791: 4, 6793: 4, 6863: 4, 6883: 4, 6906: 4, 6928: 4, 6976: 4, 6994: 4, 7051: 4, 7053: 4, 7076: 4, 7079: 4, 7085: 4, 7091: 4, 7112: 4, 7122: 4, 7146: 4, 7155: 4, 7179: 4, 7246: 4, 7285: 4, 7292: 4, 7293: 4, 7340: 4, 7438: 4, 7452: 4, 7458: 4, 7480: 4, 7571: 4, 7572: 4, 7677: 4, 7870: 4, 7889: 4, 7947: 4, 7958: 4, 8176: 4, 8214: 4, 8283: 4, 8498: 4, 8585: 4, 8656: 4, 8699: 4, 8752: 4, 67: 3, 148: 3, 259: 3, 314: 3, 343: 3, 553: 3, 642: 3, 659: 3, 685: 3, 706: 3, 735: 3, 772: 3, 775: 3, 801: 3, 819: 3, 845: 3, 871: 3, 935: 3, 1010: 3, 1023: 3, 1079: 3, 1124: 3, 1130: 3, 1221: 3, 1255: 3, 1330: 3, 1333: 3, 1358: 3, 1391: 3, 1397: 3, 1415: 3, 1435: 3, 1440: 3, 1442: 3, 1448: 3, 1502: 3, 1549: 3, 1564: 3, 1576: 3, 1600: 3, 1609: 3, 1616: 3, 1618: 3, 1639: 3, 1652: 3, 1669: 3, 1680: 3, 1695: 3, 1700: 3, 1706: 3, 1716: 3, 1732: 3, 1755: 3, 1800: 3, 1812: 3, 1821: 3, 1827: 3, 1838: 3, 1843: 3, 1853: 3, 1896: 3, 1913: 3, 1914: 3, 1915: 3, 1925: 3, 1928: 3, 1960: 3, 1979: 3, 1988: 3, 2019: 3, 2029: 3, 2030: 3, 2034: 3, 2086: 3, 2103: 3, 2111: 3, 2119: 3, 2137: 3, 2138: 3, 2147: 3, 2186: 3, 2189: 3, 2219: 3, 2231: 3, 2232: 3, 2234: 3, 2252: 3, 2265: 3, 2272: 3, 2288: 3, 2337: 3, 2338: 3, 2341: 3, 2366: 3, 2373: 3, 2395: 3, 2421: 3, 2425: 3, 2427: 3, 2439: 3, 2448: 3, 2454: 3, 2466: 3, 2505: 3, 2506: 3, 2525: 3, 2552: 3, 2558: 3, 2567: 3, 2574: 3, 2585: 3, 2588: 3, 2620: 3, 2625: 3, 2626: 3, 2636: 3, 2659: 3, 2668: 3, 2688: 3, 2691: 3, 2696: 3, 2715: 3, 2719: 3, 2720: 3, 2727: 3, 2741: 3, 2745: 3, 2750: 3, 2774: 3, 2785: 3, 2787: 3, 2805: 3, 2847: 3, 2866: 3, 2868: 3, 2884: 3, 2908: 3, 2928: 3, 2930: 3, 2942: 3, 2947: 3, 2957: 3, 2961: 3, 2996: 3, 3007: 3, 3030: 3, 3044: 3, 3048: 3, 3056: 3, 3057: 3, 3059: 3, 3070: 3, 3074: 3, 3080: 3, 3083: 3, 3085: 3, 3089: 3, 3103: 3, 3104: 3, 3116: 3, 3150: 3, 3165: 3, 3175: 3, 3179: 3, 3195: 3, 3196: 3, 3201: 3, 3212: 3, 3213: 3, 3229: 3, 3257: 3, 3269: 3, 3273: 3, 3296: 3, 3303: 3, 3310: 3, 3312: 3, 3331: 3, 3341: 3, 3342: 3, 3347: 3, 3360: 3, 3369: 3, 3372: 3, 3375: 3, 3385: 3, 3386: 3, 3405: 3, 3422: 3, 3429: 3, 3451: 3, 3501: 3, 3503: 3, 3506: 3, 3510: 3, 3526: 3, 3531: 3, 3544: 3, 3550: 3, 3553: 3, 3558: 3, 3565: 3, 3579: 3, 3587: 3, 3591: 3, 3597: 3, 3598: 3, 3602: 3, 3618: 3, 3626: 3, 3636: 3, 3642: 3, 3651: 3, 3676: 3, 3679: 3, 3687: 3, 3711: 3, 3713: 3, 3741: 3, 3742: 3, 3756: 3, 3760: 3, 3773: 3, 3781: 3, 3785: 3, 3787: 3, 3796: 3, 3797: 3, 3799: 3, 3800: 3, 3801: 3, 3811: 3, 3818: 3, 3828: 3, 3833: 3, 3836: 3, 3849: 3, 3870: 3, 3872: 3, 3888: 3, 3889: 3, 3891: 3, 3899: 3, 3904: 3, 3915: 3, 3916: 3, 3917: 3, 3930: 3, 3967: 3, 3969: 3, 3974: 3, 3982: 3, 3990: 3, 4016: 3, 4024: 3, 4033: 3, 4036: 3, 4037: 3, 4049: 3, 4051: 3, 4052: 3, 4061: 3, 4073: 3, 4085: 3, 4089: 3, 4091: 3, 4097: 3, 4098: 3, 4101: 3, 4110: 3, 4124: 3, 4158: 3, 4181: 3, 4197: 3, 4200: 3, 4205: 3, 4212: 3, 4220: 3, 4237: 3, 4245: 3, 4265: 3, 4267: 3, 4269: 3, 4278: 3, 4286: 3, 4311: 3, 4315: 3, 4330: 3, 4340: 3, 4351: 3, 4363: 3, 4366: 3, 4380: 3, 4382: 3, 4383: 3, 4386: 3, 4388: 3, 4389: 3, 4402: 3, 4407: 3, 4411: 3, 4412: 3, 4419: 3, 4432: 3, 4438: 3, 4461: 3, 4462: 3, 4465: 3, 4492: 3, 4497: 3, 4510: 3, 4519: 3, 4535: 3, 4542: 3, 4550: 3, 4552: 3, 4560: 3, 4585: 3, 4596: 3, 4598: 3, 4607: 3, 4619: 3, 4623: 3, 4625: 3, 4628: 3, 4648: 3, 4661: 3, 4673: 3, 4681: 3, 4682: 3, 4686: 3, 4694: 3, 4697: 3, 4700: 3, 4704: 3, 4705: 3, 4717: 3, 4718: 3, 4720: 3, 4742: 3, 4751: 3, 4752: 3, 4759: 3, 4770: 3, 4800: 3, 4820: 3, 4833: 3, 4846: 3, 4847: 3, 4857: 3, 4863: 3, 4865: 3, 4874: 3, 4886: 3, 4893: 3, 4894: 3, 4899: 3, 4915: 3, 4916: 3, 4917: 3, 4923: 3, 4927: 3, 4931: 3, 4934: 3, 4937: 3, 4944: 3, 4953: 3, 4975: 3, 4997: 3, 5008: 3, 5014: 3, 5018: 3, 5029: 3, 5030: 3, 5065: 3, 5068: 3, 5075: 3, 5084: 3, 5091: 3, 5095: 3, 5115: 3, 5133: 3, 5142: 3, 5156: 3, 5165: 3, 5182: 3, 5185: 3, 5196: 3, 5197: 3, 5200: 3, 5207: 3, 5209: 3, 5212: 3, 5213: 3, 5226: 3, 5233: 3, 5239: 3, 5249: 3, 5250: 3, 5252: 3, 5261: 3, 5268: 3, 5271: 3, 5278: 3, 5282: 3, 5296: 3, 5297: 3, 5306: 3, 5316: 3, 5325: 3, 5331: 3, 5333: 3, 5334: 3, 5337: 3, 5339: 3, 5346: 3, 5360: 3, 5366: 3, 5372: 3, 5376: 3, 5379: 3, 5381: 3, 5415: 3, 5419: 3, 5420: 3, 5426: 3, 5428: 3, 5432: 3, 5437: 3, 5438: 3, 5447: 3, 5458: 3, 5464: 3, 5473: 3, 5488: 3, 5494: 3, 5504: 3, 5515: 3, 5519: 3, 5524: 3, 5534: 3, 5542: 3, 5551: 3, 5557: 3, 5559: 3, 5569: 3, 5574: 3, 5586: 3, 5608: 3, 5620: 3, 5626: 3, 5627: 3, 5633: 3, 5635: 3, 5643: 3, 5645: 3, 5657: 3, 5666: 3, 5674: 3, 5675: 3, 5684: 3, 5685: 3, 5690: 3, 5694: 3, 5699: 3, 5700: 3, 5702: 3, 5718: 3, 5732: 3, 5759: 3, 5764: 3, 5767: 3, 5768: 3, 5779: 3, 5788: 3, 5797: 3, 5807: 3, 5815: 3, 5821: 3, 5822: 3, 5830: 3, 5846: 3, 5866: 3, 5869: 3, 5870: 3, 5880: 3, 5881: 3, 5882: 3, 5884: 3, 5891: 3, 5900: 3, 5908: 3, 5917: 3, 5935: 3, 5941: 3, 5945: 3, 5964: 3, 5973: 3, 5986: 3, 5989: 3, 6007: 3, 6010: 3, 6040: 3, 6049: 3, 6055: 3, 6064: 3, 6081: 3, 6100: 3, 6122: 3, 6130: 3, 6132: 3, 6138: 3, 6139: 3, 6144: 3, 6165: 3, 6168: 3, 6191: 3, 6198: 3, 6205: 3, 6206: 3, 6214: 3, 6215: 3, 6226: 3, 6256: 3, 6271: 3, 6289: 3, 6301: 3, 6314: 3, 6315: 3, 6327: 3, 6330: 3, 6346: 3, 6357: 3, 6361: 3, 6362: 3, 6367: 3, 6370: 3, 6373: 3, 6381: 3, 6389: 3, 6399: 3, 6404: 3, 6415: 3, 6422: 3, 6424: 3, 6429: 3, 6441: 3, 6446: 3, 6489: 3, 6491: 3, 6494: 3, 6536: 3, 6543: 3, 6569: 3, 6604: 3, 6638: 3, 6643: 3, 6651: 3, 6660: 3, 6677: 3, 6691: 3, 6693: 3, 6699: 3, 6704: 3, 6712: 3, 6732: 3, 6755: 3, 6760: 3, 6782: 3, 6797: 3, 6803: 3, 6807: 3, 6810: 3, 6820: 3, 6827: 3, 6852: 3, 6871: 3, 6957: 3, 6962: 3, 6973: 3, 6991: 3, 7003: 3, 7013: 3, 7020: 3, 7022: 3, 7028: 3, 7031: 3, 7082: 3, 7089: 3, 7096: 3, 7102: 3, 7123: 3, 7127: 3, 7143: 3, 7161: 3, 7175: 3, 7199: 3, 7204: 3, 7224: 3, 7261: 3, 7265: 3, 7288: 3, 7290: 3, 7291: 3, 7295: 3, 7316: 3, 7323: 3, 7325: 3, 7387: 3, 7398: 3, 7405: 3, 7457: 3, 7461: 3, 7503: 3, 7505: 3, 7511: 3, 7515: 3, 7531: 3, 7562: 3, 7584: 3, 7588: 3, 7599: 3, 7618: 3, 7669: 3, 7689: 3, 7698: 3, 7699: 3, 7703: 3, 7741: 3, 7764: 3, 7776: 3, 7798: 3, 7828: 3, 7857: 3, 7860: 3, 7967: 3, 8043: 3, 8060: 3, 8067: 3, 8135: 3, 8141: 3, 8167: 3, 8188: 3, 8224: 3, 8229: 3, 8278: 3, 8288: 3, 8295: 3, 8308: 3, 8345: 3, 8418: 3, 8435: 3, 8463: 3, 8539: 3, 8576: 3, 8577: 3, 8711: 3, 8825: 3, 100: 2, 111: 2, 114: 2, 172: 2, 194: 2, 266: 2, 361: 2, 376: 2, 474: 2, 475: 2, 563: 2, 566: 2, 581: 2, 653: 2, 658: 2, 683: 2, 697: 2, 701: 2, 707: 2, 710: 2, 714: 2, 725: 2, 753: 2, 759: 2, 769: 2, 782: 2, 798: 2, 803: 2, 811: 2, 830: 2, 863: 2, 865: 2, 890: 2, 892: 2, 899: 2, 922: 2, 970: 2, 983: 2, 1022: 2, 1031: 2, 1035: 2, 1059: 2, 1060: 2, 1077: 2, 1080: 2, 1135: 2, 1146: 2, 1190: 2, 1213: 2, 1218: 2, 1236: 2, 1257: 2, 1265: 2, 1300: 2, 1302: 2, 1305: 2, 1325: 2, 1345: 2, 1348: 2, 1360: 2, 1365: 2, 1384: 2, 1405: 2, 1418: 2, 1496: 2, 1505: 2, 1514: 2, 1528: 2, 1529: 2, 1558: 2, 1573: 2, 1578: 2, 1587: 2, 1589: 2, 1590: 2, 1591: 2, 1595: 2, 1604: 2, 1623: 2, 1648: 2, 1655: 2, 1658: 2, 1673: 2, 1683: 2, 1690: 2, 1693: 2, 1712: 2, 1723: 2, 1730: 2, 1744: 2, 1760: 2, 1781: 2, 1782: 2, 1803: 2, 1807: 2, 1819: 2, 1824: 2, 1832: 2, 1859: 2, 1862: 2, 1875: 2, 1876: 2, 1903: 2, 1917: 2, 1922: 2, 1942: 2, 1983: 2, 1994: 2, 1997: 2, 2015: 2, 2020: 2, 2024: 2, 2037: 2, 2055: 2, 2059: 2, 2062: 2, 2064: 2, 2085: 2, 2100: 2, 2101: 2, 2106: 2, 2116: 2, 2131: 2, 2158: 2, 2170: 2, 2173: 2, 2176: 2, 2178: 2, 2179: 2, 2184: 2, 2198: 2, 2207: 2, 2213: 2, 2220: 2, 2221: 2, 2225: 2, 2278: 2, 2280: 2, 2286: 2, 2290: 2, 2299: 2, 2304: 2, 2305: 2, 2308: 2, 2320: 2, 2322: 2, 2343: 2, 2345: 2, 2352: 2, 2355: 2, 2361: 2, 2363: 2, 2365: 2, 2369: 2, 2379: 2, 2394: 2, 2396: 2, 2397: 2, 2409: 2, 2411: 2, 2420: 2, 2424: 2, 2428: 2, 2435: 2, 2449: 2, 2451: 2, 2459: 2, 2472: 2, 2475: 2, 2482: 2, 2488: 2, 2512: 2, 2517: 2, 2522: 2, 2523: 2, 2528: 2, 2532: 2, 2573: 2, 2578: 2, 2614: 2, 2645: 2, 2647: 2, 2648: 2, 2655: 2, 2657: 2, 2669: 2, 2675: 2, 2707: 2, 2710: 2, 2714: 2, 2718: 2, 2721: 2, 2732: 2, 2746: 2, 2747: 2, 2757: 2, 2761: 2, 2768: 2, 2771: 2, 2775: 2, 2800: 2, 2811: 2, 2816: 2, 2828: 2, 2830: 2, 2839: 2, 2842: 2, 2849: 2, 2851: 2, 2852: 2, 2855: 2, 2857: 2, 2860: 2, 2874: 2, 2901: 2, 2906: 2, 2917: 2, 2938: 2, 2953: 2, 2956: 2, 2968: 2, 2970: 2, 2972: 2, 2974: 2, 2975: 2, 2976: 2, 2984: 2, 2987: 2, 2998: 2, 3009: 2, 3012: 2, 3019: 2, 3029: 2, 3046: 2, 3053: 2, 3067: 2, 3068: 2, 3072: 2, 3079: 2, 3082: 2, 3084: 2, 3091: 2, 3095: 2, 3102: 2, 3108: 2, 3115: 2, 3125: 2, 3131: 2, 3147: 2, 3159: 2, 3169: 2, 3171: 2, 3180: 2, 3182: 2, 3209: 2, 3222: 2, 3223: 2, 3237: 2, 3242: 2, 3249: 2, 3251: 2, 3253: 2, 3256: 2, 3264: 2, 3268: 2, 3281: 2, 3285: 2, 3304: 2, 3322: 2, 3328: 2, 3336: 2, 3345: 2, 3350: 2, 3358: 2, 3398: 2, 3406: 2, 3408: 2, 3413: 2, 3417: 2, 3418: 2, 3419: 2, 3420: 2, 3421: 2, 3425: 2, 3431: 2, 3434: 2, 3438: 2, 3439: 2, 3440: 2, 3445: 2, 3452: 2, 3456: 2, 3464: 2, 3465: 2, 3467: 2, 3472: 2, 3475: 2, 3478: 2, 3487: 2, 3511: 2, 3514: 2, 3523: 2, 3524: 2, 3536: 2, 3540: 2, 3546: 2, 3548: 2, 3557: 2, 3563: 2, 3566: 2, 3570: 2, 3577: 2, 3578: 2, 3601: 2, 3604: 2, 3605: 2, 3611: 2, 3615: 2, 3617: 2, 3621: 2, 3646: 2, 3648: 2, 3654: 2, 3662: 2, 3686: 2, 3694: 2, 3697: 2, 3698: 2, 3703: 2, 3712: 2, 3734: 2, 3736: 2, 3739: 2, 3748: 2, 3765: 2, 3783: 2, 3805: 2, 3806: 2, 3807: 2, 3816: 2, 3824: 2, 3832: 2, 3837: 2, 3838: 2, 3842: 2, 3844: 2, 3850: 2, 3856: 2, 3857: 2, 3863: 2, 3869: 2, 3873: 2, 3877: 2, 3879: 2, 3882: 2, 3883: 2, 3885: 2, 3898: 2, 3906: 2, 3924: 2, 3931: 2, 3950: 2, 3951: 2, 3991: 2, 4000: 2, 4003: 2, 4007: 2, 4018: 2, 4020: 2, 4029: 2, 4034: 2, 4042: 2, 4044: 2, 4048: 2, 4056: 2, 4063: 2, 4075: 2, 4082: 2, 4083: 2, 4086: 2, 4099: 2, 4100: 2, 4103: 2, 4111: 2, 4130: 2, 4135: 2, 4136: 2, 4141: 2, 4145: 2, 4147: 2, 4149: 2, 4159: 2, 4161: 2, 4168: 2, 4171: 2, 4178: 2, 4184: 2, 4185: 2, 4198: 2, 4202: 2, 4204: 2, 4211: 2, 4218: 2, 4229: 2, 4235: 2, 4236: 2, 4239: 2, 4260: 2, 4261: 2, 4274: 2, 4287: 2, 4290: 2, 4297: 2, 4299: 2, 4314: 2, 4316: 2, 4338: 2, 4342: 2, 4344: 2, 4345: 2, 4346: 2, 4348: 2, 4355: 2, 4361: 2, 4372: 2, 4377: 2, 4378: 2, 4384: 2, 4387: 2, 4403: 2, 4404: 2, 4405: 2, 4413: 2, 4427: 2, 4433: 2, 4435: 2, 4448: 2, 4455: 2, 4456: 2, 4458: 2, 4469: 2, 4470: 2, 4473: 2, 4484: 2, 4485: 2, 4498: 2, 4500: 2, 4511: 2, 4514: 2, 4524: 2, 4527: 2, 4541: 2, 4547: 2, 4554: 2, 4562: 2, 4575: 2, 4577: 2, 4581: 2, 4583: 2, 4586: 2, 4588: 2, 4590: 2, 4591: 2, 4592: 2, 4595: 2, 4599: 2, 4606: 2, 4610: 2, 4614: 2, 4616: 2, 4617: 2, 4630: 2, 4631: 2, 4633: 2, 4635: 2, 4636: 2, 4640: 2, 4644: 2, 4654: 2, 4689: 2, 4702: 2, 4703: 2, 4710: 2, 4714: 2, 4723: 2, 4735: 2, 4741: 2, 4743: 2, 4744: 2, 4745: 2, 4749: 2, 4760: 2, 4761: 2, 4763: 2, 4774: 2, 4775: 2, 4782: 2, 4784: 2, 4787: 2, 4791: 2, 4802: 2, 4805: 2, 4807: 2, 4817: 2, 4823: 2, 4828: 2, 4830: 2, 4836: 2, 4837: 2, 4842: 2, 4843: 2, 4845: 2, 4848: 2, 4853: 2, 4854: 2, 4864: 2, 4867: 2, 4869: 2, 4877: 2, 4878: 2, 4881: 2, 4887: 2, 4888: 2, 4905: 2, 4912: 2, 4928: 2, 4948: 2, 4949: 2, 4954: 2, 4961: 2, 4972: 2, 4978: 2, 4995: 2, 4999: 2, 5002: 2, 5011: 2, 5023: 2, 5024: 2, 5032: 2, 5041: 2, 5049: 2, 5051: 2, 5056: 2, 5057: 2, 5064: 2, 5067: 2, 5083: 2, 5085: 2, 5087: 2, 5094: 2, 5100: 2, 5106: 2, 5108: 2, 5110: 2, 5126: 2, 5138: 2, 5151: 2, 5153: 2, 5155: 2, 5169: 2, 5172: 2, 5189: 2, 5198: 2, 5203: 2, 5204: 2, 5205: 2, 5211: 2, 5216: 2, 5222: 2, 5230: 2, 5251: 2, 5253: 2, 5254: 2, 5255: 2, 5257: 2, 5262: 2, 5276: 2, 5291: 2, 5294: 2, 5299: 2, 5307: 2, 5310: 2, 5314: 2, 5315: 2, 5323: 2, 5329: 2, 5336: 2, 5338: 2, 5342: 2, 5350: 2, 5352: 2, 5355: 2, 5364: 2, 5369: 2, 5383: 2, 5388: 2, 5389: 2, 5399: 2, 5411: 2, 5412: 2, 5413: 2, 5416: 2, 5417: 2, 5421: 2, 5422: 2, 5440: 2, 5449: 2, 5451: 2, 5452: 2, 5466: 2, 5502: 2, 5507: 2, 5513: 2, 5517: 2, 5518: 2, 5520: 2, 5525: 2, 5527: 2, 5533: 2, 5539: 2, 5541: 2, 5545: 2, 5548: 2, 5554: 2, 5558: 2, 5562: 2, 5564: 2, 5566: 2, 5571: 2, 5572: 2, 5582: 2, 5605: 2, 5616: 2, 5625: 2, 5637: 2, 5641: 2, 5642: 2, 5644: 2, 5653: 2, 5658: 2, 5661: 2, 5667: 2, 5669: 2, 5670: 2, 5671: 2, 5679: 2, 5680: 2, 5683: 2, 5703: 2, 5709: 2, 5724: 2, 5725: 2, 5727: 2, 5735: 2, 5738: 2, 5750: 2, 5756: 2, 5760: 2, 5761: 2, 5769: 2, 5770: 2, 5778: 2, 5781: 2, 5782: 2, 5790: 2, 5811: 2, 5816: 2, 5820: 2, 5825: 2, 5828: 2, 5832: 2, 5840: 2, 5841: 2, 5849: 2, 5853: 2, 5856: 2, 5877: 2, 5883: 2, 5887: 2, 5889: 2, 5896: 2, 5910: 2, 5915: 2, 5916: 2, 5927: 2, 5931: 2, 5932: 2, 5947: 2, 5957: 2, 5959: 2, 5960: 2, 5965: 2, 5972: 2, 5976: 2, 5983: 2, 5988: 2, 5991: 2, 5992: 2, 5995: 2, 6013: 2, 6014: 2, 6015: 2, 6020: 2, 6024: 2, 6032: 2, 6044: 2, 6047: 2, 6051: 2, 6056: 2, 6060: 2, 6061: 2, 6067: 2, 6068: 2, 6071: 2, 6111: 2, 6113: 2, 6114: 2, 6124: 2, 6125: 2, 6127: 2, 6128: 2, 6135: 2, 6149: 2, 6154: 2, 6159: 2, 6162: 2, 6166: 2, 6169: 2, 6171: 2, 6186: 2, 6190: 2, 6192: 2, 6194: 2, 6201: 2, 6209: 2, 6211: 2, 6217: 2, 6221: 2, 6227: 2, 6229: 2, 6233: 2, 6237: 2, 6239: 2, 6245: 2, 6260: 2, 6261: 2, 6264: 2, 6273: 2, 6274: 2, 6275: 2, 6277: 2, 6282: 2, 6285: 2, 6288: 2, 6290: 2, 6292: 2, 6298: 2, 6304: 2, 6305: 2, 6306: 2, 6308: 2, 6310: 2, 6311: 2, 6316: 2, 6320: 2, 6323: 2, 6324: 2, 6334: 2, 6339: 2, 6344: 2, 6345: 2, 6351: 2, 6352: 2, 6358: 2, 6360: 2, 6374: 2, 6375: 2, 6380: 2, 6383: 2, 6385: 2, 6396: 2, 6397: 2, 6398: 2, 6434: 2, 6436: 2, 6437: 2, 6440: 2, 6451: 2, 6455: 2, 6459: 2, 6464: 2, 6471: 2, 6472: 2, 6484: 2, 6492: 2, 6496: 2, 6497: 2, 6501: 2, 6503: 2, 6506: 2, 6511: 2, 6512: 2, 6523: 2, 6532: 2, 6541: 2, 6542: 2, 6545: 2, 6549: 2, 6550: 2, 6551: 2, 6555: 2, 6558: 2, 6566: 2, 6572: 2, 6580: 2, 6582: 2, 6587: 2, 6588: 2, 6591: 2, 6602: 2, 6611: 2, 6619: 2, 6621: 2, 6626: 2, 6629: 2, 6631: 2, 6632: 2, 6634: 2, 6650: 2, 6654: 2, 6667: 2, 6670: 2, 6682: 2, 6683: 2, 6686: 2, 6687: 2, 6694: 2, 6697: 2, 6702: 2, 6703: 2, 6706: 2, 6707: 2, 6711: 2, 6720: 2, 6724: 2, 6727: 2, 6730: 2, 6740: 2, 6743: 2, 6749: 2, 6750: 2, 6754: 2, 6756: 2, 6757: 2, 6759: 2, 6773: 2, 6779: 2, 6786: 2, 6788: 2, 6792: 2, 6794: 2, 6802: 2, 6808: 2, 6839: 2, 6847: 2, 6853: 2, 6857: 2, 6859: 2, 6868: 2, 6878: 2, 6900: 2, 6901: 2, 6911: 2, 6921: 2, 6929: 2, 6936: 2, 6941: 2, 6946: 2, 6958: 2, 6982: 2, 6987: 2, 6990: 2, 6992: 2, 7010: 2, 7017: 2, 7018: 2, 7029: 2, 7030: 2, 7036: 2, 7038: 2, 7044: 2, 7048: 2, 7050: 2, 7054: 2, 7068: 2, 7073: 2, 7074: 2, 7098: 2, 7104: 2, 7113: 2, 7115: 2, 7117: 2, 7125: 2, 7142: 2, 7149: 2, 7150: 2, 7167: 2, 7169: 2, 7192: 2, 7197: 2, 7201: 2, 7206: 2, 7208: 2, 7211: 2, 7216: 2, 7217: 2, 7223: 2, 7227: 2, 7229: 2, 7231: 2, 7239: 2, 7240: 2, 7242: 2, 7243: 2, 7247: 2, 7248: 2, 7250: 2, 7256: 2, 7258: 2, 7259: 2, 7266: 2, 7273: 2, 7280: 2, 7294: 2, 7297: 2, 7299: 2, 7300: 2, 7306: 2, 7310: 2, 7319: 2, 7328: 2, 7330: 2, 7342: 2, 7343: 2, 7357: 2, 7359: 2, 7371: 2, 7372: 2, 7373: 2, 7377: 2, 7378: 2, 7379: 2, 7388: 2, 7395: 2, 7397: 2, 7413: 2, 7416: 2, 7421: 2, 7424: 2, 7434: 2, 7436: 2, 7439: 2, 7448: 2, 7451: 2, 7459: 2, 7462: 2, 7463: 2, 7472: 2, 7473: 2, 7478: 2, 7479: 2, 7490: 2, 7501: 2, 7509: 2, 7510: 2, 7522: 2, 7524: 2, 7550: 2, 7564: 2, 7567: 2, 7573: 2, 7576: 2, 7577: 2, 7610: 2, 7612: 2, 7614: 2, 7617: 2, 7623: 2, 7627: 2, 7635: 2, 7636: 2, 7644: 2, 7645: 2, 7651: 2, 7657: 2, 7661: 2, 7663: 2, 7665: 2, 7676: 2, 7687: 2, 7691: 2, 7693: 2, 7705: 2, 7709: 2, 7711: 2, 7725: 2, 7726: 2, 7740: 2, 7742: 2, 7744: 2, 7745: 2, 7757: 2, 7759: 2, 7762: 2, 7765: 2, 7767: 2, 7772: 2, 7789: 2, 7791: 2, 7795: 2, 7800: 2, 7818: 2, 7846: 2, 7847: 2, 7848: 2, 7868: 2, 7876: 2, 7879: 2, 7895: 2, 7899: 2, 7910: 2, 7918: 2, 7941: 2, 7946: 2, 7953: 2, 7955: 2, 7956: 2, 7963: 2, 7978: 2, 7981: 2, 7984: 2, 7990: 2, 7993: 2, 7996: 2, 8000: 2, 8016: 2, 8017: 2, 8018: 2, 8029: 2, 8030: 2, 8037: 2, 8040: 2, 8051: 2, 8053: 2, 8055: 2, 8065: 2, 8082: 2, 8095: 2, 8108: 2, 8117: 2, 8134: 2, 8139: 2, 8153: 2, 8158: 2, 8168: 2, 8169: 2, 8191: 2, 8193: 2, 8197: 2, 8212: 2, 8215: 2, 8228: 2, 8237: 2, 8248: 2, 8263: 2, 8264: 2, 8265: 2, 8271: 2, 8286: 2, 8296: 2, 8300: 2, 8307: 2, 8310: 2, 8318: 2, 8325: 2, 8340: 2, 8348: 2, 8361: 2, 8363: 2, 8374: 2, 8377: 2, 8423: 2, 8424: 2, 8426: 2, 8433: 2, 8436: 2, 8437: 2, 8445: 2, 8450: 2, 8451: 2, 8474: 2, 8494: 2, 8505: 2, 8543: 2, 8557: 2, 8561: 2, 8568: 2, 8569: 2, 8578: 2, 8610: 2, 8632: 2, 8641: 2, 8651: 2, 8672: 2, 8673: 2, 8677: 2, 8698: 2, 8705: 2, 8710: 2, 8733: 2, 8749: 2, 8750: 2, 8761: 2, 8783: 2, 8805: 2, 13: 1, 102: 1, 105: 1, 154: 1, 198: 1, 286: 1, 287: 1, 306: 1, 310: 1, 315: 1, 370: 1, 385: 1, 398: 1, 406: 1, 444: 1, 447: 1, 453: 1, 465: 1, 467: 1, 478: 1, 480: 1, 493: 1, 495: 1, 535: 1, 570: 1, 572: 1, 605: 1, 610: 1, 636: 1, 662: 1, 676: 1, 687: 1, 711: 1, 712: 1, 723: 1, 778: 1, 783: 1, 785: 1, 788: 1, 807: 1, 813: 1, 821: 1, 825: 1, 829: 1, 832: 1, 837: 1, 841: 1, 849: 1, 867: 1, 880: 1, 893: 1, 908: 1, 915: 1, 917: 1, 939: 1, 958: 1, 979: 1, 987: 1, 1000: 1, 1002: 1, 1003: 1, 1021: 1, 1029: 1, 1033: 1, 1052: 1, 1053: 1, 1076: 1, 1078: 1, 1087: 1, 1103: 1, 1122: 1, 1134: 1, 1164: 1, 1170: 1, 1171: 1, 1174: 1, 1180: 1, 1188: 1, 1189: 1, 1191: 1, 1195: 1, 1199: 1, 1203: 1, 1237: 1, 1242: 1, 1250: 1, 1256: 1, 1262: 1, 1284: 1, 1285: 1, 1286: 1, 1287: 1, 1288: 1, 1294: 1, 1295: 1, 1301: 1, 1304: 1, 1307: 1, 1309: 1, 1316: 1, 1317: 1, 1320: 1, 1338: 1, 1353: 1, 1363: 1, 1366: 1, 1393: 1, 1396: 1, 1400: 1, 1401: 1, 1404: 1, 1409: 1, 1410: 1, 1421: 1, 1425: 1, 1427: 1, 1438: 1, 1441: 1, 1461: 1, 1464: 1, 1472: 1, 1480: 1, 1488: 1, 1506: 1, 1509: 1, 1511: 1, 1519: 1, 1520: 1, 1522: 1, 1526: 1, 1527: 1, 1544: 1, 1553: 1, 1557: 1, 1582: 1, 1603: 1, 1607: 1, 1626: 1, 1645: 1, 1651: 1, 1666: 1, 1671: 1, 1674: 1, 1688: 1, 1692: 1, 1701: 1, 1707: 1, 1711: 1, 1725: 1, 1729: 1, 1742: 1, 1743: 1, 1745: 1, 1759: 1, 1762: 1, 1764: 1, 1767: 1, 1775: 1, 1785: 1, 1786: 1, 1788: 1, 1796: 1, 1799: 1, 1805: 1, 1806: 1, 1808: 1, 1811: 1, 1828: 1, 1829: 1, 1834: 1, 1842: 1, 1847: 1, 1854: 1, 1857: 1, 1863: 1, 1865: 1, 1870: 1, 1888: 1, 1890: 1, 1904: 1, 1911: 1, 1929: 1, 1932: 1, 1938: 1, 1940: 1, 1951: 1, 1954: 1, 1959: 1, 1969: 1, 1974: 1, 1975: 1, 1978: 1, 1981: 1, 1985: 1, 2010: 1, 2012: 1, 2017: 1, 2041: 1, 2042: 1, 2048: 1, 2061: 1, 2067: 1, 2070: 1, 2073: 1, 2077: 1, 2081: 1, 2088: 1, 2098: 1, 2102: 1, 2107: 1, 2114: 1, 2115: 1, 2118: 1, 2133: 1, 2136: 1, 2143: 1, 2152: 1, 2155: 1, 2160: 1, 2161: 1, 2164: 1, 2168: 1, 2171: 1, 2200: 1, 2202: 1, 2205: 1, 2208: 1, 2224: 1, 2226: 1, 2237: 1, 2246: 1, 2247: 1, 2254: 1, 2256: 1, 2266: 1, 2268: 1, 2281: 1, 2289: 1, 2295: 1, 2296: 1, 2302: 1, 2307: 1, 2310: 1, 2319: 1, 2325: 1, 2326: 1, 2331: 1, 2339: 1, 2340: 1, 2349: 1, 2350: 1, 2360: 1, 2368: 1, 2371: 1, 2372: 1, 2380: 1, 2381: 1, 2382: 1, 2384: 1, 2387: 1, 2389: 1, 2391: 1, 2392: 1, 2398: 1, 2406: 1, 2407: 1, 2408: 1, 2410: 1, 2413: 1, 2415: 1, 2417: 1, 2422: 1, 2423: 1, 2429: 1, 2432: 1, 2445: 1, 2446: 1, 2455: 1, 2456: 1, 2462: 1, 2486: 1, 2490: 1, 2498: 1, 2499: 1, 2501: 1, 2502: 1, 2503: 1, 2504: 1, 2514: 1, 2515: 1, 2516: 1, 2518: 1, 2519: 1, 2529: 1, 2534: 1, 2547: 1, 2553: 1, 2554: 1, 2560: 1, 2564: 1, 2565: 1, 2566: 1, 2569: 1, 2570: 1, 2580: 1, 2582: 1, 2586: 1, 2591: 1, 2598: 1, 2601: 1, 2602: 1, 2606: 1, 2608: 1, 2613: 1, 2615: 1, 2623: 1, 2634: 1, 2639: 1, 2641: 1, 2643: 1, 2649: 1, 2650: 1, 2654: 1, 2658: 1, 2663: 1, 2677: 1, 2679: 1, 2693: 1, 2704: 1, 2705: 1, 2708: 1, 2713: 1, 2717: 1, 2724: 1, 2725: 1, 2731: 1, 2734: 1, 2739: 1, 2751: 1, 2763: 1, 2764: 1, 2767: 1, 2772: 1, 2780: 1, 2784: 1, 2786: 1, 2792: 1, 2795: 1, 2797: 1, 2810: 1, 2812: 1, 2814: 1, 2821: 1, 2824: 1, 2826: 1, 2829: 1, 2832: 1, 2835: 1, 2836: 1, 2854: 1, 2861: 1, 2862: 1, 2863: 1, 2864: 1, 2869: 1, 2877: 1, 2880: 1, 2881: 1, 2882: 1, 2886: 1, 2888: 1, 2896: 1, 2898: 1, 2902: 1, 2904: 1, 2909: 1, 2911: 1, 2912: 1, 2913: 1, 2914: 1, 2916: 1, 2918: 1, 2923: 1, 2924: 1, 2927: 1, 2929: 1, 2934: 1, 2935: 1, 2937: 1, 2944: 1, 2948: 1, 2959: 1, 2963: 1, 2965: 1, 2971: 1, 2973: 1, 2977: 1, 2983: 1, 2986: 1, 2991: 1, 2999: 1, 3000: 1, 3001: 1, 3004: 1, 3008: 1, 3011: 1, 3018: 1, 3024: 1, 3031: 1, 3032: 1, 3036: 1, 3037: 1, 3039: 1, 3040: 1, 3045: 1, 3047: 1, 3049: 1, 3051: 1, 3062: 1, 3073: 1, 3075: 1, 3076: 1, 3078: 1, 3098: 1, 3106: 1, 3111: 1, 3112: 1, 3118: 1, 3119: 1, 3120: 1, 3121: 1, 3122: 1, 3123: 1, 3127: 1, 3128: 1, 3129: 1, 3132: 1, 3138: 1, 3148: 1, 3149: 1, 3158: 1, 3164: 1, 3166: 1, 3170: 1, 3173: 1, 3174: 1, 3176: 1, 3181: 1, 3183: 1, 3184: 1, 3185: 1, 3186: 1, 3187: 1, 3188: 1, 3189: 1, 3198: 1, 3200: 1, 3204: 1, 3210: 1, 3214: 1, 3216: 1, 3220: 1, 3221: 1, 3224: 1, 3225: 1, 3235: 1, 3238: 1, 3244: 1, 3247: 1, 3248: 1, 3250: 1, 3258: 1, 3266: 1, 3270: 1, 3271: 1, 3275: 1, 3277: 1, 3278: 1, 3279: 1, 3282: 1, 3283: 1, 3287: 1, 3289: 1, 3292: 1, 3295: 1, 3299: 1, 3300: 1, 3301: 1, 3302: 1, 3309: 1, 3311: 1, 3313: 1, 3314: 1, 3318: 1, 3319: 1, 3324: 1, 3325: 1, 3332: 1, 3335: 1, 3338: 1, 3339: 1, 3340: 1, 3346: 1, 3348: 1, 3349: 1, 3351: 1, 3352: 1, 3353: 1, 3355: 1, 3356: 1, 3361: 1, 3365: 1, 3366: 1, 3368: 1, 3373: 1, 3379: 1, 3387: 1, 3388: 1, 3390: 1, 3391: 1, 3394: 1, 3397: 1, 3402: 1, 3403: 1, 3407: 1, 3409: 1, 3410: 1, 3411: 1, 3412: 1, 3415: 1, 3416: 1, 3423: 1, 3426: 1, 3428: 1, 3430: 1, 3432: 1, 3433: 1, 3437: 1, 3441: 1, 3442: 1, 3443: 1, 3444: 1, 3447: 1, 3448: 1, 3457: 1, 3462: 1, 3466: 1, 3469: 1, 3470: 1, 3476: 1, 3480: 1, 3481: 1, 3482: 1, 3483: 1, 3491: 1, 3493: 1, 3495: 1, 3496: 1, 3507: 1, 3509: 1, 3517: 1, 3519: 1, 3522: 1, 3527: 1, 3528: 1, 3537: 1, 3538: 1, 3539: 1, 3541: 1, 3543: 1, 3549: 1, 3551: 1, 3554: 1, 3555: 1, 3559: 1, 3564: 1, 3569: 1, 3575: 1, 3576: 1, 3581: 1, 3582: 1, 3583: 1, 3590: 1, 3594: 1, 3595: 1, 3596: 1, 3606: 1, 3607: 1, 3608: 1, 3609: 1, 3610: 1, 3614: 1, 3619: 1, 3630: 1, 3631: 1, 3632: 1, 3633: 1, 3639: 1, 3640: 1, 3641: 1, 3647: 1, 3650: 1, 3653: 1, 3655: 1, 3656: 1, 3659: 1, 3661: 1, 3667: 1, 3669: 1, 3671: 1, 3672: 1, 3674: 1, 3678: 1, 3682: 1, 3688: 1, 3689: 1, 3690: 1, 3691: 1, 3692: 1, 3693: 1, 3695: 1, 3709: 1, 3715: 1, 3716: 1, 3719: 1, 3721: 1, 3722: 1, 3725: 1, 3726: 1, 3727: 1, 3729: 1, 3733: 1, 3735: 1, 3747: 1, 3750: 1, 3752: 1, 3755: 1, 3757: 1, 3761: 1, 3762: 1, 3764: 1, 3766: 1, 3767: 1, 3768: 1, 3776: 1, 3777: 1, 3780: 1, 3784: 1, 3786: 1, 3789: 1, 3790: 1, 3793: 1, 3808: 1, 3809: 1, 3810: 1, 3813: 1, 3814: 1, 3815: 1, 3817: 1, 3819: 1, 3820: 1, 3825: 1, 3826: 1, 3827: 1, 3829: 1, 3834: 1, 3839: 1, 3841: 1, 3843: 1, 3845: 1, 3847: 1, 3851: 1, 3852: 1, 3854: 1, 3862: 1, 3866: 1, 3868: 1, 3875: 1, 3876: 1, 3880: 1, 3886: 1, 3887: 1, 3892: 1, 3893: 1, 3896: 1, 3900: 1, 3901: 1, 3912: 1, 3914: 1, 3926: 1, 3929: 1, 3932: 1, 3933: 1, 3934: 1, 3936: 1, 3937: 1, 3939: 1, 3940: 1, 3941: 1, 3943: 1, 3944: 1, 3949: 1, 3953: 1, 3954: 1, 3955: 1, 3956: 1, 3959: 1, 3961: 1, 3963: 1, 3965: 1, 3968: 1, 3970: 1, 3980: 1, 3981: 1, 3983: 1, 3984: 1, 3985: 1, 3987: 1, 3988: 1, 3989: 1, 3994: 1, 3995: 1, 3996: 1, 3998: 1, 4002: 1, 4004: 1, 4005: 1, 4006: 1, 4009: 1, 4012: 1, 4013: 1, 4014: 1, 4019: 1, 4021: 1, 4023: 1, 4026: 1, 4031: 1, 4039: 1, 4040: 1, 4041: 1, 4045: 1, 4050: 1, 4053: 1, 4054: 1, 4055: 1, 4065: 1, 4067: 1, 4079: 1, 4087: 1, 4094: 1, 4102: 1, 4104: 1, 4107: 1, 4112: 1, 4116: 1, 4121: 1, 4122: 1, 4126: 1, 4127: 1, 4128: 1, 4129: 1, 4131: 1, 4134: 1, 4138: 1, 4143: 1, 4144: 1, 4154: 1, 4156: 1, 4157: 1, 4160: 1, 4164: 1, 4165: 1, 4167: 1, 4169: 1, 4173: 1, 4174: 1, 4176: 1, 4177: 1, 4179: 1, 4180: 1, 4182: 1, 4183: 1, 4187: 1, 4188: 1, 4190: 1, 4191: 1, 4194: 1, 4199: 1, 4201: 1, 4203: 1, 4206: 1, 4207: 1, 4210: 1, 4214: 1, 4217: 1, 4219: 1, 4221: 1, 4232: 1, 4233: 1, 4240: 1, 4242: 1, 4243: 1, 4246: 1, 4247: 1, 4253: 1, 4254: 1, 4255: 1, 4256: 1, 4259: 1, 4264: 1, 4270: 1, 4271: 1, 4273: 1, 4277: 1, 4280: 1, 4281: 1, 4283: 1, 4284: 1, 4285: 1, 4288: 1, 4292: 1, 4294: 1, 4307: 1, 4309: 1, 4310: 1, 4312: 1, 4313: 1, 4319: 1, 4320: 1, 4326: 1, 4327: 1, 4328: 1, 4329: 1, 4331: 1, 4335: 1, 4336: 1, 4337: 1, 4339: 1, 4343: 1, 4347: 1, 4350: 1, 4353: 1, 4362: 1, 4364: 1, 4365: 1, 4367: 1, 4368: 1, 4370: 1, 4371: 1, 4373: 1, 4375: 1, 4381: 1, 4385: 1, 4391: 1, 4396: 1, 4398: 1, 4401: 1, 4409: 1, 4410: 1, 4414: 1, 4415: 1, 4417: 1, 4418: 1, 4422: 1, 4424: 1, 4425: 1, 4426: 1, 4429: 1, 4434: 1, 4436: 1, 4437: 1, 4440: 1, 4442: 1, 4445: 1, 4447: 1, 4452: 1, 4454: 1, 4459: 1, 4460: 1, 4464: 1, 4466: 1, 4467: 1, 4472: 1, 4475: 1, 4476: 1, 4477: 1, 4478: 1, 4479: 1, 4480: 1, 4481: 1, 4483: 1, 4486: 1, 4490: 1, 4493: 1, 4505: 1, 4506: 1, 4509: 1, 4512: 1, 4513: 1, 4516: 1, 4520: 1, 4522: 1, 4525: 1, 4526: 1, 4529: 1, 4531: 1, 4533: 1, 4534: 1, 4536: 1, 4538: 1, 4539: 1, 4543: 1, 4544: 1, 4551: 1, 4553: 1, 4555: 1, 4558: 1, 4565: 1, 4566: 1, 4567: 1, 4569: 1, 4570: 1, 4571: 1, 4576: 1, 4579: 1, 4582: 1, 4584: 1, 4587: 1, 4593: 1, 4594: 1, 4600: 1, 4601: 1, 4602: 1, 4603: 1, 4604: 1, 4605: 1, 4609: 1, 4612: 1, 4613: 1, 4620: 1, 4621: 1, 4624: 1, 4637: 1, 4638: 1, 4639: 1, 4641: 1, 4642: 1, 4643: 1, 4646: 1, 4647: 1, 4650: 1, 4655: 1, 4656: 1, 4657: 1, 4662: 1, 4664: 1, 4665: 1, 4668: 1, 4671: 1, 4672: 1, 4675: 1, 4676: 1, 4677: 1, 4680: 1, 4692: 1, 4695: 1, 4698: 1, 4699: 1, 4706: 1, 4707: 1, 4709: 1, 4711: 1, 4713: 1, 4715: 1, 4716: 1, 4719: 1, 4721: 1, 4726: 1, 4727: 1, 4728: 1, 4730: 1, 4731: 1, 4732: 1, 4733: 1, 4734: 1, 4736: 1, 4737: 1, 4738: 1, 4740: 1, 4746: 1, 4747: 1, 4753: 1, 4754: 1, 4755: 1, 4757: 1, 4766: 1, 4768: 1, 4769: 1, 4771: 1, 4773: 1, 4778: 1, 4780: 1, 4788: 1, 4790: 1, 4792: 1, 4793: 1, 4794: 1, 4796: 1, 4798: 1, 4803: 1, 4806: 1, 4808: 1, 4809: 1, 4812: 1, 4813: 1, 4814: 1, 4816: 1, 4818: 1, 4821: 1, 4824: 1, 4825: 1, 4829: 1, 4835: 1, 4839: 1, 4840: 1, 4844: 1, 4851: 1, 4852: 1, 4855: 1, 4858: 1, 4859: 1, 4860: 1, 4866: 1, 4868: 1, 4872: 1, 4880: 1, 4882: 1, 4884: 1, 4890: 1, 4891: 1, 4892: 1, 4895: 1, 4896: 1, 4897: 1, 4898: 1, 4904: 1, 4906: 1, 4908: 1, 4909: 1, 4911: 1, 4913: 1, 4918: 1, 4921: 1, 4924: 1, 4926: 1, 4929: 1, 4932: 1, 4933: 1, 4936: 1, 4938: 1, 4939: 1, 4940: 1, 4941: 1, 4942: 1, 4943: 1, 4946: 1, 4947: 1, 4951: 1, 4952: 1, 4955: 1, 4958: 1, 4959: 1, 4960: 1, 4962: 1, 4964: 1, 4965: 1, 4966: 1, 4967: 1, 4969: 1, 4970: 1, 4971: 1, 4979: 1, 4983: 1, 4986: 1, 4987: 1, 4988: 1, 4989: 1, 4990: 1, 4991: 1, 4992: 1, 4996: 1, 5001: 1, 5003: 1, 5006: 1, 5009: 1, 5010: 1, 5012: 1, 5013: 1, 5015: 1, 5016: 1, 5017: 1, 5021: 1, 5026: 1, 5027: 1, 5028: 1, 5031: 1, 5034: 1, 5035: 1, 5037: 1, 5038: 1, 5039: 1, 5040: 1, 5042: 1, 5043: 1, 5044: 1, 5045: 1, 5046: 1, 5047: 1, 5048: 1, 5050: 1, 5053: 1, 5055: 1, 5058: 1, 5059: 1, 5060: 1, 5061: 1, 5062: 1, 5069: 1, 5074: 1, 5077: 1, 5078: 1, 5080: 1, 5081: 1, 5082: 1, 5086: 1, 5090: 1, 5092: 1, 5096: 1, 5101: 1, 5104: 1, 5105: 1, 5107: 1, 5109: 1, 5111: 1, 5112: 1, 5113: 1, 5116: 1, 5118: 1, 5119: 1, 5120: 1, 5121: 1, 5122: 1, 5124: 1, 5125: 1, 5127: 1, 5128: 1, 5129: 1, 5130: 1, 5131: 1, 5134: 1, 5135: 1, 5136: 1, 5137: 1, 5140: 1, 5141: 1, 5145: 1, 5146: 1, 5147: 1, 5148: 1, 5149: 1, 5150: 1, 5152: 1, 5160: 1, 5161: 1, 5166: 1, 5167: 1, 5168: 1, 5174: 1, 5177: 1, 5179: 1, 5180: 1, 5183: 1, 5184: 1, 5186: 1, 5187: 1, 5201: 1, 5202: 1, 5214: 1, 5218: 1, 5219: 1, 5220: 1, 5223: 1, 5224: 1, 5227: 1, 5229: 1, 5231: 1, 5232: 1, 5235: 1, 5236: 1, 5237: 1, 5240: 1, 5241: 1, 5243: 1, 5246: 1, 5247: 1, 5248: 1, 5256: 1, 5258: 1, 5263: 1, 5265: 1, 5266: 1, 5269: 1, 5270: 1, 5277: 1, 5279: 1, 5283: 1, 5284: 1, 5285: 1, 5286: 1, 5287: 1, 5288: 1, 5289: 1, 5290: 1, 5292: 1, 5293: 1, 5298: 1, 5300: 1, 5301: 1, 5304: 1, 5305: 1, 5308: 1, 5309: 1, 5311: 1, 5313: 1, 5318: 1, 5319: 1, 5320: 1, 5322: 1, 5324: 1, 5327: 1, 5335: 1, 5340: 1, 5341: 1, 5343: 1, 5347: 1, 5351: 1, 5353: 1, 5354: 1, 5356: 1, 5357: 1, 5358: 1, 5359: 1, 5362: 1, 5363: 1, 5365: 1, 5367: 1, 5371: 1, 5373: 1, 5374: 1, 5377: 1, 5378: 1, 5380: 1, 5382: 1, 5384: 1, 5391: 1, 5393: 1, 5394: 1, 5395: 1, 5397: 1, 5400: 1, 5401: 1, 5402: 1, 5404: 1, 5406: 1, 5407: 1, 5409: 1, 5423: 1, 5424: 1, 5429: 1, 5430: 1, 5431: 1, 5436: 1, 5442: 1, 5444: 1, 5445: 1, 5446: 1, 5450: 1, 5454: 1, 5455: 1, 5456: 1, 5457: 1, 5459: 1, 5460: 1, 5463: 1, 5465: 1, 5468: 1, 5469: 1, 5470: 1, 5471: 1, 5472: 1, 5474: 1, 5475: 1, 5479: 1, 5480: 1, 5484: 1, 5486: 1, 5487: 1, 5489: 1, 5491: 1, 5492: 1, 5495: 1, 5496: 1, 5498: 1, 5499: 1, 5500: 1, 5501: 1, 5505: 1, 5509: 1, 5510: 1, 5514: 1, 5521: 1, 5522: 1, 5526: 1, 5529: 1, 5530: 1, 5531: 1, 5535: 1, 5536: 1, 5538: 1, 5540: 1, 5543: 1, 5544: 1, 5549: 1, 5550: 1, 5552: 1, 5553: 1, 5555: 1, 5556: 1, 5561: 1, 5563: 1, 5565: 1, 5567: 1, 5573: 1, 5576: 1, 5577: 1, 5578: 1, 5579: 1, 5580: 1, 5581: 1, 5583: 1, 5587: 1, 5588: 1, 5589: 1, 5590: 1, 5593: 1, 5594: 1, 5595: 1, 5596: 1, 5597: 1, 5598: 1, 5599: 1, 5600: 1, 5603: 1, 5604: 1, 5606: 1, 5607: 1, 5609: 1, 5611: 1, 5612: 1, 5613: 1, 5614: 1, 5617: 1, 5618: 1, 5622: 1, 5623: 1, 5628: 1, 5629: 1, 5630: 1, 5631: 1, 5632: 1, 5634: 1, 5639: 1, 5640: 1, 5646: 1, 5647: 1, 5648: 1, 5649: 1, 5651: 1, 5652: 1, 5654: 1, 5662: 1, 5663: 1, 5665: 1, 5668: 1, 5672: 1, 5673: 1, 5676: 1, 5677: 1, 5682: 1, 5686: 1, 5687: 1, 5688: 1, 5689: 1, 5691: 1, 5692: 1, 5693: 1, 5695: 1, 5696: 1, 5697: 1, 5701: 1, 5704: 1, 5705: 1, 5707: 1, 5708: 1, 5710: 1, 5711: 1, 5714: 1, 5715: 1, 5717: 1, 5719: 1, 5720: 1, 5721: 1, 5723: 1, 5726: 1, 5728: 1, 5729: 1, 5730: 1, 5731: 1, 5733: 1, 5737: 1, 5739: 1, 5740: 1, 5741: 1, 5743: 1, 5744: 1, 5745: 1, 5746: 1, 5747: 1, 5749: 1, 5751: 1, 5753: 1, 5754: 1, 5755: 1, 5757: 1, 5762: 1, 5765: 1, 5766: 1, 5771: 1, 5772: 1, 5773: 1, 5774: 1, 5775: 1, 5776: 1, 5777: 1, 5780: 1, 5783: 1, 5784: 1, 5786: 1, 5787: 1, 5789: 1, 5791: 1, 5792: 1, 5793: 1, 5794: 1, 5795: 1, 5796: 1, 5798: 1, 5799: 1, 5800: 1, 5803: 1, 5804: 1, 5805: 1, 5808: 1, 5810: 1, 5812: 1, 5813: 1, 5814: 1, 5817: 1, 5818: 1, 5819: 1, 5823: 1, 5824: 1, 5826: 1, 5827: 1, 5829: 1, 5831: 1, 5834: 1, 5835: 1, 5836: 1, 5837: 1, 5842: 1, 5843: 1, 5844: 1, 5845: 1, 5848: 1, 5850: 1, 5851: 1, 5852: 1, 5857: 1, 5858: 1, 5859: 1, 5860: 1, 5861: 1, 5862: 1, 5863: 1, 5864: 1, 5865: 1, 5867: 1, 5871: 1, 5875: 1, 5876: 1, 5878: 1, 5879: 1, 5885: 1, 5888: 1, 5890: 1, 5894: 1, 5895: 1, 5897: 1, 5898: 1, 5899: 1, 5901: 1, 5902: 1, 5904: 1, 5907: 1, 5909: 1, 5911: 1, 5912: 1, 5913: 1, 5918: 1, 5919: 1, 5921: 1, 5923: 1, 5924: 1, 5926: 1, 5928: 1, 5929: 1, 5930: 1, 5933: 1, 5934: 1, 5936: 1, 5937: 1, 5938: 1, 5939: 1, 5940: 1, 5942: 1, 5943: 1, 5944: 1, 5950: 1, 5951: 1, 5953: 1, 5954: 1, 5955: 1, 5956: 1, 5958: 1, 5961: 1, 5962: 1, 5963: 1, 5966: 1, 5967: 1, 5969: 1, 5970: 1, 5971: 1, 5974: 1, 5975: 1, 5978: 1, 5981: 1, 5982: 1, 5984: 1, 5987: 1, 5990: 1, 5993: 1, 5994: 1, 5996: 1, 5997: 1, 5999: 1, 6000: 1, 6001: 1, 6003: 1, 6004: 1, 6008: 1, 6009: 1, 6016: 1, 6017: 1, 6019: 1, 6021: 1, 6025: 1, 6027: 1, 6028: 1, 6029: 1, 6030: 1, 6031: 1, 6033: 1, 6035: 1, 6038: 1, 6042: 1, 6045: 1, 6046: 1, 6048: 1, 6050: 1, 6052: 1, 6053: 1, 6054: 1, 6062: 1, 6063: 1, 6065: 1, 6066: 1, 6069: 1, 6070: 1, 6072: 1, 6073: 1, 6074: 1, 6076: 1, 6077: 1, 6078: 1, 6079: 1, 6080: 1, 6082: 1, 6083: 1, 6084: 1, 6086: 1, 6088: 1, 6089: 1, 6090: 1, 6091: 1, 6092: 1, 6093: 1, 6095: 1, 6097: 1, 6098: 1, 6099: 1, 6101: 1, 6102: 1, 6103: 1, 6105: 1, 6106: 1, 6107: 1, 6108: 1, 6109: 1, 6110: 1, 6115: 1, 6116: 1, 6118: 1, 6119: 1, 6120: 1, 6123: 1, 6126: 1, 6131: 1, 6136: 1, 6140: 1, 6141: 1, 6142: 1, 6145: 1, 6147: 1, 6148: 1, 6150: 1, 6151: 1, 6152: 1, 6155: 1, 6156: 1, 6157: 1, 6160: 1, 6161: 1, 6163: 1, 6164: 1, 6167: 1, 6170: 1, 6172: 1, 6173: 1, 6174: 1, 6176: 1, 6177: 1, 6179: 1, 6180: 1, 6181: 1, 6182: 1, 6183: 1, 6185: 1, 6187: 1, 6188: 1, 6189: 1, 6193: 1, 6195: 1, 6197: 1, 6199: 1, 6200: 1, 6202: 1, 6204: 1, 6208: 1, 6210: 1, 6212: 1, 6213: 1, 6216: 1, 6218: 1, 6219: 1, 6220: 1, 6222: 1, 6223: 1, 6224: 1, 6225: 1, 6230: 1, 6231: 1, 6232: 1, 6234: 1, 6235: 1, 6236: 1, 6238: 1, 6240: 1, 6241: 1, 6242: 1, 6243: 1, 6244: 1, 6246: 1, 6247: 1, 6248: 1, 6249: 1, 6250: 1, 6251: 1, 6252: 1, 6254: 1, 6255: 1, 6257: 1, 6258: 1, 6262: 1, 6263: 1, 6265: 1, 6266: 1, 6267: 1, 6268: 1, 6272: 1, 6276: 1, 6278: 1, 6279: 1, 6280: 1, 6281: 1, 6283: 1, 6284: 1, 6286: 1, 6287: 1, 6291: 1, 6293: 1, 6294: 1, 6295: 1, 6296: 1, 6297: 1, 6299: 1, 6300: 1, 6303: 1, 6309: 1, 6312: 1, 6313: 1, 6317: 1, 6318: 1, 6319: 1, 6321: 1, 6325: 1, 6326: 1, 6329: 1, 6331: 1, 6332: 1, 6333: 1, 6335: 1, 6336: 1, 6337: 1, 6338: 1, 6340: 1, 6341: 1, 6342: 1, 6343: 1, 6348: 1, 6350: 1, 6353: 1, 6354: 1, 6355: 1, 6356: 1, 6365: 1, 6366: 1, 6368: 1, 6371: 1, 6372: 1, 6376: 1, 6377: 1, 6378: 1, 6382: 1, 6384: 1, 6387: 1, 6388: 1, 6390: 1, 6392: 1, 6393: 1, 6394: 1, 6395: 1, 6400: 1, 6401: 1, 6403: 1, 6405: 1, 6408: 1, 6409: 1, 6410: 1, 6411: 1, 6412: 1, 6413: 1, 6414: 1, 6416: 1, 6417: 1, 6418: 1, 6419: 1, 6420: 1, 6423: 1, 6425: 1, 6426: 1, 6427: 1, 6428: 1, 6430: 1, 6431: 1, 6432: 1, 6433: 1, 6435: 1, 6438: 1, 6439: 1, 6445: 1, 6447: 1, 6448: 1, 6449: 1, 6450: 1, 6452: 1, 6453: 1, 6454: 1, 6456: 1, 6457: 1, 6458: 1, 6460: 1, 6461: 1, 6462: 1, 6463: 1, 6465: 1, 6469: 1, 6470: 1, 6473: 1, 6474: 1, 6475: 1, 6476: 1, 6477: 1, 6479: 1, 6480: 1, 6482: 1, 6485: 1, 6486: 1, 6487: 1, 6488: 1, 6490: 1, 6499: 1, 6500: 1, 6502: 1, 6504: 1, 6505: 1, 6507: 1, 6508: 1, 6509: 1, 6513: 1, 6514: 1, 6515: 1, 6516: 1, 6517: 1, 6518: 1, 6519: 1, 6520: 1, 6521: 1, 6522: 1, 6524: 1, 6525: 1, 6526: 1, 6527: 1, 6528: 1, 6529: 1, 6530: 1, 6531: 1, 6533: 1, 6535: 1, 6538: 1, 6539: 1, 6540: 1, 6544: 1, 6546: 1, 6547: 1, 6548: 1, 6552: 1, 6553: 1, 6554: 1, 6556: 1, 6557: 1, 6559: 1, 6560: 1, 6561: 1, 6563: 1, 6564: 1, 6565: 1, 6567: 1, 6568: 1, 6570: 1, 6571: 1, 6574: 1, 6575: 1, 6578: 1, 6579: 1, 6581: 1, 6583: 1, 6584: 1, 6585: 1, 6586: 1, 6589: 1, 6590: 1, 6593: 1, 6594: 1, 6597: 1, 6598: 1, 6599: 1, 6600: 1, 6601: 1, 6603: 1, 6605: 1, 6606: 1, 6612: 1, 6613: 1, 6614: 1, 6615: 1, 6616: 1, 6617: 1, 6618: 1, 6624: 1, 6625: 1, 6627: 1, 6628: 1, 6633: 1, 6635: 1, 6636: 1, 6637: 1, 6639: 1, 6640: 1, 6641: 1, 6642: 1, 6644: 1, 6646: 1, 6647: 1, 6648: 1, 6649: 1, 6652: 1, 6653: 1, 6655: 1, 6656: 1, 6657: 1, 6658: 1, 6662: 1, 6663: 1, 6664: 1, 6665: 1, 6668: 1, 6669: 1, 6671: 1, 6673: 1, 6674: 1, 6676: 1, 6678: 1, 6680: 1, 6681: 1, 6684: 1, 6685: 1, 6688: 1, 6692: 1, 6696: 1, 6698: 1, 6700: 1, 6701: 1, 6705: 1, 6708: 1, 6709: 1, 6710: 1, 6713: 1, 6715: 1, 6716: 1, 6717: 1, 6718: 1, 6719: 1, 6721: 1, 6722: 1, 6723: 1, 6725: 1, 6728: 1, 6729: 1, 6731: 1, 6733: 1, 6734: 1, 6735: 1, 6736: 1, 6737: 1, 6738: 1, 6739: 1, 6741: 1, 6742: 1, 6744: 1, 6745: 1, 6746: 1, 6747: 1, 6748: 1, 6751: 1, 6752: 1, 6753: 1, 6758: 1, 6761: 1, 6762: 1, 6763: 1, 6764: 1, 6765: 1, 6767: 1, 6768: 1, 6769: 1, 6770: 1, 6771: 1, 6772: 1, 6776: 1, 6777: 1, 6778: 1, 6781: 1, 6784: 1, 6785: 1, 6787: 1, 6789: 1, 6790: 1, 6795: 1, 6796: 1, 6798: 1, 6799: 1, 6800: 1, 6801: 1, 6805: 1, 6806: 1, 6809: 1, 6812: 1, 6815: 1, 6816: 1, 6817: 1, 6818: 1, 6819: 1, 6821: 1, 6822: 1, 6823: 1, 6824: 1, 6825: 1, 6826: 1, 6828: 1, 6829: 1, 6830: 1, 6831: 1, 6832: 1, 6833: 1, 6834: 1, 6835: 1, 6836: 1, 6837: 1, 6838: 1, 6840: 1, 6841: 1, 6842: 1, 6844: 1, 6845: 1, 6846: 1, 6848: 1, 6849: 1, 6850: 1, 6851: 1, 6854: 1, 6855: 1, 6856: 1, 6858: 1, 6860: 1, 6864: 1, 6865: 1, 6866: 1, 6867: 1, 6869: 1, 6872: 1, 6874: 1, 6875: 1, 6876: 1, 6877: 1, 6879: 1, 6880: 1, 6881: 1, 6882: 1, 6885: 1, 6886: 1, 6887: 1, 6888: 1, 6889: 1, 6890: 1, 6891: 1, 6892: 1, 6893: 1, 6894: 1, 6895: 1, 6896: 1, 6897: 1, 6898: 1, 6899: 1, 6902: 1, 6903: 1, 6904: 1, 6905: 1, 6907: 1, 6908: 1, 6909: 1, 6910: 1, 6912: 1, 6913: 1, 6914: 1, 6915: 1, 6916: 1, 6917: 1, 6918: 1, 6920: 1, 6922: 1, 6923: 1, 6924: 1, 6925: 1, 6926: 1, 6927: 1, 6930: 1, 6931: 1, 6933: 1, 6934: 1, 6935: 1, 6937: 1, 6939: 1, 6940: 1, 6942: 1, 6943: 1, 6944: 1, 6945: 1, 6947: 1, 6948: 1, 6949: 1, 6950: 1, 6951: 1, 6952: 1, 6953: 1, 6954: 1, 6955: 1, 6956: 1, 6960: 1, 6961: 1, 6963: 1, 6964: 1, 6965: 1, 6966: 1, 6967: 1, 6968: 1, 6969: 1, 6970: 1, 6971: 1, 6972: 1, 6975: 1, 6977: 1, 6978: 1, 6979: 1, 6980: 1, 6981: 1, 6983: 1, 6984: 1, 6985: 1, 6986: 1, 6988: 1, 6989: 1, 6993: 1, 6995: 1, 6996: 1, 6997: 1, 6998: 1, 6999: 1, 7000: 1, 7001: 1, 7002: 1, 7004: 1, 7006: 1, 7008: 1, 7009: 1, 7011: 1, 7012: 1, 7014: 1, 7015: 1, 7016: 1, 7019: 1, 7021: 1, 7024: 1, 7025: 1, 7026: 1, 7027: 1, 7032: 1, 7033: 1, 7034: 1, 7035: 1, 7037: 1, 7039: 1, 7040: 1, 7041: 1, 7042: 1, 7043: 1, 7045: 1, 7046: 1, 7047: 1, 7049: 1, 7052: 1, 7055: 1, 7056: 1, 7057: 1, 7058: 1, 7060: 1, 7061: 1, 7062: 1, 7064: 1, 7066: 1, 7067: 1, 7069: 1, 7070: 1, 7071: 1, 7072: 1, 7075: 1, 7077: 1, 7080: 1, 7081: 1, 7083: 1, 7084: 1, 7086: 1, 7087: 1, 7088: 1, 7090: 1, 7092: 1, 7093: 1, 7094: 1, 7095: 1, 7097: 1, 7099: 1, 7100: 1, 7101: 1, 7103: 1, 7105: 1, 7106: 1, 7107: 1, 7108: 1, 7109: 1, 7110: 1, 7111: 1, 7114: 1, 7116: 1, 7118: 1, 7119: 1, 7120: 1, 7121: 1, 7124: 1, 7126: 1, 7128: 1, 7129: 1, 7130: 1, 7132: 1, 7133: 1, 7134: 1, 7135: 1, 7136: 1, 7139: 1, 7140: 1, 7141: 1, 7144: 1, 7145: 1, 7147: 1, 7148: 1, 7151: 1, 7152: 1, 7153: 1, 7154: 1, 7156: 1, 7157: 1, 7158: 1, 7159: 1, 7160: 1, 7162: 1, 7163: 1, 7164: 1, 7165: 1, 7166: 1, 7168: 1, 7170: 1, 7171: 1, 7172: 1, 7173: 1, 7174: 1, 7176: 1, 7180: 1, 7181: 1, 7182: 1, 7183: 1, 7184: 1, 7185: 1, 7186: 1, 7187: 1, 7188: 1, 7189: 1, 7190: 1, 7191: 1, 7193: 1, 7194: 1, 7195: 1, 7196: 1, 7198: 1, 7200: 1, 7202: 1, 7203: 1, 7205: 1, 7207: 1, 7209: 1, 7210: 1, 7212: 1, 7213: 1, 7214: 1, 7215: 1, 7218: 1, 7219: 1, 7220: 1, 7221: 1, 7222: 1, 7225: 1, 7226: 1, 7228: 1, 7230: 1, 7232: 1, 7233: 1, 7234: 1, 7235: 1, 7236: 1, 7237: 1, 7238: 1, 7241: 1, 7244: 1, 7245: 1, 7249: 1, 7251: 1, 7252: 1, 7253: 1, 7254: 1, 7255: 1, 7257: 1, 7260: 1, 7262: 1, 7263: 1, 7264: 1, 7267: 1, 7268: 1, 7270: 1, 7271: 1, 7272: 1, 7274: 1, 7275: 1, 7276: 1, 7277: 1, 7278: 1, 7279: 1, 7281: 1, 7282: 1, 7283: 1, 7284: 1, 7286: 1, 7287: 1, 7289: 1, 7296: 1, 7298: 1, 7301: 1, 7302: 1, 7303: 1, 7304: 1, 7307: 1, 7308: 1, 7309: 1, 7311: 1, 7312: 1, 7313: 1, 7314: 1, 7315: 1, 7317: 1, 7318: 1, 7320: 1, 7321: 1, 7322: 1, 7324: 1, 7326: 1, 7327: 1, 7329: 1, 7331: 1, 7332: 1, 7333: 1, 7334: 1, 7336: 1, 7337: 1, 7338: 1, 7339: 1, 7341: 1, 7344: 1, 7345: 1, 7346: 1, 7347: 1, 7348: 1, 7349: 1, 7350: 1, 7351: 1, 7352: 1, 7353: 1, 7354: 1, 7355: 1, 7356: 1, 7358: 1, 7360: 1, 7361: 1, 7362: 1, 7363: 1, 7364: 1, 7365: 1, 7366: 1, 7367: 1, 7368: 1, 7369: 1, 7370: 1, 7374: 1, 7375: 1, 7376: 1, 7381: 1, 7382: 1, 7383: 1, 7384: 1, 7385: 1, 7386: 1, 7389: 1, 7390: 1, 7391: 1, 7392: 1, 7393: 1, 7394: 1, 7396: 1, 7399: 1, 7400: 1, 7401: 1, 7402: 1, 7404: 1, 7406: 1, 7407: 1, 7408: 1, 7409: 1, 7410: 1, 7411: 1, 7412: 1, 7414: 1, 7415: 1, 7417: 1, 7418: 1, 7419: 1, 7420: 1, 7422: 1, 7423: 1, 7425: 1, 7426: 1, 7427: 1, 7428: 1, 7429: 1, 7430: 1, 7431: 1, 7432: 1, 7433: 1, 7435: 1, 7437: 1, 7440: 1, 7441: 1, 7442: 1, 7443: 1, 7444: 1, 7445: 1, 7446: 1, 7447: 1, 7449: 1, 7450: 1, 7453: 1, 7454: 1, 7455: 1, 7460: 1, 7464: 1, 7465: 1, 7466: 1, 7467: 1, 7468: 1, 7469: 1, 7470: 1, 7471: 1, 7474: 1, 7475: 1, 7476: 1, 7477: 1, 7481: 1, 7482: 1, 7483: 1, 7484: 1, 7485: 1, 7486: 1, 7487: 1, 7488: 1, 7489: 1, 7491: 1, 7492: 1, 7494: 1, 7495: 1, 7496: 1, 7497: 1, 7498: 1, 7499: 1, 7500: 1, 7502: 1, 7504: 1, 7506: 1, 7507: 1, 7508: 1, 7512: 1, 7514: 1, 7516: 1, 7517: 1, 7518: 1, 7519: 1, 7520: 1, 7521: 1, 7523: 1, 7525: 1, 7526: 1, 7527: 1, 7528: 1, 7529: 1, 7532: 1, 7533: 1, 7534: 1, 7535: 1, 7536: 1, 7537: 1, 7538: 1, 7539: 1, 7540: 1, 7541: 1, 7542: 1, 7543: 1, 7544: 1, 7545: 1, 7546: 1, 7547: 1, 7548: 1, 7549: 1, 7551: 1, 7552: 1, 7553: 1, 7554: 1, 7555: 1, 7556: 1, 7557: 1, 7558: 1, 7559: 1, 7560: 1, 7561: 1, 7563: 1, 7565: 1, 7566: 1, 7568: 1, 7569: 1, 7570: 1, 7574: 1, 7575: 1, 7578: 1, 7579: 1, 7580: 1, 7581: 1, 7582: 1, 7583: 1, 7585: 1, 7586: 1, 7587: 1, 7589: 1, 7590: 1, 7591: 1, 7592: 1, 7594: 1, 7595: 1, 7596: 1, 7597: 1, 7598: 1, 7600: 1, 7601: 1, 7602: 1, 7603: 1, 7604: 1, 7605: 1, 7606: 1, 7609: 1, 7611: 1, 7613: 1, 7615: 1, 7616: 1, 7619: 1, 7620: 1, 7621: 1, 7622: 1, 7624: 1, 7625: 1, 7626: 1, 7628: 1, 7629: 1, 7630: 1, 7631: 1, 7632: 1, 7633: 1, 7634: 1, 7637: 1, 7638: 1, 7639: 1, 7640: 1, 7641: 1, 7642: 1, 7643: 1, 7646: 1, 7649: 1, 7650: 1, 7652: 1, 7653: 1, 7654: 1, 7655: 1, 7656: 1, 7658: 1, 7659: 1, 7660: 1, 7662: 1, 7664: 1, 7666: 1, 7667: 1, 7668: 1, 7670: 1, 7671: 1, 7672: 1, 7673: 1, 7674: 1, 7678: 1, 7679: 1, 7680: 1, 7681: 1, 7682: 1, 7683: 1, 7684: 1, 7685: 1, 7686: 1, 7688: 1, 7690: 1, 7692: 1, 7694: 1, 7695: 1, 7696: 1, 7697: 1, 7700: 1, 7701: 1, 7702: 1, 7704: 1, 7706: 1, 7707: 1, 7708: 1, 7710: 1, 7712: 1, 7714: 1, 7715: 1, 7716: 1, 7717: 1, 7718: 1, 7719: 1, 7720: 1, 7721: 1, 7722: 1, 7723: 1, 7724: 1, 7727: 1, 7728: 1, 7729: 1, 7730: 1, 7731: 1, 7732: 1, 7733: 1, 7734: 1, 7735: 1, 7736: 1, 7737: 1, 7738: 1, 7739: 1, 7743: 1, 7746: 1, 7747: 1, 7749: 1, 7750: 1, 7751: 1, 7752: 1, 7753: 1, 7755: 1, 7756: 1, 7758: 1, 7760: 1, 7761: 1, 7763: 1, 7766: 1, 7768: 1, 7769: 1, 7770: 1, 7771: 1, 7773: 1, 7774: 1, 7775: 1, 7777: 1, 7778: 1, 7779: 1, 7780: 1, 7781: 1, 7782: 1, 7783: 1, 7784: 1, 7785: 1, 7786: 1, 7787: 1, 7788: 1, 7790: 1, 7792: 1, 7793: 1, 7794: 1, 7796: 1, 7797: 1, 7799: 1, 7801: 1, 7802: 1, 7803: 1, 7804: 1, 7805: 1, 7806: 1, 7807: 1, 7808: 1, 7809: 1, 7810: 1, 7811: 1, 7812: 1, 7813: 1, 7814: 1, 7815: 1, 7816: 1, 7817: 1, 7819: 1, 7820: 1, 7821: 1, 7822: 1, 7823: 1, 7824: 1, 7825: 1, 7826: 1, 7829: 1, 7830: 1, 7831: 1, 7832: 1, 7833: 1, 7834: 1, 7835: 1, 7836: 1, 7837: 1, 7838: 1, 7839: 1, 7840: 1, 7841: 1, 7842: 1, 7843: 1, 7844: 1, 7845: 1, 7849: 1, 7850: 1, 7851: 1, 7852: 1, 7853: 1, 7854: 1, 7855: 1, 7856: 1, 7858: 1, 7859: 1, 7862: 1, 7863: 1, 7864: 1, 7865: 1, 7866: 1, 7867: 1, 7869: 1, 7871: 1, 7872: 1, 7873: 1, 7875: 1, 7877: 1, 7878: 1, 7880: 1, 7881: 1, 7882: 1, 7883: 1, 7884: 1, 7885: 1, 7886: 1, 7887: 1, 7888: 1, 7890: 1, 7891: 1, 7892: 1, 7893: 1, 7894: 1, 7896: 1, 7897: 1, 7898: 1, 7900: 1, 7901: 1, 7902: 1, 7903: 1, 7904: 1, 7905: 1, 7906: 1, 7907: 1, 7908: 1, 7909: 1, 7911: 1, 7912: 1, 7913: 1, 7914: 1, 7915: 1, 7916: 1, 7917: 1, 7919: 1, 7920: 1, 7921: 1, 7922: 1, 7923: 1, 7924: 1, 7925: 1, 7926: 1, 7927: 1, 7928: 1, 7929: 1, 7930: 1, 7931: 1, 7932: 1, 7933: 1, 7934: 1, 7935: 1, 7936: 1, 7937: 1, 7938: 1, 7939: 1, 7940: 1, 7942: 1, 7943: 1, 7944: 1, 7945: 1, 7948: 1, 7949: 1, 7950: 1, 7951: 1, 7952: 1, 7954: 1, 7957: 1, 7959: 1, 7960: 1, 7961: 1, 7962: 1, 7964: 1, 7965: 1, 7966: 1, 7968: 1, 7969: 1, 7970: 1, 7971: 1, 7972: 1, 7973: 1, 7974: 1, 7975: 1, 7976: 1, 7977: 1, 7979: 1, 7980: 1, 7982: 1, 7983: 1, 7985: 1, 7986: 1, 7987: 1, 7988: 1, 7989: 1, 7991: 1, 7992: 1, 7994: 1, 7995: 1, 7997: 1, 7998: 1, 7999: 1, 8001: 1, 8002: 1, 8003: 1, 8004: 1, 8006: 1, 8007: 1, 8008: 1, 8009: 1, 8010: 1, 8011: 1, 8012: 1, 8013: 1, 8014: 1, 8015: 1, 8019: 1, 8020: 1, 8021: 1, 8022: 1, 8023: 1, 8024: 1, 8025: 1, 8026: 1, 8027: 1, 8031: 1, 8032: 1, 8033: 1, 8034: 1, 8035: 1, 8036: 1, 8038: 1, 8039: 1, 8041: 1, 8042: 1, 8044: 1, 8045: 1, 8046: 1, 8047: 1, 8048: 1, 8049: 1, 8050: 1, 8052: 1, 8054: 1, 8056: 1, 8057: 1, 8058: 1, 8059: 1, 8061: 1, 8062: 1, 8063: 1, 8064: 1, 8066: 1, 8068: 1, 8069: 1, 8070: 1, 8071: 1, 8072: 1, 8073: 1, 8074: 1, 8075: 1, 8076: 1, 8077: 1, 8078: 1, 8079: 1, 8080: 1, 8081: 1, 8083: 1, 8084: 1, 8085: 1, 8086: 1, 8087: 1, 8088: 1, 8089: 1, 8090: 1, 8091: 1, 8092: 1, 8093: 1, 8094: 1, 8096: 1, 8097: 1, 8098: 1, 8099: 1, 8100: 1, 8101: 1, 8102: 1, 8103: 1, 8104: 1, 8105: 1, 8106: 1, 8107: 1, 8109: 1, 8110: 1, 8111: 1, 8112: 1, 8113: 1, 8114: 1, 8115: 1, 8116: 1, 8118: 1, 8119: 1, 8120: 1, 8121: 1, 8122: 1, 8123: 1, 8124: 1, 8125: 1, 8126: 1, 8127: 1, 8128: 1, 8129: 1, 8130: 1, 8131: 1, 8132: 1, 8133: 1, 8136: 1, 8137: 1, 8138: 1, 8140: 1, 8142: 1, 8143: 1, 8144: 1, 8145: 1, 8146: 1, 8147: 1, 8148: 1, 8149: 1, 8150: 1, 8151: 1, 8152: 1, 8154: 1, 8155: 1, 8156: 1, 8157: 1, 8159: 1, 8160: 1, 8161: 1, 8162: 1, 8163: 1, 8164: 1, 8165: 1, 8166: 1, 8170: 1, 8171: 1, 8172: 1, 8173: 1, 8174: 1, 8175: 1, 8177: 1, 8178: 1, 8179: 1, 8180: 1, 8181: 1, 8182: 1, 8183: 1, 8184: 1, 8185: 1, 8186: 1, 8187: 1, 8189: 1, 8190: 1, 8194: 1, 8195: 1, 8196: 1, 8198: 1, 8199: 1, 8200: 1, 8201: 1, 8202: 1, 8203: 1, 8204: 1, 8205: 1, 8206: 1, 8207: 1, 8208: 1, 8209: 1, 8210: 1, 8211: 1, 8213: 1, 8216: 1, 8217: 1, 8218: 1, 8219: 1, 8220: 1, 8221: 1, 8222: 1, 8223: 1, 8225: 1, 8226: 1, 8227: 1, 8230: 1, 8231: 1, 8232: 1, 8233: 1, 8234: 1, 8235: 1, 8236: 1, 8238: 1, 8239: 1, 8240: 1, 8241: 1, 8242: 1, 8243: 1, 8244: 1, 8245: 1, 8246: 1, 8247: 1, 8249: 1, 8250: 1, 8251: 1, 8252: 1, 8253: 1, 8254: 1, 8255: 1, 8256: 1, 8257: 1, 8258: 1, 8259: 1, 8260: 1, 8261: 1, 8262: 1, 8266: 1, 8267: 1, 8268: 1, 8269: 1, 8270: 1, 8272: 1, 8273: 1, 8274: 1, 8275: 1, 8276: 1, 8277: 1, 8279: 1, 8280: 1, 8281: 1, 8282: 1, 8284: 1, 8285: 1, 8287: 1, 8289: 1, 8290: 1, 8291: 1, 8292: 1, 8293: 1, 8294: 1, 8297: 1, 8298: 1, 8299: 1, 8301: 1, 8302: 1, 8303: 1, 8304: 1, 8305: 1, 8306: 1, 8309: 1, 8311: 1, 8312: 1, 8313: 1, 8314: 1, 8315: 1, 8316: 1, 8317: 1, 8319: 1, 8320: 1, 8321: 1, 8322: 1, 8323: 1, 8324: 1, 8326: 1, 8327: 1, 8329: 1, 8330: 1, 8331: 1, 8332: 1, 8333: 1, 8334: 1, 8335: 1, 8337: 1, 8338: 1, 8339: 1, 8341: 1, 8342: 1, 8343: 1, 8344: 1, 8346: 1, 8347: 1, 8349: 1, 8350: 1, 8351: 1, 8352: 1, 8353: 1, 8354: 1, 8355: 1, 8356: 1, 8357: 1, 8358: 1, 8359: 1, 8360: 1, 8362: 1, 8364: 1, 8365: 1, 8366: 1, 8367: 1, 8368: 1, 8369: 1, 8370: 1, 8371: 1, 8372: 1, 8373: 1, 8375: 1, 8376: 1, 8378: 1, 8379: 1, 8380: 1, 8381: 1, 8382: 1, 8383: 1, 8384: 1, 8385: 1, 8386: 1, 8387: 1, 8388: 1, 8389: 1, 8390: 1, 8391: 1, 8392: 1, 8393: 1, 8394: 1, 8395: 1, 8396: 1, 8397: 1, 8399: 1, 8400: 1, 8401: 1, 8402: 1, 8403: 1, 8404: 1, 8405: 1, 8406: 1, 8407: 1, 8408: 1, 8409: 1, 8410: 1, 8411: 1, 8412: 1, 8413: 1, 8414: 1, 8415: 1, 8416: 1, 8417: 1, 8419: 1, 8420: 1, 8421: 1, 8422: 1, 8425: 1, 8427: 1, 8428: 1, 8429: 1, 8430: 1, 8431: 1, 8432: 1, 8434: 1, 8438: 1, 8439: 1, 8440: 1, 8441: 1, 8442: 1, 8443: 1, 8444: 1, 8446: 1, 8447: 1, 8448: 1, 8449: 1, 8452: 1, 8453: 1, 8454: 1, 8455: 1, 8456: 1, 8457: 1, 8458: 1, 8459: 1, 8460: 1, 8461: 1, 8462: 1, 8464: 1, 8465: 1, 8466: 1, 8467: 1, 8468: 1, 8469: 1, 8470: 1, 8471: 1, 8472: 1, 8473: 1, 8475: 1, 8476: 1, 8477: 1, 8478: 1, 8479: 1, 8480: 1, 8481: 1, 8482: 1, 8483: 1, 8484: 1, 8487: 1, 8488: 1, 8489: 1, 8490: 1, 8491: 1, 8492: 1, 8493: 1, 8495: 1, 8496: 1, 8497: 1, 8499: 1, 8500: 1, 8501: 1, 8502: 1, 8503: 1, 8504: 1, 8506: 1, 8507: 1, 8508: 1, 8509: 1, 8510: 1, 8511: 1, 8512: 1, 8513: 1, 8514: 1, 8515: 1, 8516: 1, 8517: 1, 8518: 1, 8519: 1, 8520: 1, 8521: 1, 8522: 1, 8523: 1, 8524: 1, 8525: 1, 8526: 1, 8527: 1, 8528: 1, 8529: 1, 8530: 1, 8531: 1, 8532: 1, 8533: 1, 8534: 1, 8535: 1, 8536: 1, 8537: 1, 8538: 1, 8540: 1, 8541: 1, 8542: 1, 8544: 1, 8545: 1, 8546: 1, 8547: 1, 8548: 1, 8549: 1, 8550: 1, 8551: 1, 8552: 1, 8553: 1, 8554: 1, 8555: 1, 8556: 1, 8558: 1, 8559: 1, 8560: 1, 8562: 1, 8563: 1, 8564: 1, 8565: 1, 8566: 1, 8567: 1, 8570: 1, 8571: 1, 8572: 1, 8573: 1, 8574: 1, 8575: 1, 8579: 1, 8580: 1, 8581: 1, 8582: 1, 8583: 1, 8584: 1, 8586: 1, 8587: 1, 8588: 1, 8589: 1, 8590: 1, 8591: 1, 8592: 1, 8593: 1, 8594: 1, 8595: 1, 8596: 1, 8597: 1, 8598: 1, 8599: 1, 8600: 1, 8601: 1, 8602: 1, 8603: 1, 8604: 1, 8605: 1, 8606: 1, 8607: 1, 8608: 1, 8609: 1, 8611: 1, 8612: 1, 8613: 1, 8614: 1, 8615: 1, 8616: 1, 8617: 1, 8618: 1, 8619: 1, 8620: 1, 8621: 1, 8622: 1, 8623: 1, 8624: 1, 8625: 1, 8626: 1, 8627: 1, 8628: 1, 8629: 1, 8630: 1, 8631: 1, 8633: 1, 8634: 1, 8635: 1, 8636: 1, 8637: 1, 8638: 1, 8639: 1, 8640: 1, 8642: 1, 8643: 1, 8644: 1, 8645: 1, 8646: 1, 8647: 1, 8648: 1, 8649: 1, 8650: 1, 8652: 1, 8653: 1, 8654: 1, 8655: 1, 8657: 1, 8658: 1, 8659: 1, 8660: 1, 8661: 1, 8662: 1, 8663: 1, 8664: 1, 8665: 1, 8666: 1, 8667: 1, 8668: 1, 8669: 1, 8670: 1, 8671: 1, 8674: 1, 8675: 1, 8676: 1, 8678: 1, 8679: 1, 8680: 1, 8681: 1, 8682: 1, 8683: 1, 8684: 1, 8685: 1, 8686: 1, 8687: 1, 8688: 1, 8689: 1, 8690: 1, 8691: 1, 8692: 1, 8693: 1, 8694: 1, 8695: 1, 8696: 1, 8697: 1, 8700: 1, 8701: 1, 8702: 1, 8703: 1, 8704: 1, 8706: 1, 8707: 1, 8708: 1, 8709: 1, 8712: 1, 8713: 1, 8714: 1, 8715: 1, 8716: 1, 8717: 1, 8718: 1, 8719: 1, 8720: 1, 8721: 1, 8722: 1, 8723: 1, 8724: 1, 8725: 1, 8726: 1, 8727: 1, 8728: 1, 8729: 1, 8730: 1, 8731: 1, 8732: 1, 8734: 1, 8735: 1, 8736: 1, 8737: 1, 8738: 1, 8739: 1, 8740: 1, 8741: 1, 8742: 1, 8743: 1, 8744: 1, 8745: 1, 8746: 1, 8747: 1, 8748: 1, 8751: 1, 8753: 1, 8754: 1, 8755: 1, 8756: 1, 8757: 1, 8758: 1, 8759: 1, 8760: 1, 8762: 1, 8764: 1, 8765: 1, 8766: 1, 8767: 1, 8768: 1, 8769: 1, 8770: 1, 8771: 1, 8772: 1, 8773: 1, 8774: 1, 8775: 1, 8776: 1, 8777: 1, 8778: 1, 8779: 1, 8780: 1, 8781: 1, 8782: 1, 8784: 1, 8785: 1, 8786: 1, 8787: 1, 8788: 1, 8789: 1, 8790: 1, 8791: 1, 8792: 1, 8793: 1, 8794: 1, 8795: 1, 8796: 1, 8797: 1, 8798: 1, 8800: 1, 8801: 1, 8802: 1, 8803: 1, 8804: 1, 8806: 1, 8807: 1, 8808: 1, 8809: 1, 8810: 1, 8811: 1, 8812: 1, 8813: 1, 8814: 1, 8815: 1, 8816: 1, 8817: 1, 8818: 1, 8819: 1, 8820: 1, 8821: 1, 8822: 1, 8823: 1, 8824: 1, 8826: 1, 8827: 1, 8828: 1, 8829: 1})\n",
            "\n",
            "Counts array after removing banned tokens: [0.0000e+00 6.2683e+04 1.5430e+03 ... 1.0000e+00 1.0000e+00 1.0000e+00]\n",
            "\n",
            "Shape of the counts array: (8830,)\n",
            "\n",
            "Total: 474859.0\n",
            "\n",
            "Probability: [0.00000000e+00 1.32003395e-01 3.24938561e-03 ... 2.10588827e-06\n",
            " 2.10588827e-06 2.10588827e-06]\n",
            "\n",
            "log_p: [  0.          -2.02492764  -5.72928934 ... -13.0707732  -13.0707732\n",
            " -13.0707732 ]\n",
            "\n",
            "\n",
            "Uniform entropy: 9.09\n",
            "Marginal entropy: 5.37\n",
            "Bias: [-1.00000000e+09 -2.02492764e+00 -5.72928934e+00 ... -1.30707732e+01\n",
            " -1.30707732e+01 -1.30707732e+01]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Remove the ['START'] Token from the key in train and test dataset.***"
      ],
      "metadata": {
        "id": "EbwxflVCb2r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train['key'] = [a[1:] for a in df_train['key']]\n",
        "# df_test['key'] = [a[1:] for a in df_test['key']]"
      ],
      "metadata": {
        "id": "-ErYP8CabQzC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_tensor = tokens_to_tensor(vocab_dict=vocab_dict, vocabulary_size=vocabulary_size)\n",
        "#     tgt_ids = test_tensor.preprocessing([[1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 12, 13, 14, 15], [1, 16, 17, 18, 1, 19, 20, 15]])\n",
        "#     out = decoder(tgt_ids)"
      ],
      "metadata": {
        "id": "mpkjASHi_BIQ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, feature_extractor, output_layer, index_to_word, vocab_dict, vocabulary_size, num_layers=1,\n",
        "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    # self.word_to_index = tf.keras.layers.StringLookup(\n",
        "    #     mask_token=\"\",\n",
        "    #     vocabulary=tokenizer.get_vocabulary())\n",
        "    self.index_to_word = index_to_word\n",
        "\n",
        "    self.proj = tf.keras.layers.Dense(256)\n",
        "\n",
        "    self.tensor = tokens_to_tensor(vocab_dict=vocab_dict, vocabulary_size=vocabulary_size)\n",
        "\n",
        "    self.seq_embedding = EmbeddingLayer(\n",
        "        vocab_size=vocabulary_size,\n",
        "        depth=units,\n",
        "        max_length=max_length)\n",
        "\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)]\n",
        "\n",
        "    self.output_layer = output_layer"
      ],
      "metadata": {
        "id": "3MRywMLtwPLv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  @Captioner.add_method\n",
        "  def __call__(self, image, txt):\n",
        "    if image.shape[-1] == 3:\n",
        "      # Apply the feature-extractor, if you get an RGB image.\n",
        "      image = self.feature_extractor(image)\n",
        "\n",
        "    # Flatten the feature map\n",
        "    # image = tf.reshape(image, [image.shape[0], -1])\n",
        "\n",
        "    batch_size = tf.shape(image)[0]\n",
        "    H, W, C = image.shape[1], image.shape[2], image.shape[3]\n",
        "\n",
        "    # 1) flatten spatial dims\n",
        "    image = tf.reshape(image, (batch_size, H*W, C))\n",
        "\n",
        "    image = self.proj(image)\n",
        "\n",
        "    print(f\"Image shape: {image.shape}\")\n",
        "\n",
        "    ## convert the text into the tensor\n",
        "    txt = self.tensor.preprocessing(txt)\n",
        "\n",
        "    txt = self.seq_embedding(txt)\n",
        "\n",
        "    txt = txt.detach().numpy()\n",
        "\n",
        "    print(f\"After embedding: {txt}\")\n",
        "\n",
        "    # Look at the image\n",
        "    for dec_layer in self.decoder_layers:\n",
        "      txt = dec_layer(inputs=(image, txt))\n",
        "\n",
        "    print(f\"After completing decoder layer: {txt}\")\n",
        "    txt = self.output_layer(txt)\n",
        "\n",
        "    return txt"
      ],
      "metadata": {
        "id": "lwtJBeTvcmrg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Captioner(feature_extractor=feature_extractor, output_layer=output_layer, index_to_word=index_to_word, vocab_dict=vocab_dict, vocabulary_size=vocabulary_size,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ],
      "metadata": {
        "id": "ijgTYC5eczPA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[1],[2]], dtype=torch.long).detach().numpy()"
      ],
      "metadata": {
        "id": "Vj8m-FjXB5P_",
        "outputId": "520f24e2-2157-4030-983f-2797e646525f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [2]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.concat([torch.tensor([1], dtype=torch.long), torch.tensor([2], dtype=torch.long)], axis=1)"
      ],
      "metadata": {
        "id": "kaIJ87ZcBGHU"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_token(last_logits, temperature=1.0, top_k=None, top_p=None):\n",
        "    # last_logits: (batch, vocab)\n",
        "    # 1) temperature\n",
        "    logits = last_logits / tf.cast(temperature, last_logits.dtype)\n",
        "\n",
        "    # 2) top_k\n",
        "    if top_k is not None and top_k > 0:\n",
        "        values, _ = tf.math.top_k(logits, k=top_k)\n",
        "        min_val   = values[:, -1, None]\n",
        "        logits    = tf.where(logits < min_val,\n",
        "                             tf.fill(tf.shape(logits), -1e9),\n",
        "                             logits)\n",
        "\n",
        "    # 3) top_p\n",
        "    if top_p is not None and top_p < 1.0:\n",
        "        sorted_logits = tf.sort(logits, direction=\"DESCENDING\", axis=-1)\n",
        "        sorted_probs  = tf.nn.softmax(sorted_logits, axis=-1)\n",
        "        cum_probs     = tf.cumsum(sorted_probs, axis=-1)\n",
        "        # find first index where cum_probs > top_p:\n",
        "        cutoff_idx    = tf.reduce_min(\n",
        "            tf.where(cum_probs > top_p,\n",
        "                     tf.range(tf.shape(cum_probs)[-1]),\n",
        "                     tf.fill(tf.shape(cum_probs), tf.shape(cum_probs)[-1]))\n",
        "        )\n",
        "        cutoff_val    = sorted_logits[:, cutoff_idx][:, None]\n",
        "        logits        = tf.where(logits < cutoff_val,\n",
        "                                 tf.fill(tf.shape(logits), -1e9),\n",
        "                                 logits)\n",
        "\n",
        "    # 4) sample\n",
        "    return tf.random.categorical(logits, num_samples=1, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "HkRQqJgCxB_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1, top_p=0.9, top_k=30):\n",
        "  initial = [[vocab_dict['[START]']]] # (batch, sequence)\n",
        "  img_features = self.feature_extractor(image)\n",
        "\n",
        "  tokens = initial # (batch, sequence)\n",
        "  print(tokens)\n",
        "  for n in range(50):\n",
        "    preds = self(img_features, tokens).numpy()  # (batch, sequence, vocab)\n",
        "\n",
        "    # print(f\"Predictions: {preds.shape}\")\n",
        "    preds = preds[:,-1, :]  #(batch, vocab)\n",
        "    print(f\"Predictions: {preds}\")\n",
        "    print(f\"Predictions shape: {preds.shape}\")\n",
        "\n",
        "    if temperature==0:\n",
        "        next = np.argmax(preds, axis=-1)  # (batch, 1)\n",
        "    else:\n",
        "        logits = tf.convert_to_tensor(preds / temperature)\n",
        "        print(f\"Logits: {logits}\")\n",
        "\n",
        "        if top_k is not None and top_k > 0:\n",
        "            values, _ = tf.math.top_k(logits, k=top_k)\n",
        "            print(f\"After getting top_k: {values} and shape is: {values.shape}\")\n",
        "            min_val   = values[:, -1, None]\n",
        "            print(f\"Minimum value: {min_val}\")\n",
        "            logits = tf.where(logits < min_val,\n",
        "                                tf.fill(tf.shape(logits), -1e9),\n",
        "                                logits)\n",
        "            print(f\"After filling the top k value as it is and otherwise fill as infinity: {logits}\")\n",
        "\n",
        "        if top_p is not None and top_p < 1.0:\n",
        "            sorted_logits = tf.sort(logits, direction=\"DESCENDING\", axis=-1)\n",
        "            print(f\"Sorted logits in descending: {sorted_logits}\")\n",
        "            sorted_probs = tf.nn.softmax(sorted_logits, axis=-1)\n",
        "            print(f\"After softmax layer: {sorted_probs}\")\n",
        "            cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
        "            print(f\"Cumulative probability: {cum_probs}\")\n",
        "            # find first index where cum_probs > top_p:\n",
        "            cutoff_idx    = tf.reduce_min(\n",
        "                tf.where(cum_probs > top_p,\n",
        "                        tf.range(tf.shape(cum_probs)[-1]),\n",
        "                        tf.fill(tf.shape(cum_probs), tf.shape(cum_probs)[-1]))\n",
        "            )\n",
        "            cutoff_val    = sorted_logits[:, cutoff_idx][:, None]\n",
        "            logits        = tf.where(logits < cutoff_val,\n",
        "                                    tf.fill(tf.shape(logits), -1e9),\n",
        "                                    logits)\n",
        "\n",
        "        next = tf.random.categorical(logits, num_samples=1)\n",
        "        next = next.numpy().squeeze(-1)\n",
        "        # next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "    print(f\"Next: {next}\")\n",
        "    batch0_id = next[0]\n",
        "    max_logit_val  = preds[0, batch0_id]\n",
        "    print(f\"Next token ID: {batch0_id} with logit {max_logit_val:.3f}\")\n",
        "\n",
        "    next_tokens = [index_to_word[i] for i in next]\n",
        "    print(f\"Next tokens: {next_tokens}\")\n",
        "\n",
        "    tokens = np.concatenate([tokens, next[:, None]], axis=1)\n",
        "    # tokens = tf.concat([tokens, next], axis=1) # (batch, sequence)\n",
        "\n",
        "    if next[0] == vocab_dict['[END]']:\n",
        "      break\n",
        "\n",
        "  print(tokens)\n",
        "  words = [index_to_word[i] for i in tokens[0]]\n",
        "  print(f\"Words: {words}\")\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  print(f\"Result: {result}\\n\")\n",
        "  return result.numpy().decode()"
      ],
      "metadata": {
        "id": "7D_6ckf8dBiJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.simple_gen(\"215214751_e913b6ff09.jpg\", temperature=1.0)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "AWe8e9z2hX6r",
        "outputId": "423c385b-12fb-480b-d2bf-7e764676fe86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [7557]\n",
            "Next token ID: 7557 with logit 0.000\n",
            "Next tokens: ['661']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [2019]\n",
            "Next token ID: 2019 with logit 0.000\n",
            "Next tokens: ['strikes']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [3021]\n",
            "Next token ID: 3021 with logit 0.000\n",
            "Next tokens: ['pretends']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [5326]\n",
            "Next token ID: 5326 with logit 0.000\n",
            "Next tokens: ['directs']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [5264]\n",
            "Next token ID: 5264 with logit 0.000\n",
            "Next tokens: ['tugowar']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [7070]\n",
            "Next token ID: 7070 with logit 0.000\n",
            "Next tokens: ['ballons']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [6937]\n",
            "Next token ID: 6937 with logit 0.000\n",
            "Next tokens: ['caucasion']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [5157]\n",
            "Next token ID: 5157 with logit 0.000\n",
            "Next tokens: ['paperwork']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [2769]\n",
            "Next token ID: 2769 with logit 0.000\n",
            "Next tokens: ['presentation']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [4343]\n",
            "Next token ID: 4343 with logit 0.000\n",
            "Next tokens: ['fumble']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [8614]\n",
            "Next token ID: 8614 with logit 0.000\n",
            "Next tokens: ['bronze']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [7557]\n",
            "Next token ID: 7557 with logit 0.000\n",
            "Next tokens: ['661']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [6905]\n",
            "Next token ID: 6905 with logit 0.000\n",
            "Next tokens: ['production']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [6937]\n",
            "Next token ID: 6937 with logit 0.000\n",
            "Next tokens: ['caucasion']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [5017]\n",
            "Next token ID: 5017 with logit 0.000\n",
            "Next tokens: ['potty']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [4343]\n",
            "Next token ID: 4343 with logit 0.000\n",
            "Next tokens: ['fumble']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [7070]\n",
            "Next token ID: 7070 with logit 0.000\n",
            "Next tokens: ['ballons']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [2895]\n",
            "Next token ID: 2895 with logit 0.000\n",
            "Next tokens: ['tap']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [7557]\n",
            "Next token ID: 7557 with logit 0.000\n",
            "Next tokens: ['661']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [2769]\n",
            "Next token ID: 2769 with logit 0.000\n",
            "Next tokens: ['presentation']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [5264]\n",
            "Next token ID: 5264 with logit 0.000\n",
            "Next tokens: ['tugowar']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [2099]\n",
            "Next token ID: 2099 with logit 0.000\n",
            "Next tokens: ['disc']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [3021]\n",
            "Next token ID: 3021 with logit 0.000\n",
            "Next tokens: ['pretends']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [5326]\n",
            "Next token ID: 5326 with logit 0.000\n",
            "Next tokens: ['directs']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [7557]\n",
            "Next token ID: 7557 with logit 0.000\n",
            "Next tokens: ['661']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [5498]\n",
            "Next token ID: 5498 with logit 0.000\n",
            "Next tokens: ['piling']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [7320]\n",
            "Next token ID: 7320 with logit 0.000\n",
            "Next tokens: ['coloful']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [5017]\n",
            "Next token ID: 5017 with logit 0.000\n",
            "Next tokens: ['potty']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.7525832   1.1109288  -0.7345768  ...  2.4231927  -2.0381057\n",
            "   -0.9003954 ]\n",
            "  [ 0.08888775  0.65123105  0.06738496 ...  2.4231927  -2.0379982\n",
            "   -0.9003954 ]\n",
            "  [ 0.1567142  -0.3052181   0.22356755 ...  2.4231927  -2.037891\n",
            "   -0.9003954 ]\n",
            "  ...\n",
            "  [-0.6290101  -0.8814067  -0.97753286 ...  2.423178   -2.033055\n",
            "   -0.90040815]\n",
            "  [-1.5208378  -0.52921563 -0.10177958 ...  2.4231772  -2.0329475\n",
            "   -0.9004087 ]\n",
            "  [-1.7063358   0.41152126  0.2644137  ...  2.4231768  -2.0328403\n",
            "   -0.9004093 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.9663883  -0.05817012  0.04190207 ... -0.24608365 -0.73711956\n",
            "    0.33204958]\n",
            "  [ 1.0160148  -0.12310376  0.11472353 ... -0.21134166 -0.74083436\n",
            "    0.32632095]\n",
            "  [ 1.0149794  -0.22385062  0.1272084  ... -0.18533552 -0.7464081\n",
            "    0.32371384]\n",
            "  ...\n",
            "  [ 0.94347966 -0.31723195  0.00171992 ... -0.23389441 -0.68538475\n",
            "    0.3086319 ]\n",
            "  [ 0.85752285 -0.31068856  0.0843582  ... -0.217064   -0.7305329\n",
            "    0.323905  ]\n",
            "  [ 0.8550731  -0.18768138  0.13598204 ... -0.21601655 -0.7467506\n",
            "    0.3411994 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0066214  0.00685676 0.00874048 ... 0.01417275 0.01340065\n",
            "    0.00829932]\n",
            "   [0.00758247 0.00787803 0.01048286 ... 0.01489506 0.01414616\n",
            "    0.0094837 ]\n",
            "   [0.00810373 0.00849584 0.01126505 ... 0.01553955 0.01473887\n",
            "    0.01106056]\n",
            "   ...\n",
            "   [0.00640147 0.0067098  0.00594674 ... 0.01440163 0.01575031\n",
            "    0.01303956]\n",
            "   [0.00623694 0.00644259 0.00564631 ... 0.01446017 0.01525769\n",
            "    0.01285357]\n",
            "   [0.00686618 0.00696315 0.00601419 ... 0.01483882 0.01519606\n",
            "    0.01301838]]\n",
            "\n",
            "  [[0.02246882 0.01884739 0.01310477 ... 0.01653814 0.01853696\n",
            "    0.02440806]\n",
            "   [0.02176966 0.018875   0.01401072 ... 0.01726454 0.01921752\n",
            "    0.02502926]\n",
            "   [0.02156843 0.01922886 0.01474707 ... 0.01780528 0.01945671\n",
            "    0.02526013]\n",
            "   ...\n",
            "   [0.01690808 0.01711456 0.0163736  ... 0.01380425 0.01534064\n",
            "    0.02017323]\n",
            "   [0.01915779 0.01864666 0.01860822 ... 0.01428013 0.01581636\n",
            "    0.02110328]\n",
            "   [0.02111311 0.01998201 0.02061025 ... 0.01449676 0.01603541\n",
            "    0.02186047]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 5.5941588e-01 -1.5942547e-01 -6.7288123e-02 ...  2.6687901e-02\n",
            "   -7.9961026e-01 -9.8106705e-02]\n",
            "  [ 6.0740185e-01 -2.0817123e-01 -3.2101944e-04 ...  6.0780738e-02\n",
            "   -8.0261225e-01 -1.1131480e-01]\n",
            "  [ 6.1687267e-01 -3.0121276e-01  1.4039928e-02 ...  7.8914694e-02\n",
            "   -8.1462222e-01 -1.1643745e-01]\n",
            "  ...\n",
            "  [ 5.7833320e-01 -4.5250455e-01 -1.8096134e-01 ...  7.5478889e-02\n",
            "   -8.0495197e-01 -9.4299845e-02]\n",
            "  [ 4.7015750e-01 -4.2850310e-01 -9.0613559e-02 ...  8.0980614e-02\n",
            "   -8.4408104e-01 -8.7939739e-02]\n",
            "  [ 4.5620555e-01 -3.0496022e-01 -3.7533939e-02 ...  7.5626791e-02\n",
            "   -8.3614731e-01 -9.5037073e-02]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 1.5605801   1.5276719   0.59485877 ...  1.036306   -1.4131017\n",
            "   -0.16401936]\n",
            "  [ 1.559877    1.5213332   0.5959878  ...  1.0464375  -1.4128886\n",
            "   -0.15858302]\n",
            "  [ 1.555351    1.5106556   0.5881037  ...  1.0567075  -1.4143012\n",
            "   -0.15172718]\n",
            "  ...\n",
            "  [ 1.5548576   1.4871927   0.5989791  ...  1.0538741  -1.4079653\n",
            "   -0.14610048]\n",
            "  [ 1.5399792   1.4897226   0.59654015 ...  1.0652385  -1.4082297\n",
            "   -0.1322669 ]\n",
            "  [ 1.5362239   1.5031414   0.5858557  ...  1.0781996  -1.4076774\n",
            "   -0.12120336]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01425752 0.01562817 0.01531282 ... 0.00848517 0.00765617\n",
            "    0.00808848]\n",
            "   [0.0144582  0.01573517 0.01529707 ... 0.00852516 0.00765175\n",
            "    0.00812178]\n",
            "   [0.01473074 0.01592296 0.01545569 ... 0.00858437 0.00767341\n",
            "    0.00817263]\n",
            "   ...\n",
            "   [0.01402481 0.01530471 0.01428459 ... 0.00850437 0.00752018\n",
            "    0.00779522]\n",
            "   [0.0140239  0.01528037 0.01448159 ... 0.00838649 0.00746348\n",
            "    0.00786944]\n",
            "   [0.01376943 0.01503892 0.01459018 ... 0.00815641 0.00733851\n",
            "    0.00779991]]\n",
            "\n",
            "  [[0.02489942 0.02048996 0.03378359 ... 0.01347715 0.01588862\n",
            "    0.01354409]\n",
            "   [0.02472239 0.02034979 0.0337103  ... 0.01362697 0.01611475\n",
            "    0.01355909]\n",
            "   [0.02458335 0.02026622 0.03362452 ... 0.01377395 0.01633741\n",
            "    0.01363343]\n",
            "   ...\n",
            "   [0.02658934 0.02199774 0.03520608 ... 0.01367313 0.01597796\n",
            "    0.01371826]\n",
            "   [0.0261003  0.02159788 0.03442817 ... 0.0138523  0.01614941\n",
            "    0.01365578]\n",
            "   [0.02586388 0.02136373 0.03387315 ... 0.0137842  0.0160064\n",
            "    0.01354054]]]]\n",
            "\n",
            "After feed forward neural network: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "\n",
            "After completing decoder layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "Successfully enter into the output layer: [[[ 1.1996526   1.9973316   0.2563803  ...  1.460769   -1.1185534\n",
            "    0.17320919]\n",
            "  [ 1.1956216   1.9908319   0.25692937 ...  1.4699279  -1.1172704\n",
            "    0.17921546]\n",
            "  [ 1.1892184   1.9803416   0.2491585  ...  1.4789145  -1.1173227\n",
            "    0.1867321 ]\n",
            "  ...\n",
            "  [ 1.1869756   1.9516553   0.2723171  ...  1.4758117  -1.1329728\n",
            "    0.19537507]\n",
            "  [ 1.16997     1.947249    0.2722208  ...  1.4846959  -1.1307732\n",
            "    0.2073673 ]\n",
            "  [ 1.1670607   1.9532534   0.26488972 ...  1.4967473  -1.126592\n",
            "    0.21570869]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "Predictions shape: (1, 8830)\n",
            "Logits: [[8.4990665e-05 7.9495578e-05 1.4404961e-04 ... 1.6869149e-04\n",
            "  1.0965452e-04 1.6780994e-04]]\n",
            "After getting top_k: [[0.00026077 0.00025839 0.00024426 0.00024423 0.0002442  0.00024235\n",
            "  0.00023819 0.0002356  0.0002335  0.0002328  0.00023002 0.00022353\n",
            "  0.00022206 0.00022178 0.00021999 0.00021992 0.00021991 0.00021728\n",
            "  0.00021694 0.00021653 0.0002165  0.00021326 0.00021319 0.00021237\n",
            "  0.0002119  0.0002104  0.00020985 0.00020934 0.00020929 0.0002086 ]] and shape is: (1, 30)\n",
            "Minimum value: [[0.0002086]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 2.6076782e-04  2.5839044e-04  2.4425582e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.03333452 0.03333444 0.03333397 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.03333452 0.06666896 0.10000293 ... 1.         1.         1.        ]]\n",
            "Next: [2019]\n",
            "Next token ID: 2019 with logit 0.000\n",
            "Next tokens: ['strikes']\n",
            "[[   0 4101 5017 6677 6514 7557 7557 4101 7557 2099 5498 6677 5498 2019\n",
            "  3311 8614 7557 2019 7320 8192 3311 4343 7557 2019 3021 5326 5264 7070\n",
            "  6937 5157 2769 4343 8614 7557 6905 6937 5017 4343 7070 2895 7557 2769\n",
            "  5264 2099 3021 5326 7557 5498 7320 5017 2019]]\n",
            "Words: ['[START]', 'offstage', 'potty', 'carves', 'christmastime', '661', '661', 'offstage', '661', 'disc', 'piling', 'carves', 'piling', 'strikes', 'distressed', 'bronze', '661', 'strikes', 'coloful', 'medals', 'distressed', 'fumble', '661', 'strikes', 'pretends', 'directs', 'tugowar', 'ballons', 'caucasion', 'paperwork', 'presentation', 'fumble', 'bronze', '661', 'production', 'caucasion', 'potty', 'fumble', 'ballons', 'tap', '661', 'presentation', 'tugowar', 'disc', 'pretends', 'directs', '661', 'piling', 'coloful', 'potty', 'strikes']\n",
            "Result: b'[START] offstage potty carves christmastime 661 661 offstage 661 disc piling carves piling strikes distressed bronze 661 strikes coloful medals distressed fumble 661 strikes pretends directs tugowar ballons caucasion paperwork presentation fumble bronze 661 production caucasion potty fumble ballons tap 661 presentation tugowar disc pretends directs 661 piling coloful potty strikes'\n",
            "\n",
            "[START] offstage potty carves christmastime 661 661 offstage 661 disc piling carves piling strikes distressed bronze 661 strikes coloful medals distressed fumble 661 strikes pretends directs tugowar ballons caucasion paperwork presentation fumble bronze 661 production caucasion potty fumble ballons tap 661 presentation tugowar disc pretends directs 661 piling coloful potty strikes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model.simple_gen(\"215214751_e913b6ff09.jpg\", temperature=t)\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3xsJWaVvd4jN",
        "outputId": "4b19f93f-54b4-4c6f-facf-5e20818ceda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Tensor is unhashable. Instead, use tensor.ref() as the key.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-57-3834535615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"215214751_e913b6ff09.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-56-2145700339.py\u001b[0m in \u001b[0;36msimple_gen\u001b[0;34m(self, image, temperature)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#   print(tokens[0, 1:-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Words: {words}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;31m# EagerTensors are never hashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     raise TypeError(\"Tensor is unhashable. \"\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \"Instead, use tensor.ref() as the key.\")\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Tensor is unhashable. Instead, use tensor.ref() as the key."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class temp_convert_output_to_tensor:\n",
        "    def __init__(self, vocab_dict, vocabulary_size, max_len=50):\n",
        "      self.vocab_dict = vocab_dict\n",
        "      self.vocabulary_size = vocabulary_size\n",
        "      self.max_len = max_len\n",
        "      self.preprocess_text = preprocess_text()\n",
        "\n",
        "    def preprocessing(self, caption_batch):\n",
        "      all_captions_tokenIDs = []\n",
        "\n",
        "      for s_tokenID in caption_batch:\n",
        "    #     ## split the caption\n",
        "    #     s_tokenID = []\n",
        "\n",
        "    #     ## find the token ID for the each token from vocab_dict.\n",
        "    #     for word in caption:\n",
        "    #       if word in self.vocab_dict:\n",
        "    #         s_tokenID.append(self.vocab_dict[word])\n",
        "\n",
        "    #       else:\n",
        "    #         s_tokenID.append(0)\n",
        "\n",
        "        ## padding the length\n",
        "        for _ in range(0, self.max_len-len(s_tokenID)):\n",
        "          s_tokenID.append(0)\n",
        "\n",
        "        all_captions_tokenIDs.append(s_tokenID)\n",
        "\n",
        "      tgt_ids = torch.tensor(all_captions_tokenIDs, dtype=torch.long)\n",
        "      return tgt_ids"
      ],
      "metadata": {
        "id": "wR-dgjGtr8Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oY2nFaoHd_y_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1c5f6b-f8de-40ac-bc61-c1af12037b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4055, -0.4849, -0.2018,  ...,  0.6381, -1.3429,  1.1430],\n",
            "         [ 0.6739,  1.2321,  0.5574,  ...,  1.3237,  1.2483,  0.9730],\n",
            "         [ 2.5945, -0.8670,  0.5196,  ...,  1.4123, -0.1160,  1.1735],\n",
            "         ...,\n",
            "         [ 0.4736, -1.4479,  2.0100,  ...,  2.1350,  2.7324,  0.1760],\n",
            "         [-0.4182, -1.0957,  1.7640,  ...,  2.1350,  2.7325,  0.1760],\n",
            "         [-0.6037, -0.1550,  0.8888,  ...,  2.1350,  2.7326,  0.1760]],\n",
            "\n",
            "        [[-0.4055, -0.4849, -0.2018,  ...,  0.6381, -1.3429,  1.1430],\n",
            "         [ 0.6016,  0.6899,  0.1445,  ...,  0.5265,  1.0655, -0.1205],\n",
            "         [ 1.1073,  1.7684,  1.8862,  ...,  2.5377,  0.4864,  0.0394],\n",
            "         ...,\n",
            "         [ 0.4736, -1.4479,  2.0100,  ...,  2.1350,  2.7324,  0.1760],\n",
            "         [-0.4182, -1.0957,  1.7640,  ...,  2.1350,  2.7325,  0.1760],\n",
            "         [-0.6037, -0.1550,  0.8888,  ...,  2.1350,  2.7326,  0.1760]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([2, 50, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "_l9NLDI1BXsd",
        "outputId": "c4bcbb26-b612-49de-a639-8828dd52ecb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 caption  \\\n",
              "0      [1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 1...   \n",
              "1                         [1, 16, 17, 18, 1, 19, 20, 15]   \n",
              "2                      [1, 21, 16, 7, 18, 1, 19, 22, 15]   \n",
              "3                 [1, 21, 16, 7, 23, 11, 24, 25, 22, 15]   \n",
              "4         [1, 21, 16, 3, 1, 4, 5, 17, 18, 1, 19, 26, 15]   \n",
              "...                                                  ...   \n",
              "40163  [234, 1258, 38, 1, 4025, 3, 561, 35, 1202, 306...   \n",
              "40164        [1, 75, 3, 1, 4, 161, 112, 1, 164, 202, 15]   \n",
              "40165            [1, 75, 6, 164, 7, 488, 3, 23, 227, 15]   \n",
              "40166  [1, 187, 3, 1, 110, 161, 7, 8, 1, 164, 202, 54...   \n",
              "40167                  [1, 164, 288, 3, 1, 110, 161, 15]   \n",
              "\n",
              "                           image  \n",
              "0      1000268201_693b08cb0e.jpg  \n",
              "1      1000268201_693b08cb0e.jpg  \n",
              "2      1000268201_693b08cb0e.jpg  \n",
              "3      1000268201_693b08cb0e.jpg  \n",
              "4      1000268201_693b08cb0e.jpg  \n",
              "...                          ...  \n",
              "40163   997338199_7343367d7f.jpg  \n",
              "40164   997722733_0cb5439472.jpg  \n",
              "40165   997722733_0cb5439472.jpg  \n",
              "40166   997722733_0cb5439472.jpg  \n",
              "40167   997722733_0cb5439472.jpg  \n",
              "\n",
              "[40168 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39878c4d-cfe2-4999-afc0-b1a176dad458\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 1...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 16, 17, 18, 1, 19, 20, 15]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 21, 16, 7, 18, 1, 19, 22, 15]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 21, 16, 7, 23, 11, 24, 25, 22, 15]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 21, 16, 3, 1, 4, 5, 17, 18, 1, 19, 26, 15]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40163</th>\n",
              "      <td>[234, 1258, 38, 1, 4025, 3, 561, 35, 1202, 306...</td>\n",
              "      <td>997338199_7343367d7f.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40164</th>\n",
              "      <td>[1, 75, 3, 1, 4, 161, 112, 1, 164, 202, 15]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40165</th>\n",
              "      <td>[1, 75, 6, 164, 7, 488, 3, 23, 227, 15]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40166</th>\n",
              "      <td>[1, 187, 3, 1, 110, 161, 7, 8, 1, 164, 202, 54...</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40167</th>\n",
              "      <td>[1, 164, 288, 3, 1, 110, 161, 15]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40168 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39878c4d-cfe2-4999-afc0-b1a176dad458')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-39878c4d-cfe2-4999-afc0-b1a176dad458 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-39878c4d-cfe2-4999-afc0-b1a176dad458');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c18e505c-ff3f-46b7-82ce-2fb63e8d2705\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c18e505c-ff3f-46b7-82ce-2fb63e8d2705')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c18e505c-ff3f-46b7-82ce-2fb63e8d2705 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_680b3cf7-8ad8-490d-894e-5b0346cb9de0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_680b3cf7-8ad8-490d-894e-5b0346cb9de0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 40168,\n  \"fields\": [\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3115174046_9e96b9ce47.jpg\",\n          \"3107592525_0bcd00777e.jpg\",\n          \"2426724282_237bca30b5.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['caption'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdAud5cVAAIz",
        "outputId": "01374a61-80b1-45a5-ca1f-1e1d10a689bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 12, 13, 14, 15]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9mYqxECBVzR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}