{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhpatel40/image_captioning/blob/main/image_captioning_using_custom_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwRVImNojLdI",
        "outputId": "02fd0ee1-971c-4baa-8986-e4cbbcbca632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/flickr8k\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.6.0.163-1+cuda11.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xco_n10jgHF",
        "outputId": "0df13b1c-dccd-4cc0-c849-44d4c6d3f1f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcudnn8\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 446 MB of archives.\n",
            "After this operation, 1,140 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn8 8.6.0.163-1+cuda11.8 [446 MB]\n",
            "Fetched 446 MB in 5s (93.4 MB/s)\n",
            "Selecting previously unselected package libcudnn8.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.6.0.163-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.6.0.163-1+cuda11.8) ...\n",
            "Setting up libcudnn8 (8.6.0.163-1+cuda11.8) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow estimator keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "822h4FEJjmv0",
        "outputId": "d7d09ef1-ab61-4f60-e21f-c1affb9297ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[33mWARNING: Skipping estimator as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: keras 3.8.0\n",
            "Uninstalling keras-3.8.0:\n",
            "  Successfully uninstalled keras-3.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjpGG9vljpRZ",
        "outputId": "bc2c5b56-21c8-465f-8ef0-f0d9197f1fc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.11/dist-packages (2.18.1)\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.11/dist-packages (4.9.9)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Downloading keras-3.11.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.7.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (1.17.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.67.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.23.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_datasets) (25.3.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow_datasets) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.70.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow_text-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.11.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, keras, tensorflow, tensorflow_text\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow_text\n",
            "    Found existing installation: tensorflow-text 2.18.1\n",
            "    Uninstalling tensorflow-text-2.18.1:\n",
            "      Successfully uninstalled tensorflow-text-2.18.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.11.1 ml-dtypes-0.5.3 tensorboard-2.19.0 tensorflow-2.19.0 tensorflow_text-2.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZPU53bEjs6I",
        "outputId": "685f183a-9957-42d2-ca16-119c5f6c7e77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import csv\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "zsSYUDkEsUD8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions_file = path + \"/captions.txt\"\n",
        "\n",
        "with open(captions_file, \"r\") as f:\n",
        "    captions = f.read().splitlines()"
      ],
      "metadata": {
        "id": "jozNHPT1js3i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset_dict = {}\n",
        "\n",
        "for caption in captions[1:-1]:\n",
        "    reader = csv.reader([caption])\n",
        "    image, caption = tuple(next(reader))\n",
        "    if image not in full_dataset_dict:\n",
        "        full_dataset_dict[image] = []\n",
        "    full_dataset_dict[image].append(caption)"
      ],
      "metadata": {
        "id": "AW_yMohekOSV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Load the model***"
      ],
      "metadata": {
        "id": "jpB7h87JsjIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (224, 224, 3)\n",
        "restnet = tf.keras.applications.ResNet50V2(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False)\n",
        "restnet.trainable=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8BaN6zNkQE-",
        "outputId": "b95f931c-ae6b-46b5-8351-75c78026d4ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94668760/94668760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***function to load the model and resize for the model***"
      ],
      "metadata": {
        "id": "aFxQS6TutGb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    full_image_path = f\"{path}/Images/{image_path}\"\n",
        "    img = tf.io.read_file(full_image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img"
      ],
      "metadata": {
        "id": "oVWYCk1tmYA1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.newaxis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9APSXyquM9q",
        "outputId": "4e1aaf06-6209-450b-d2d8-d0ef6929b705"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.path.exists(f\"{path}/Images/1000268201_693b08cb0e.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhvTJGY8urtS",
        "outputId": "a5cdd04f-7532-43f0-f5aa-8df34cd78af3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_batch = load_image(list(full_dataset_dict.keys())[0])[tf.newaxis, :]\n",
        "\n",
        "print(test_img_batch.shape)\n",
        "print(restnet(test_img_batch).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQmmnZtFs7C2",
        "outputId": "e5fc9fad-8587-4474-a441-82eb1202aa97"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 224, 224, 3)\n",
            "(1, 7, 7, 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Preprocess Captions***"
      ],
      "metadata": {
        "id": "F6MEWebdvP7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Function for the remove punctuation, lower case, and add the special token before and after the captions.***"
      ],
      "metadata": {
        "id": "XoqoPCvavT8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "class preprocess_text:\n",
        "   def __init__(self):\n",
        "      pass\n",
        "\n",
        "   def standardize(self, s:List):\n",
        "      s = tf.constant(s, dtype=tf.string)\n",
        "      s = tf.strings.lower(s)\n",
        "      s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
        "      byte_arr = s.numpy()\n",
        "\n",
        "      # Decode each element safely\n",
        "      decoded = [\n",
        "          b.decode('utf-8') if isinstance(b, (bytes, bytearray)) else str(b)\n",
        "          for b in byte_arr\n",
        "      ]\n",
        "\n",
        "      return [f\"[START] {t} [END]\" for t in decoded]\n",
        "\n",
        "   def make_vocabulary_dict(self, full_dataset_dict):\n",
        "      vocabulary_size = 2\n",
        "      vocab_dict = {'[PAD]': 0, '[START]': 1}\n",
        "      for image in full_dataset_dict:\n",
        "          for caption in full_dataset_dict[image]:\n",
        "              for word in caption.split():\n",
        "                  if word not in vocab_dict:\n",
        "                      vocab_dict[word] = vocabulary_size\n",
        "                      vocabulary_size += 1\n",
        "                  else:\n",
        "                      continue\n",
        "\n",
        "      return vocab_dict\n",
        "\n",
        "   def index_to_word(self, vocab_dict):\n",
        "      index_to_word = {index: word for word, index in vocab_dict.items()}\n",
        "      return index_to_word\n",
        "\n",
        "   def word_to_index(self, s):\n",
        "      return [vocab_dict[word] for word in s.split()]"
      ],
      "metadata": {
        "id": "XsuhGDJswti4"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tokens_to_tensor:\n",
        "    def __init__(self, vocab_dict, vocabulary_size, max_len=50):\n",
        "      self.vocab_dict = vocab_dict\n",
        "      self.vocabulary_size = vocabulary_size\n",
        "      self.max_len = max_len\n",
        "      self.preprocess_text = preprocess_text()\n",
        "\n",
        "    def preprocessing(self, caption_batch):\n",
        "      all_captions_tokenIDs = []\n",
        "\n",
        "      for s_tokenID in caption_batch:\n",
        "        ## split the caption\n",
        "        s_tokenID = []\n",
        "\n",
        "        ## find the token ID for the each token from vocab_dict.\n",
        "        # for word in caption:\n",
        "        #   if word in self.vocab_dict:\n",
        "        #     s_tokenID.append(self.vocab_dict[word])\n",
        "\n",
        "        #   else:\n",
        "        #     s_tokenID.append(0)\n",
        "\n",
        "        ## padding the length\n",
        "        for _ in range(0, self.max_len-len(s_tokenID)):\n",
        "          s_tokenID.append(0)\n",
        "\n",
        "        all_captions_tokenIDs.append(s_tokenID)\n",
        "\n",
        "      tgt_ids = torch.tensor(all_captions_tokenIDs, dtype=torch.long)\n",
        "      return tgt_ids"
      ],
      "metadata": {
        "id": "FMT3Bk0RwcoC"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_preprocess = preprocess_text(vocab_dict, vocabulary_size)\n",
        "# standardize([\"a cat on the sat, and i would like to tell with her.\"])"
      ],
      "metadata": {
        "id": "NwsuBWTuxSRb"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temp_captions = list(standardize(full_dataset_dict[\"2083434441_a93bc6306b.jpg\"]))\n",
        "# print(temp_captions)"
      ],
      "metadata": {
        "id": "XEzLuhijvwdl"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing = preprocess_text()"
      ],
      "metadata": {
        "id": "hmRflgXs80wD"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in full_dataset_dict:\n",
        "    full_dataset_dict[image] = preprocessing.standardize(full_dataset_dict[image])"
      ],
      "metadata": {
        "id": "6tM46m664U6f"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset_dict[\"2083434441_a93bc6306b.jpg\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKO7fU6P6yc1",
        "outputId": "a910e82a-b3bc-4e61-8d93-dd74ea890bf0"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[START] an elderly woman is riding a bicycle in the city as a yellow taxi is about to pass by  end [END]',\n",
              " '[START] an elderly woman rides a bicycle along a city street  end [END]',\n",
              " '[START] an older woman with blond hair rides a bicycle down the street  end [END]',\n",
              " '[START] a woman in a grey overcoat rides her bicycle along a street  end [END]',\n",
              " '[START] older woman wearing glassses riding a bicycle with a shopping bag on the handle  yellow car is in the background  end [END]']"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make the vocabulary size"
      ],
      "metadata": {
        "id": "kyqRrsS36nfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dict = preprocessing.make_vocabulary_dict(full_dataset_dict)"
      ],
      "metadata": {
        "id": "SdeygPRt8KOW"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Convert the dataset into the index***"
      ],
      "metadata": {
        "id": "7MS8OzrNFQW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = {}\n",
        "for image in full_dataset_dict:\n",
        "    for caption in full_dataset_dict[image]:\n",
        "        full_dataset[caption] = image"
      ],
      "metadata": {
        "id": "Zhe5clWL9E6g"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'length of the full dataset each caption with image: {len(list(full_dataset.keys()))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GDqKHeL-BU5",
        "outputId": "58e62948-2264-41b6-f5d0-1c9bd8b2fed9"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the full dataset each caption with image: 40168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Make the index to word ***"
      ],
      "metadata": {
        "id": "65yG_z8vFZsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = preprocessing.index_to_word(vocab_dict)"
      ],
      "metadata": {
        "id": "WvvBDmzNFBqC"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Make the dataframe from the dictionary***"
      ],
      "metadata": {
        "id": "bZK0OaQx-RR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict(full_dataset, orient='index', columns=['image'])\n",
        "df = df.reset_index().rename(columns={'index': 'caption'})\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "TD4Fr4Xy-Qyt",
        "outputId": "1c202392-bb0f-4d28-cd83-0cfad7a2e27b"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 caption  \\\n",
              "0      [START] a child in a pink dress is climbing up...   \n",
              "1      [START] a girl going into a wooden building  e...   \n",
              "2      [START] a little girl climbing into a wooden p...   \n",
              "3      [START] a little girl climbing the stairs to h...   \n",
              "4      [START] a little girl in a pink dress going in...   \n",
              "...                                                  ...   \n",
              "40163  [START] woman writing on a pad in room with go...   \n",
              "40164  [START] a man in a pink shirt climbs a rock fa...   \n",
              "40165  [START] a man is rock climbing high in the air...   \n",
              "40166  [START] a person in a red shirt climbing up a ...   \n",
              "40167   [START] a rock climber in a red shirt  end [END]   \n",
              "\n",
              "                           image  \n",
              "0      1000268201_693b08cb0e.jpg  \n",
              "1      1000268201_693b08cb0e.jpg  \n",
              "2      1000268201_693b08cb0e.jpg  \n",
              "3      1000268201_693b08cb0e.jpg  \n",
              "4      1000268201_693b08cb0e.jpg  \n",
              "...                          ...  \n",
              "40163   997338199_7343367d7f.jpg  \n",
              "40164   997722733_0cb5439472.jpg  \n",
              "40165   997722733_0cb5439472.jpg  \n",
              "40166   997722733_0cb5439472.jpg  \n",
              "40167   997722733_0cb5439472.jpg  \n",
              "\n",
              "[40168 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4fa68dc-e6fe-4616-9efc-addfda409086\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[START] a child in a pink dress is climbing up...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[START] a girl going into a wooden building  e...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[START] a little girl climbing into a wooden p...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[START] a little girl climbing the stairs to h...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[START] a little girl in a pink dress going in...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40163</th>\n",
              "      <td>[START] woman writing on a pad in room with go...</td>\n",
              "      <td>997338199_7343367d7f.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40164</th>\n",
              "      <td>[START] a man in a pink shirt climbs a rock fa...</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40165</th>\n",
              "      <td>[START] a man is rock climbing high in the air...</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40166</th>\n",
              "      <td>[START] a person in a red shirt climbing up a ...</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40167</th>\n",
              "      <td>[START] a rock climber in a red shirt  end [END]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40168 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4fa68dc-e6fe-4616-9efc-addfda409086')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b4fa68dc-e6fe-4616-9efc-addfda409086 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b4fa68dc-e6fe-4616-9efc-addfda409086');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-53e4fdf6-9af7-445f-9f62-ee81760ff226\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53e4fdf6-9af7-445f-9f62-ee81760ff226')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-53e4fdf6-9af7-445f-9f62-ee81760ff226 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_b474d40f-44cf-44de-8a10-2b51ee11aa6f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b474d40f-44cf-44de-8a10-2b51ee11aa6f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 40168,\n  \"fields\": [\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 40168,\n        \"samples\": [\n          \"[START] a boat with two fishermen sits peacefully on the lake  end [END]\",\n          \"[START] a black and white dog jumps to get the frisbee  end [END]\",\n          \"[START] a dog runs on the beach with a red dog toy with a yellow rope attached  end [END]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3115174046_9e96b9ce47.jpg\",\n          \"3107592525_0bcd00777e.jpg\",\n          \"2426724282_237bca30b5.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['caption'] = df['caption'].apply(lambda x: x.replace(\"  \", \" \"))\n",
        "df['caption'] = df['caption'].apply(lambda x: preprocessing.word_to_index(x))"
      ],
      "metadata": {
        "id": "0-rOY_r9-ioG"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "zm0m7gon-9Dz",
        "outputId": "4d199d1a-aa76-41bb-c28e-f37037d4b497"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 caption  \\\n",
              "0      [1, 2, 3, 4, 2, 5, 6, 7, 8, 9, 2, 10, 11, 12, ...   \n",
              "1                  [1, 2, 18, 19, 20, 2, 21, 22, 16, 17]   \n",
              "2               [1, 2, 23, 18, 8, 20, 2, 21, 24, 16, 17]   \n",
              "3          [1, 2, 23, 18, 8, 25, 12, 26, 27, 24, 16, 17]   \n",
              "4      [1, 2, 23, 18, 4, 2, 5, 6, 19, 20, 2, 21, 28, ...   \n",
              "...                                                  ...   \n",
              "40163  [1, 236, 1259, 40, 2, 4026, 4, 562, 37, 1203, ...   \n",
              "40164  [1, 2, 77, 4, 2, 5, 163, 114, 2, 166, 204, 16,...   \n",
              "40165     [1, 2, 77, 7, 166, 8, 489, 4, 25, 229, 16, 17]   \n",
              "40166  [1, 2, 189, 4, 2, 112, 163, 8, 9, 2, 166, 204,...   \n",
              "40167           [1, 2, 166, 290, 4, 2, 112, 163, 16, 17]   \n",
              "\n",
              "                           image  \n",
              "0      1000268201_693b08cb0e.jpg  \n",
              "1      1000268201_693b08cb0e.jpg  \n",
              "2      1000268201_693b08cb0e.jpg  \n",
              "3      1000268201_693b08cb0e.jpg  \n",
              "4      1000268201_693b08cb0e.jpg  \n",
              "...                          ...  \n",
              "40163   997338199_7343367d7f.jpg  \n",
              "40164   997722733_0cb5439472.jpg  \n",
              "40165   997722733_0cb5439472.jpg  \n",
              "40166   997722733_0cb5439472.jpg  \n",
              "40167   997722733_0cb5439472.jpg  \n",
              "\n",
              "[40168 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e937cce1-d705-45ae-abe2-ab9eb766732c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 2, 3, 4, 2, 5, 6, 7, 8, 9, 2, 10, 11, 12, ...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 2, 18, 19, 20, 2, 21, 22, 16, 17]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 2, 23, 18, 8, 20, 2, 21, 24, 16, 17]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 2, 23, 18, 8, 25, 12, 26, 27, 24, 16, 17]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 2, 23, 18, 4, 2, 5, 6, 19, 20, 2, 21, 28, ...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40163</th>\n",
              "      <td>[1, 236, 1259, 40, 2, 4026, 4, 562, 37, 1203, ...</td>\n",
              "      <td>997338199_7343367d7f.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40164</th>\n",
              "      <td>[1, 2, 77, 4, 2, 5, 163, 114, 2, 166, 204, 16,...</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40165</th>\n",
              "      <td>[1, 2, 77, 7, 166, 8, 489, 4, 25, 229, 16, 17]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40166</th>\n",
              "      <td>[1, 2, 189, 4, 2, 112, 163, 8, 9, 2, 166, 204,...</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40167</th>\n",
              "      <td>[1, 2, 166, 290, 4, 2, 112, 163, 16, 17]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40168 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e937cce1-d705-45ae-abe2-ab9eb766732c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e937cce1-d705-45ae-abe2-ab9eb766732c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e937cce1-d705-45ae-abe2-ab9eb766732c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5227d373-2c09-4c45-b55c-d587bd7d2c5c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5227d373-2c09-4c45-b55c-d587bd7d2c5c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5227d373-2c09-4c45-b55c-d587bd7d2c5c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_2f326e69-c0bf-4002-9f8e-dfbe003c0102\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2f326e69-c0bf-4002-9f8e-dfbe003c0102 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 40168,\n  \"fields\": [\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3115174046_9e96b9ce47.jpg\",\n          \"3107592525_0bcd00777e.jpg\",\n          \"2426724282_237bca30b5.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert dict keys and values to lists\n",
        "# keys = list(full_dataset.keys())\n",
        "# values = list(full_dataset.values())\n",
        "\n",
        "# # Split keys and corresponding values together\n",
        "# keys_train, keys_test, vals_train, vals_test = sklearn.model_selection.train_test_split(\n",
        "#     keys, values,\n",
        "#     test_size=0.2,\n",
        "#     random_state=42,  # for reproducibility\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "# # Reconstruct train/test dicts\n",
        "# train_data = dict(zip(keys_train, vals_train))\n",
        "# test_data  = dict(zip(keys_test, vals_test))"
      ],
      "metadata": {
        "id": "9NwsZ3uxlTGU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Train data lengh: {len(list(train_data.keys()))}\")\n",
        "# print(f\"Test data length: {len(list(test_data.keys()))}\")"
      ],
      "metadata": {
        "id": "SX3a_yHSlS9y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Make the dataframe from the dictionary***"
      ],
      "metadata": {
        "id": "9PB1Z9LVkYJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train = pd.DataFrame.from_dict(train_data, orient='index', columns=['value'])\n",
        "# df_train = df_train.reset_index().rename(columns={'index': 'key'})\n",
        "# df_train"
      ],
      "metadata": {
        "id": "CWoXmmW_F0Yt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_test = pd.DataFrame.from_dict(test_data, orient='index', columns=['value'])\n",
        "# df_test = df_test.reset_index().rename(columns={'index': 'key'})\n",
        "# df_test"
      ],
      "metadata": {
        "id": "C3OQxRPPk2uW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Feature extractor that extract the feature from the image***"
      ],
      "metadata": {
        "id": "C3NS28irnDK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extractor(img):\n",
        "    resize_img = load_image(img)[tf.newaxis, :]\n",
        "    img_feature = restnet(resize_img)\n",
        "    return img_feature"
      ],
      "metadata": {
        "id": "RqiZGDP11XB4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Transformer Embedding part of token***"
      ],
      "metadata": {
        "id": "JS05ryJp24N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(vocab_dict.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWymKj7e38QO",
        "outputId": "41e5095b-4340-4d36-c631-d8e778ce2e89"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8831"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(list(vocab_dict.keys()))\n",
        "vocabulary_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpys_FJKsocd",
        "outputId": "0157631c-d4d3-4bca-9515-dd75d955d031"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8831"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 50):\n",
        "        super().__init__()\n",
        "        # Create a long enough P matrix once\n",
        "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len,1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                             (-math.log(10000.0) / d_model))  # (d_model/2,)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape (batch_size, seq_len, d_model)\n",
        "        Returns:\n",
        "            x + positional encoding, same shape\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        # only take the first seq_len positions\n",
        "        return x + self.pe[:, :seq_len]\n",
        "\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, vocab_size: int, max_length, depth: int):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, depth)\n",
        "        self.pos_enc     = PositionalEncoding(depth, max_length)\n",
        "        # this single layer can serve as demonstration; in practice you’d stack several of these\n",
        "        # self.self_attn   = nn.MultiheadAttention(d_model, nhead)\n",
        "        # self.ffn         = nn.Sequential(\n",
        "        #     nn.Linear(d_model, 4*d_model),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(4*d_model, d_model)\n",
        "        # )\n",
        "        # self.layernorm1  = nn.LayerNorm(d_model)\n",
        "        # self.layernorm2  = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tgt: target token IDs, shape (batch_size, tgt_len)\n",
        "        Returns:\n",
        "            output embeddings, shape (batch_size, tgt_len, d_model)\n",
        "        \"\"\"\n",
        "        bsz, tgt_len = tgt.size()\n",
        "\n",
        "        # 1) Token embedding\n",
        "        tok_emb = self.token_embed(tgt)                     # (bsz, tgt_len, d_model)\n",
        "\n",
        "        # 2) Positional embedding\n",
        "        x = self.pos_enc(tok_emb)                           # (bsz, tgt_len, d_model)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "# if __name__ == \"__main__\":\n",
        "#     B, L, V, D, H = 2, 10, 1000, 512, 8\n",
        "#     decoder = EmbeddingLayer(vocab_size=V, max_length=50, depth=D)\n",
        "#     test_tensor = convert_output_to_tensor(vocab_dict=vocab_dict, vocabulary_size=vocabulary_size)\n",
        "#     tgt_ids = test_tensor.preprocessing([[1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 12, 13, 14, 15], [1, 16, 17, 18, 1, 19, 20, 15]])\n",
        "#     out = decoder(tgt_ids)                                # (B, L, D)\n",
        "#     print(out)\n",
        "#     print(out.shape)  # -> torch.Size([2, 10, 512])\n"
      ],
      "metadata": {
        "id": "Zzy2o9C_1wqN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_length, depth):\n",
        "    super().__init__()\n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
        "\n",
        "    self.token_embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=depth,\n",
        "        mask_zero=True)\n",
        "\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "    self.max_length = max_length\n",
        "\n",
        "  def __call__(self, seq):\n",
        "    # seq = convert_into_index(seq)\n",
        "    print(f\"Calling Sequence embedding: {seq}\")\n",
        "\n",
        "    for _ in range(0, self.max_length-len(seq)):\n",
        "        seq.append(0)\n",
        "\n",
        "    seq = np.array(seq, dtype=np.int32)\n",
        "    print(f\"Sequence for embedding: {seq}\")\n",
        "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
        "\n",
        "    print(f\"After token embedding: {seq.shape}\\n\")\n",
        "\n",
        "    token_emb = seq\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[0])  # (seq)\n",
        "    x = x[tf.newaxis, :]  # (1, seq)\n",
        "    print(x.shape)\n",
        "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
        "\n",
        "    print(f\"After positional embedding: {x.shape}\\n\")\n",
        "\n",
        "    return self.add([seq, token_emb[np.newaxis, :, :]])"
      ],
      "metadata": {
        "id": "NJ2r-3d40nTD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # Use Add instead of + so the keras mask propagates through.\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    attn = self.mha(query=x, value=x,\n",
        "                    use_causal_mask=True)\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "EoFF1KkRKJGv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def __call__(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(\n",
        "             query=x, value=y,\n",
        "             return_attention_scores=True)\n",
        "\n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "dgfIg6SDKX7b"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
        "    ])\n",
        "\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)\n"
      ],
      "metadata": {
        "id": "8r_sGDknKcIH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "\n",
        "\n",
        "  def __call__(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    print(\"Sucessfully entering into the decoder layer\")\n",
        "    # Text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    print(f\"After completing self attention layer: {out_seq.shape}\\n\")\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "\n",
        "    print(f'After completing cross attention layer: {out_seq}\\n')\n",
        "\n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    print(f\"After completing last attention score: {self.last_attention_scores}\\n\")\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    print(f'After feed forward neural network: {out_seq}\\n')\n",
        "\n",
        "    return out_seq"
      ],
      "metadata": {
        "id": "dOFVz8xjKcBi"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=vocabulary_size, activation='softmax', **kwargs)\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens)\n",
        "\n",
        "    print(f\"Counts in the Token Output: {counts}\\n\")\n",
        "\n",
        "    counts_arr = np.zeros(shape=(vocabulary_size,))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    print(f\"Counts array after removing banned tokens: {counts_arr}\\n\")\n",
        "    print(f'Shape of the counts array: {counts_arr.shape}\\n')\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    print(f\"Total: {total}\\n\")\n",
        "    p = counts_arr/total   ## get the empirical probability of each token. This is used to compute entropy and bias terms in probability space.\n",
        "    print(f\"Probability: {p}\\n\")\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "    print(f\"log_p: {log_p}\\n\")\n",
        "\n",
        "    entropy = -(log_p*p).sum()  ## Computes the marginal entropy. it describe how much uncertainty remains if you only knew the marginal token distribution.\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(vocabulary_size):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "    print(f\"Bias: {self.bias}\\n\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    print(f\"Successfully enter into the output layer: {x}\")\n",
        "    print(f\"input shape of the output layer: {x.shape}\")\n",
        "    x = self.dense(x)\n",
        "    # TODO(b/250038731): Fix this.\n",
        "    # An Add layer doesn't work because of the different shapes.\n",
        "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
        "    # the losses.\n",
        "    return x"
      ],
      "metadata": {
        "id": "y_Agoh--KXOc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert_into_index('the dog is playing with the stick on the beach')"
      ],
      "metadata": {
        "id": "KeXeteI9-c9A"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Converting the caption into index***"
      ],
      "metadata": {
        "id": "FGsVP3mo_xUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for index, caption in enumerate(df_train['key'], start=0):\n",
        "#     caption = caption.replace(\"  \", \" \")\n",
        "#     caption = caption.replace(\"[END]\", \"\")\n",
        "#     df_train.at[index, 'key'] = convert_into_index(caption)"
      ],
      "metadata": {
        "id": "9ewYKbf79r02"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for index, caption in enumerate(df_test['key'], start=0):\n",
        "#     caption = caption.replace(\"  \", \" \")\n",
        "#     caption = caption.replace(\"[END]\", \"\")\n",
        "#     df_test.at[index, 'key'] = convert_into_index(caption)"
      ],
      "metadata": {
        "id": "DBOy7cOI_w5V"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train"
      ],
      "metadata": {
        "id": "FB2c8wsK_iEU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in vocab_dict.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "    if value == 2:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Z73KEeBP3n",
        "outputId": "d9961805-b582-47e0-8166-445d5a2429ac"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAD]: 0\n",
            "[START]: 1\n",
            "a: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = TokenOutput(banned_tokens=['[START]', '[PAD]'])\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(df['caption'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqYXixcyrolb",
        "outputId": "5bfb3af8-619f-4ee0-8cba-6057bf3b99ce"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40168/40168 [00:00<00:00, 708877.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts in the Token Output: Counter({2: 62683, 16: 40168, 4: 18876, 24: 18283, 39: 10707, 7: 9316, 30: 8840, 29: 8018, 36: 7756, 76: 7248, 11: 6707, 47: 5571, 41: 3927, 28: 3801, 147: 3575, 32: 3486, 235: 3400, 17: 3323, 25: 3172, 102: 3061, 45: 2913, 198: 2881, 142: 2752, 111: 2669, 74: 2629, 42: 2556, 13: 2430, 80: 2356, 163: 2268, 48: 2057, 119: 2035, 35: 1977, 124: 1976, 79: 1968, 387: 1821, 162: 1806, 143: 1787, 135: 1773, 22: 1766, 66: 1586, 3: 1543, 188: 1534, 175: 1465, 178: 1445, 167: 1402, 58: 1385, 347: 1373, 63: 1367, 92: 1324, 65: 1275, 9: 1259, 179: 1258, 81: 1247, 64: 1232, 128: 1225, 145: 1217, 197: 1216, 223: 1212, 26: 1178, 156: 1164, 439: 1151, 275: 1113, 19: 1070, 228: 1050, 137: 1041, 130: 1025, 224: 983, 231: 970, 186: 953, 193: 947, 46: 942, 211: 920, 132: 919, 647: 902, 129: 894, 139: 869, 295: 867, 549: 858, 436: 844, 75: 786, 38: 772, 375: 759, 87: 749, 141: 743, 165: 743, 51: 742, 95: 741, 5: 735, 1232: 731, 277: 725, 257: 693, 357: 691, 491: 690, 96: 680, 290: 677, 458: 665, 907: 652, 242: 645, 160: 633, 398: 624, 404: 595, 551: 586, 225: 581, 788: 578, 57: 577, 661: 563, 299: 562, 150: 554, 297: 552, 514: 551, 395: 529, 261: 525, 351: 518, 67: 515, 190: 512, 21: 510, 90: 503, 8: 497, 438: 491, 381: 485, 203: 482, 1338: 482, 208: 471, 120: 470, 484: 470, 281: 467, 161: 464, 437: 462, 318: 461, 478: 452, 348: 441, 241: 439, 419: 435, 298: 434, 424: 432, 37: 429, 540: 427, 949: 427, 256: 419, 346: 418, 172: 417, 401: 417, 552: 417, 471: 413, 282: 405, 82: 403, 511: 402, 70: 401, 418: 399, 486: 397, 622: 397, 283: 392, 2927: 392, 40: 388, 421: 378, 553: 378, 879: 378, 133: 377, 503: 377, 78: 375, 653: 375, 1174: 373, 217: 367, 955: 365, 255: 359, 548: 358, 94: 356, 483: 356, 6: 348, 98: 346, 245: 342, 444: 340, 123: 339, 250: 336, 523: 334, 238: 331, 675: 325, 151: 318, 1052: 317, 570: 316, 205: 314, 519: 312, 874: 310, 423: 306, 55: 305, 71: 304, 146: 302, 575: 302, 1009: 297, 400: 291, 1230: 287, 20: 284, 1127: 283, 1495: 283, 489: 281, 237: 279, 1211: 277, 207: 276, 433: 276, 1404: 270, 694: 268, 191: 261, 350: 260, 598: 259, 291: 258, 368: 258, 635: 256, 308: 254, 243: 251, 1273: 250, 559: 249, 109: 248, 169: 247, 232: 247, 505: 247, 963: 247, 61: 246, 248: 246, 259: 246, 482: 245, 962: 245, 740: 244, 821: 240, 429: 236, 938: 234, 600: 231, 853: 231, 385: 227, 878: 226, 164: 225, 1531: 224, 617: 223, 273: 221, 581: 218, 406: 217, 976: 216, 397: 215, 116: 211, 672: 209, 234: 208, 296: 208, 405: 208, 100: 206, 451: 204, 509: 204, 166: 203, 402: 202, 522: 201, 827: 201, 118: 200, 113: 198, 174: 198, 470: 196, 196: 195, 420: 195, 885: 195, 321: 194, 840: 193, 148: 192, 262: 190, 403: 190, 533: 190, 91: 189, 1115: 188, 229: 184, 538: 184, 762: 184, 388: 183, 324: 182, 729: 181, 2612: 180, 122: 178, 301: 178, 183: 177, 263: 175, 527: 174, 706: 174, 227: 172, 809: 172, 1179: 172, 966: 171, 134: 170, 136: 170, 584: 170, 215: 169, 417: 169, 586: 169, 805: 169, 2191: 168, 1085: 165, 233: 163, 354: 163, 501: 162, 412: 161, 665: 160, 337: 159, 656: 159, 526: 158, 543: 158, 1418: 158, 159: 157, 446: 156, 1435: 156, 219: 155, 2186: 155, 278: 154, 220: 153, 409: 153, 790: 153, 1049: 153, 204: 152, 222: 152, 18: 149, 1124: 149, 209: 148, 474: 148, 185: 147, 54: 146, 349: 146, 355: 146, 671: 146, 857: 146, 899: 145, 1488: 145, 369: 144, 3617: 144, 269: 142, 973: 142, 114: 141, 1740: 141, 244: 139, 425: 139, 170: 138, 463: 138, 493: 137, 778: 137, 851: 137, 1448: 136, 1098: 135, 734: 134, 1059: 134, 562: 132, 649: 132, 877: 132, 1705: 132, 93: 131, 1114: 131, 435: 130, 943: 130, 33: 129, 270: 128, 294: 128, 154: 127, 226: 125, 530: 125, 572: 124, 539: 123, 580: 123, 735: 123, 964: 123, 1010: 123, 289: 121, 906: 121, 1176: 121, 739: 119, 1050: 119, 1141: 119, 1599: 119, 2051: 119, 587: 118, 213: 117, 697: 116, 1535: 116, 99: 115, 666: 115, 719: 115, 1116: 115, 1360: 115, 396: 114, 158: 113, 252: 113, 461: 113, 392: 112, 284: 111, 594: 111, 722: 111, 944: 111, 1233: 111, 695: 110, 12: 109, 602: 109, 10: 108, 356: 108, 477: 108, 563: 108, 691: 108, 901: 108, 230: 107, 276: 106, 310: 106, 525: 106, 745: 106, 1083: 106, 1332: 106, 1357: 106, 1016: 105, 88: 104, 378: 104, 1246: 104, 415: 103, 1228: 103, 1942: 102, 667: 101, 748: 101, 761: 101, 1383: 101, 1937: 101, 534: 100, 657: 100, 1639: 100, 510: 99, 624: 99, 705: 98, 839: 98, 867: 98, 210: 97, 502: 97, 1438: 97, 1525: 97, 339: 96, 855: 96, 1173: 96, 253: 95, 326: 95, 504: 95, 1297: 95, 1447: 94, 1560: 94, 459: 93, 619: 93, 1017: 93, 1382: 93, 367: 92, 574: 92, 605: 92, 627: 92, 1161: 92, 1748: 92, 829: 91, 1056: 91, 1309: 91, 1623: 91, 650: 90, 1074: 90, 1118: 90, 302: 89, 565: 89, 965: 89, 1420: 89, 1452: 89, 856: 88, 1571: 88, 2376: 88, 138: 87, 618: 87, 763: 87, 1167: 87, 3489: 87, 303: 86, 641: 86, 690: 86, 858: 86, 1134: 86, 293: 85, 1047: 85, 1736: 85, 1788: 85, 376: 84, 323: 83, 361: 83, 365: 83, 585: 83, 3059: 83, 340: 82, 633: 82, 727: 82, 1568: 82, 239: 81, 342: 81, 932: 81, 1213: 81, 1275: 81, 1314: 81, 455: 80, 507: 80, 583: 80, 1417: 80, 644: 79, 717: 79, 796: 79, 1255: 79, 1737: 79, 1991: 79, 2246: 79, 184: 78, 601: 78, 621: 78, 1643: 78, 2003: 78, 330: 77, 384: 77, 416: 77, 480: 77, 1838: 77, 264: 76, 495: 76, 728: 76, 939: 76, 1234: 76, 2008: 76, 2240: 76, 443: 75, 781: 75, 1570: 75, 2029: 75, 441: 74, 596: 74, 1322: 74, 1651: 74, 2033: 74, 389: 73, 490: 73, 769: 73, 1393: 73, 2154: 73, 306: 72, 556: 72, 638: 72, 797: 72, 837: 72, 1133: 72, 1264: 72, 1477: 72, 338: 71, 1262: 71, 140: 70, 171: 70, 279: 70, 524: 70, 908: 70, 1492: 70, 108: 69, 636: 69, 921: 69, 304: 68, 558: 68, 1069: 68, 3209: 68, 182: 67, 268: 67, 579: 67, 613: 67, 733: 67, 1020: 67, 1038: 67, 1076: 67, 1225: 67, 1461: 67, 2481: 67, 2920: 67, 251: 66, 462: 66, 520: 66, 592: 66, 655: 66, 951: 66, 978: 66, 1679: 66, 2093: 66, 3513: 66, 180: 65, 221: 65, 1265: 65, 1427: 65, 2050: 65, 2192: 65, 2508: 65, 610: 64, 700: 64, 1454: 64, 1803: 64, 3220: 64, 59: 63, 336: 63, 947: 63, 982: 63, 1070: 63, 1951: 63, 2002: 63, 5196: 63, 593: 62, 703: 62, 1215: 62, 1532: 62, 2405: 62, 56: 61, 331: 61, 472: 61, 497: 61, 737: 61, 1028: 61, 1244: 61, 86: 60, 775: 60, 1138: 60, 1203: 60, 1722: 60, 925: 59, 979: 59, 992: 59, 1958: 59, 595: 58, 957: 58, 1066: 58, 1117: 58, 1440: 58, 1483: 58, 1669: 58, 1870: 58, 1949: 58, 2197: 58, 2557: 58, 44: 57, 110: 57, 422: 57, 518: 57, 1272: 57, 1595: 57, 1685: 57, 1880: 57, 3219: 57, 77: 56, 608: 56, 904: 56, 926: 56, 1067: 56, 1356: 56, 1954: 56, 2045: 56, 2596: 56, 168: 55, 246: 55, 394: 55, 576: 55, 849: 55, 883: 55, 1542: 55, 1625: 55, 1751: 55, 121: 54, 2955: 54, 212: 53, 515: 53, 597: 53, 746: 53, 823: 53, 832: 53, 1399: 53, 1572: 53, 1720: 53, 2347: 53, 4163: 53, 366: 52, 410: 52, 473: 52, 488: 52, 1423: 52, 1486: 52, 1539: 52, 2076: 52, 2556: 52, 2573: 52, 83: 51, 876: 51, 1253: 51, 1432: 51, 1437: 51, 1563: 51, 1637: 51, 1699: 51, 1899: 51, 15: 50, 216: 50, 456: 50, 777: 50, 1280: 50, 1490: 50, 2271: 50, 3091: 50, 3526: 50, 513: 49, 577: 49, 774: 49, 852: 49, 983: 49, 1064: 49, 1247: 49, 1278: 49, 1342: 49, 1814: 49, 1817: 49, 1841: 49, 2012: 49, 2163: 49, 2237: 49, 3291: 49, 3911: 49, 52: 48, 426: 48, 428: 48, 447: 48, 836: 48, 954: 48, 1005: 48, 1163: 48, 1654: 48, 1873: 48, 2124: 48, 469: 47, 1650: 47, 2270: 47, 332: 46, 1090: 46, 1561: 46, 1999: 46, 2126: 46, 49: 45, 104: 45, 214: 45, 218: 45, 512: 45, 766: 45, 1319: 45, 1498: 45, 1612: 45, 2058: 45, 2130: 45, 2141: 45, 2647: 45, 2808: 45, 3229: 45, 460: 44, 625: 44, 750: 44, 911: 44, 914: 44, 1282: 44, 1425: 44, 1476: 44, 1482: 44, 1609: 44, 1657: 44, 1747: 44, 73: 43, 89: 43, 157: 43, 320: 43, 465: 43, 545: 43, 634: 43, 678: 43, 886: 43, 956: 43, 1241: 43, 1294: 43, 1300: 43, 1390: 43, 1499: 43, 2475: 43, 3909: 43, 3919: 43, 4210: 43, 4322: 43, 990: 42, 1071: 42, 1320: 42, 1327: 42, 1387: 42, 1433: 42, 2147: 42, 2789: 42, 3686: 42, 4012: 42, 53: 41, 390: 41, 749: 41, 887: 41, 1474: 41, 1946: 41, 2484: 41, 2944: 41, 3393: 41, 430: 40, 434: 40, 882: 40, 896: 40, 919: 40, 1312: 40, 1543: 40, 1585: 40, 1935: 40, 2135: 40, 2167: 40, 2479: 40, 2522: 40, 2677: 40, 3318: 40, 4688: 40, 105: 39, 329: 39, 620: 39, 648: 39, 768: 39, 811: 39, 969: 39, 1162: 39, 1223: 39, 1276: 39, 1631: 39, 1632: 39, 1728: 39, 4628: 39, 117: 38, 333: 38, 506: 38, 578: 38, 819: 38, 1008: 38, 1025: 38, 1037: 38, 1137: 38, 1139: 38, 1206: 38, 1291: 38, 1606: 38, 2114: 38, 2442: 38, 2628: 38, 3061: 38, 3089: 38, 31: 37, 131: 37, 240: 37, 272: 37, 591: 37, 685: 37, 741: 37, 912: 37, 1043: 37, 1086: 37, 1128: 37, 1150: 37, 1580: 37, 1852: 37, 2026: 37, 2092: 37, 2330: 37, 2359: 37, 2471: 37, 2488: 37, 2981: 37, 3261: 37, 3516: 37, 3958: 37, 464: 36, 546: 36, 669: 36, 844: 36, 1613: 36, 1767: 36, 1872: 36, 1988: 36, 2007: 36, 2354: 36, 2462: 36, 305: 35, 364: 35, 535: 35, 673: 35, 756: 35, 996: 35, 1220: 35, 1328: 35, 1718: 35, 2168: 35, 2196: 35, 2217: 35, 2465: 35, 2613: 35, 2846: 35, 2991: 35, 4835: 35, 206: 34, 319: 34, 432: 34, 498: 34, 933: 34, 1072: 34, 1126: 34, 1144: 34, 1467: 34, 1468: 34, 1945: 34, 1964: 34, 2075: 34, 2417: 34, 3029: 34, 176: 33, 452: 33, 681: 33, 710: 33, 738: 33, 847: 33, 1065: 33, 1224: 33, 1546: 33, 1874: 33, 2067: 33, 2582: 33, 380: 32, 632: 32, 689: 32, 787: 32, 793: 32, 875: 32, 929: 32, 972: 32, 1207: 32, 1259: 32, 1487: 32, 1778: 32, 1842: 32, 1900: 32, 1977: 32, 2143: 32, 2251: 32, 2461: 32, 2792: 32, 3072: 32, 3162: 32, 4196: 32, 126: 31, 200: 31, 358: 31, 453: 31, 764: 31, 806: 31, 995: 31, 1457: 31, 2466: 31, 2951: 31, 3635: 31, 3646: 31, 3711: 31, 3747: 31, 4280: 31, 62: 30, 325: 30, 327: 30, 555: 30, 623: 30, 680: 30, 818: 30, 897: 30, 968: 30, 1046: 30, 1087: 30, 1231: 30, 1358: 30, 1485: 30, 1567: 30, 1630: 30, 1678: 30, 1749: 30, 1759: 30, 1824: 30, 2298: 30, 2534: 30, 2942: 30, 3066: 30, 3093: 30, 3289: 30, 3534: 30, 3770: 30, 43: 29, 236: 29, 431: 29, 873: 29, 934: 29, 1048: 29, 1099: 29, 1311: 29, 1711: 29, 1774: 29, 1917: 29, 1990: 29, 2052: 29, 2360: 29, 2521: 29, 2859: 29, 2892: 29, 2940: 29, 3705: 29, 3724: 29, 177: 28, 254: 28, 309: 28, 542: 28, 905: 28, 985: 28, 998: 28, 1000: 28, 1142: 28, 1329: 28, 1374: 28, 1493: 28, 1501: 28, 1716: 28, 1750: 28, 1794: 28, 2022: 28, 2079: 28, 2166: 28, 2501: 28, 2532: 28, 2591: 28, 3078: 28, 3295: 28, 3299: 28, 3684: 28, 4277: 28, 201: 27, 521: 27, 544: 27, 747: 27, 861: 27, 931: 27, 967: 27, 1146: 27, 1153: 27, 1188: 27, 1260: 27, 1446: 27, 1602: 27, 1869: 27, 2337: 27, 2368: 27, 2731: 27, 2868: 27, 3118: 27, 3665: 27, 3906: 27, 3978: 27, 274: 26, 285: 26, 352: 26, 373: 26, 528: 26, 676: 26, 986: 26, 1012: 26, 1068: 26, 1073: 26, 1183: 26, 1372: 26, 1384: 26, 1484: 26, 1496: 26, 1547: 26, 1598: 26, 1626: 26, 1638: 26, 1738: 26, 1940: 26, 2027: 26, 2028: 26, 2039: 26, 2146: 26, 2155: 26, 2315: 26, 2469: 26, 2745: 26, 2794: 26, 2842: 26, 2886: 26, 3206: 26, 3862: 26, 4303: 26, 5737: 26, 152: 25, 517: 25, 731: 25, 948: 25, 1021: 25, 1040: 25, 1084: 25, 1352: 25, 1415: 25, 1586: 25, 1603: 25, 1729: 25, 1770: 25, 1840: 25, 1875: 25, 1972: 25, 2229: 25, 2258: 25, 2525: 25, 2690: 25, 2782: 25, 2932: 25, 3064: 25, 3157: 25, 3666: 25, 3702: 25, 3836: 25, 5193: 25, 5904: 25, 265: 24, 383: 24, 560: 24, 679: 24, 801: 24, 975: 24, 993: 24, 997: 24, 1226: 24, 1373: 24, 1645: 24, 1856: 24, 1890: 24, 1934: 24, 2077: 24, 2200: 24, 2280: 24, 2498: 24, 2528: 24, 2546: 24, 2638: 24, 2795: 24, 3102: 24, 3247: 24, 3861: 24, 3923: 24, 4217: 24, 4283: 24, 4302: 24, 6623: 24, 153: 23, 379: 23, 628: 23, 765: 23, 1160: 23, 1245: 23, 1365: 23, 1927: 23, 1953: 23, 2100: 23, 2122: 23, 2152: 23, 2243: 23, 2284: 23, 2302: 23, 2307: 23, 2826: 23, 2872: 23, 2921: 23, 3281: 23, 3390: 23, 3460: 23, 3563: 23, 6781: 23, 60: 22, 334: 22, 529: 22, 603: 22, 743: 22, 795: 22, 950: 22, 1094: 22, 1217: 22, 1341: 22, 1344: 22, 1363: 22, 1375: 22, 1764: 22, 1963: 22, 2017: 22, 2061: 22, 2066: 22, 2084: 22, 2183: 22, 2212: 22, 2276: 22, 2464: 22, 2549: 22, 2668: 22, 2674: 22, 2696: 22, 3034: 22, 3478: 22, 3587: 22, 3703: 22, 3823: 22, 4619: 22, 629: 21, 670: 21, 709: 21, 716: 21, 800: 21, 890: 21, 920: 21, 1002: 21, 1097: 21, 1102: 21, 1194: 21, 1377: 21, 1600: 21, 1642: 21, 1785: 21, 1894: 21, 2001: 21, 2175: 21, 2219: 21, 2403: 21, 2542: 21, 2895: 21, 2900: 21, 2941: 21, 2946: 21, 3464: 21, 3499: 21, 3589: 21, 3621: 21, 3653: 21, 3661: 21, 3977: 21, 4609: 21, 4923: 21, 5986: 21, 84: 20, 144: 20, 292: 20, 300: 20, 450: 20, 569: 20, 668: 20, 744: 20, 785: 20, 927: 20, 1019: 20, 1026: 20, 1164: 20, 1184: 20, 1347: 20, 1429: 20, 1430: 20, 1597: 20, 1647: 20, 1661: 20, 1793: 20, 1859: 20, 1906: 20, 1948: 20, 1992: 20, 2036: 20, 2040: 20, 2454: 20, 2494: 20, 2634: 20, 2712: 20, 2738: 20, 2784: 20, 2849: 20, 2857: 20, 3268: 20, 3378: 20, 3841: 20, 4358: 20, 4447: 20, 4541: 20, 5192: 20, 6035: 20, 615: 19, 645: 19, 696: 19, 1110: 19, 1155: 19, 1242: 19, 1299: 19, 1345: 19, 1350: 19, 1662: 19, 1686: 19, 1734: 19, 1798: 19, 1802: 19, 1883: 19, 1901: 19, 1968: 19, 2239: 19, 2324: 19, 2377: 19, 2419: 19, 2617: 19, 2684: 19, 2727: 19, 2763: 19, 3006: 19, 3011: 19, 3141: 19, 3178: 19, 3244: 19, 4134: 19, 4167: 19, 4304: 19, 6408: 19, 440: 18, 449: 18, 609: 18, 612: 18, 1055: 18, 1062: 18, 1208: 18, 1236: 18, 1240: 18, 1313: 18, 1380: 18, 1386: 18, 1445: 18, 1494: 18, 1534: 18, 1556: 18, 1799: 18, 1810: 18, 1851: 18, 2072: 18, 2081: 18, 2097: 18, 2109: 18, 2151: 18, 2263: 18, 2441: 18, 2480: 18, 2604: 18, 2713: 18, 2743: 18, 2873: 18, 2926: 18, 3098: 18, 3156: 18, 3285: 18, 3428: 18, 3500: 18, 3501: 18, 3536: 18, 3739: 18, 4031: 18, 4060: 18, 4061: 18, 4085: 18, 4187: 18, 4273: 18, 4276: 18, 4359: 18, 4565: 18, 4740: 18, 537: 17, 557: 17, 599: 17, 604: 17, 720: 17, 753: 17, 816: 17, 1092: 17, 1140: 17, 1202: 17, 1412: 17, 1518: 17, 1533: 17, 1549: 17, 1553: 17, 1660: 17, 1742: 17, 1781: 17, 1805: 17, 1847: 17, 1887: 17, 1902: 17, 1920: 17, 1947: 17, 2015: 17, 2041: 17, 2088: 17, 2106: 17, 2164: 17, 2213: 17, 2264: 17, 2268: 17, 2277: 17, 2343: 17, 2391: 17, 2406: 17, 2509: 17, 2685: 17, 2771: 17, 3106: 17, 3294: 17, 3306: 17, 3316: 17, 3331: 17, 3334: 17, 3455: 17, 3543: 17, 3562: 17, 3624: 17, 3650: 17, 4305: 17, 4395: 17, 4575: 17, 4616: 17, 5174: 17, 7138: 17, 359: 16, 360: 16, 589: 16, 699: 16, 859: 16, 898: 16, 953: 16, 991: 16, 1045: 16, 1057: 16, 1058: 16, 1122: 16, 1149: 16, 1201: 16, 1221: 16, 1403: 16, 1526: 16, 1555: 16, 1641: 16, 1682: 16, 1697: 16, 1714: 16, 1821: 16, 1959: 16, 2053: 16, 2069: 16, 2105: 16, 2121: 16, 2140: 16, 2234: 16, 2283: 16, 2336: 16, 2413: 16, 2538: 16, 2551: 16, 2691: 16, 2703: 16, 2802: 16, 2911: 16, 3053: 16, 3095: 16, 3164: 16, 3191: 16, 3256: 16, 3372: 16, 3509: 16, 3936: 16, 4077: 16, 4228: 16, 4664: 16, 4915: 16, 4974: 16, 5463: 16, 6303: 16, 85: 15, 247: 15, 249: 15, 313: 15, 317: 15, 442: 15, 771: 15, 772: 15, 888: 15, 1015: 15, 1129: 15, 1145: 15, 1239: 15, 1248: 15, 1333: 15, 1388: 15, 1389: 15, 1459: 15, 1505: 15, 1582: 15, 1665: 15, 1666: 15, 1668: 15, 1725: 15, 1732: 15, 1757: 15, 1867: 15, 1913: 15, 1925: 15, 1932: 15, 2129: 15, 2170: 15, 2254: 15, 2319: 15, 2387: 15, 2482: 15, 2492: 15, 2539: 15, 2576: 15, 2584: 15, 2623: 15, 2698: 15, 2717: 15, 2749: 15, 2816: 15, 2860: 15, 2906: 15, 2956: 15, 3043: 15, 3195: 15, 3253: 15, 3377: 15, 3454: 15, 3461: 15, 3517: 15, 3572: 15, 3685: 15, 3718: 15, 3732: 15, 3908: 15, 3924: 15, 3928: 15, 4002: 15, 4072: 15, 4140: 15, 4152: 15, 4370: 15, 4862: 15, 5369: 15, 5386: 15, 5586: 15, 6574: 15, 34: 14, 72: 14, 411: 14, 531: 14, 701: 14, 730: 14, 791: 14, 863: 14, 892: 14, 922: 14, 994: 14, 1014: 14, 1095: 14, 1112: 14, 1157: 14, 1185: 14, 1250: 14, 1277: 14, 1316: 14, 1369: 14, 1371: 14, 1395: 14, 1502: 14, 1538: 14, 1566: 14, 1648: 14, 1710: 14, 1780: 14, 1911: 14, 1921: 14, 1928: 14, 1956: 14, 2054: 14, 2250: 14, 2286: 14, 2349: 14, 2355: 14, 2443: 14, 2495: 14, 2541: 14, 2620: 14, 2661: 14, 2702: 14, 2879: 14, 3137: 14, 3154: 14, 3169: 14, 3235: 14, 3308: 14, 3358: 14, 3447: 14, 3557: 14, 3706: 14, 3729: 14, 3943: 14, 4033: 14, 4071: 14, 4079: 14, 4164: 14, 4239: 14, 4496: 14, 4497: 14, 4692: 14, 4765: 14, 4798: 14, 4978: 14, 6407: 14, 280: 13, 413: 13, 547: 13, 723: 13, 755: 13, 758: 13, 803: 13, 917: 13, 945: 13, 989: 13, 1109: 13, 1261: 13, 1267: 13, 1268: 13, 1271: 13, 1353: 13, 1381: 13, 1455: 13, 1479: 13, 1491: 13, 1573: 13, 1578: 13, 1614: 13, 1621: 13, 1636: 13, 1671: 13, 1723: 13, 1735: 13, 1754: 13, 1836: 13, 1903: 13, 2009: 13, 2010: 13, 2150: 13, 2195: 13, 2202: 13, 2230: 13, 2252: 13, 2335: 13, 2384: 13, 2394: 13, 2477: 13, 2478: 13, 2569: 13, 2606: 13, 2636: 13, 2645: 13, 2652: 13, 2672: 13, 2766: 13, 2799: 13, 3017: 13, 3036: 13, 3152: 13, 3179: 13, 3275: 13, 3292: 13, 3307: 13, 4235: 13, 4377: 13, 4451: 13, 4549: 13, 4778: 13, 5349: 13, 5406: 13, 5620: 13, 6469: 13, 6775: 13, 7494: 13, 286: 12, 335: 12, 343: 12, 541: 12, 646: 12, 652: 12, 674: 12, 683: 12, 742: 12, 792: 12, 845: 12, 880: 12, 941: 12, 960: 12, 1042: 12, 1096: 12, 1107: 12, 1180: 12, 1197: 12, 1198: 12, 1283: 12, 1400: 12, 1424: 12, 1471: 12, 1480: 12, 1537: 12, 1562: 12, 1695: 12, 1698: 12, 1704: 12, 1792: 12, 1795: 12, 1849: 12, 1892: 12, 1997: 12, 2037: 12, 2064: 12, 2293: 12, 2325: 12, 2329: 12, 2379: 12, 2438: 12, 2496: 12, 2544: 12, 2657: 12, 2701: 12, 2750: 12, 2767: 12, 2934: 12, 3007: 12, 3024: 12, 3039: 12, 3097: 12, 3200: 12, 3218: 12, 3264: 12, 3462: 12, 3491: 12, 3522: 12, 3709: 12, 3831: 12, 3947: 12, 3948: 12, 4009: 12, 4078: 12, 4109: 12, 4114: 12, 4173: 12, 4306: 12, 4380: 12, 4442: 12, 4502: 12, 4653: 12, 4675: 12, 4725: 12, 5053: 12, 5764: 12, 6380: 12, 372: 11, 427: 11, 487: 11, 492: 11, 588: 11, 704: 11, 714: 11, 725: 11, 752: 11, 834: 11, 870: 11, 889: 11, 915: 11, 981: 11, 1119: 11, 1148: 11, 1151: 11, 1170: 11, 1182: 11, 1249: 11, 1325: 11, 1368: 11, 1379: 11, 1413: 11, 1463: 11, 1478: 11, 1504: 11, 1544: 11, 1715: 11, 1719: 11, 1741: 11, 1755: 11, 1879: 11, 1924: 11, 2014: 11, 2046: 11, 2070: 11, 2110: 11, 2125: 11, 2158: 11, 2160: 11, 2176: 11, 2181: 11, 2314: 11, 2580: 11, 2754: 11, 2791: 11, 2876: 11, 3056: 11, 3131: 11, 3212: 11, 3475: 11, 3505: 11, 3593: 11, 3667: 11, 3682: 11, 3772: 11, 3822: 11, 3939: 11, 3963: 11, 4023: 11, 4058: 11, 4242: 11, 4290: 11, 4297: 11, 4319: 11, 4550: 11, 4805: 11, 4833: 11, 4901: 11, 5099: 11, 5602: 11, 6179: 11, 6364: 11, 7714: 11, 370: 10, 516: 10, 561: 10, 631: 10, 693: 10, 865: 10, 1111: 10, 1156: 10, 1178: 10, 1195: 10, 1205: 10, 1209: 10, 1229: 10, 1307: 10, 1391: 10, 1421: 10, 1444: 10, 1451: 10, 1500: 10, 1519: 10, 1584: 10, 1644: 10, 1664: 10, 1676: 10, 1680: 10, 1709: 10, 1819: 10, 1831: 10, 1857: 10, 1884: 10, 1966: 10, 1971: 10, 1993: 10, 2073: 10, 2118: 10, 2128: 10, 2236: 10, 2256: 10, 2260: 10, 2312: 10, 2322: 10, 2404: 10, 2512: 10, 2536: 10, 2605: 10, 2673: 10, 2679: 10, 2682: 10, 2729: 10, 2790: 10, 2803: 10, 2835: 10, 2854: 10, 2890: 10, 2923: 10, 3135: 10, 3198: 10, 3231: 10, 3232: 10, 3330: 10, 3338: 10, 3345: 10, 3382: 10, 3451: 10, 3486: 10, 3506: 10, 3574: 10, 3575: 10, 3604: 10, 3613: 10, 3750: 10, 3776: 10, 3780: 10, 3795: 10, 3882: 10, 4133: 10, 4231: 10, 4292: 10, 4301: 10, 4309: 10, 4325: 10, 4353: 10, 4396: 10, 4409: 10, 4429: 10, 4445: 10, 4450: 10, 4519: 10, 4598: 10, 4646: 10, 4796: 10, 4994: 10, 4995: 10, 5023: 10, 5165: 10, 5333: 10, 5404: 10, 5415: 10, 5434: 10, 5507: 10, 5682: 10, 5802: 10, 5906: 10, 6003: 10, 6197: 10, 6624: 10, 7676: 10, 125: 9, 322: 9, 408: 9, 485: 9, 607: 9, 626: 9, 642: 9, 682: 9, 817: 9, 828: 9, 871: 9, 895: 9, 930: 9, 935: 9, 1007: 9, 1093: 9, 1100: 9, 1101: 9, 1106: 9, 1199: 9, 1212: 9, 1235: 9, 1252: 9, 1281: 9, 1335: 9, 1337: 9, 1351: 9, 1370: 9, 1434: 9, 1509: 9, 1516: 9, 1536: 9, 1581: 9, 1633: 9, 1687: 9, 1727: 9, 1752: 9, 1762: 9, 1769: 9, 1777: 9, 1790: 9, 1815: 9, 1816: 9, 1827: 9, 1846: 9, 1896: 9, 1910: 9, 1957: 9, 1962: 9, 1981: 9, 1983: 9, 1985: 9, 2006: 9, 2048: 9, 2080: 9, 2094: 9, 2188: 9, 2211: 9, 2224: 9, 2228: 9, 2245: 9, 2265: 9, 2292: 9, 2365: 9, 2432: 9, 2458: 9, 2470: 9, 2472: 9, 2493: 9, 2527: 9, 2593: 9, 2595: 9, 2653: 9, 2675: 9, 2710: 9, 2736: 9, 2737: 9, 2739: 9, 2759: 9, 2818: 9, 2824: 9, 2845: 9, 2851: 9, 2877: 9, 2891: 9, 2904: 9, 2916: 9, 2967: 9, 2989: 9, 2990: 9, 2994: 9, 3023: 9, 3042: 9, 3114: 9, 3127: 9, 3173: 9, 3262: 9, 3363: 9, 3379: 9, 3383: 9, 3384: 9, 3396: 9, 3469: 9, 3490: 9, 3495: 9, 3546: 9, 3561: 9, 3581: 9, 3678: 9, 3744: 9, 3745: 9, 3803: 9, 3865: 9, 3980: 9, 3987: 9, 4026: 9, 4036: 9, 4059: 9, 4063: 9, 4067: 9, 4082: 9, 4093: 9, 4110: 9, 4249: 9, 4307: 9, 4398: 9, 4431: 9, 4452: 9, 4508: 9, 4531: 9, 4538: 9, 4627: 9, 4667: 9, 4680: 9, 4697: 9, 4800: 9, 4902: 9, 4921: 9, 5160: 9, 5388: 9, 5468: 9, 5478: 9, 5639: 9, 6027: 9, 6058: 9, 6105: 9, 6596: 9, 6608: 9, 6646: 9, 6871: 9, 7008: 9, 7066: 9, 7306: 9, 7648: 9, 8006: 9, 8399: 9, 97: 8, 127: 8, 202: 8, 258: 8, 314: 8, 341: 8, 345: 8, 391: 8, 662: 8, 664: 8, 759: 8, 807: 8, 913: 8, 946: 8, 952: 8, 970: 8, 977: 8, 1018: 8, 1113: 8, 1120: 8, 1132: 8, 1143: 8, 1152: 8, 1169: 8, 1186: 8, 1187: 8, 1216: 8, 1279: 8, 1396: 8, 1407: 8, 1408: 8, 1414: 8, 1464: 8, 1470: 8, 1508: 8, 1524: 8, 1557: 8, 1677: 8, 1683: 8, 1868: 8, 1922: 8, 1973: 8, 1987: 8, 2019: 8, 2047: 8, 2142: 8, 2207: 8, 2218: 8, 2244: 8, 2262: 8, 2272: 8, 2288: 8, 2299: 8, 2304: 8, 2331: 8, 2389: 8, 2439: 8, 2445: 8, 2560: 8, 2590: 8, 2619: 8, 2643: 8, 2666: 8, 2741: 8, 2780: 8, 2805: 8, 2809: 8, 2810: 8, 2828: 8, 2880: 8, 2888: 8, 2937: 8, 2952: 8, 3018: 8, 3088: 8, 3144: 8, 3194: 8, 3240: 8, 3263: 8, 3277: 8, 3287: 8, 3322: 8, 3344: 8, 3364: 8, 3368: 8, 3394: 8, 3533: 8, 3585: 8, 3590: 8, 3594: 8, 3614: 8, 3630: 8, 3636: 8, 3671: 8, 3700: 8, 3715: 8, 3731: 8, 3733: 8, 3752: 8, 3754: 8, 3783: 8, 3824: 8, 3879: 8, 3953: 8, 3973: 8, 3974: 8, 3979: 8, 4118: 8, 4194: 8, 4232: 8, 4245: 8, 4251: 8, 4259: 8, 4444: 8, 4472: 8, 4490: 8, 4524: 8, 4630: 8, 4686: 8, 4759: 8, 4763: 8, 4823: 8, 4981: 8, 5008: 8, 5073: 8, 5158: 8, 5159: 8, 5179: 8, 5194: 8, 5261: 8, 5346: 8, 5362: 8, 5462: 8, 5479: 8, 5504: 8, 5548: 8, 5576: 8, 5657: 8, 5661: 8, 5840: 8, 5875: 8, 6040: 8, 6348: 8, 6812: 8, 6933: 8, 189: 7, 194: 7, 266: 7, 363: 7, 414: 7, 499: 7, 568: 7, 590: 7, 639: 7, 658: 7, 732: 7, 757: 7, 848: 7, 862: 7, 942: 7, 1013: 7, 1029: 7, 1044: 7, 1089: 7, 1103: 7, 1108: 7, 1177: 7, 1292: 7, 1323: 7, 1362: 7, 1450: 7, 1458: 7, 1469: 7, 1472: 7, 1517: 7, 1551: 7, 1552: 7, 1576: 7, 1616: 7, 1688: 7, 1758: 7, 1766: 7, 1773: 7, 1878: 7, 1885: 7, 1907: 7, 1908: 7, 1938: 7, 1950: 7, 2023: 7, 2034: 7, 2055: 7, 2059: 7, 2083: 7, 2090: 7, 2095: 7, 2133: 7, 2173: 7, 2184: 7, 2189: 7, 2259: 7, 2328: 7, 2345: 7, 2437: 7, 2444: 7, 2451: 7, 2485: 7, 2543: 7, 2563: 7, 2572: 7, 2625: 7, 2641: 7, 2662: 7, 2686: 7, 2688: 7, 2800: 7, 2820: 7, 2821: 7, 2834: 7, 2839: 7, 2933: 7, 2947: 7, 2959: 7, 2993: 7, 3004: 7, 3014: 7, 3016: 7, 3028: 7, 3065: 7, 3087: 7, 3100: 7, 3101: 7, 3108: 7, 3115: 7, 3136: 7, 3163: 7, 3168: 7, 3203: 7, 3216: 7, 3298: 7, 3355: 7, 3360: 7, 3375: 7, 3450: 7, 3530: 7, 3553: 7, 3623: 7, 3707: 7, 3719: 7, 3741: 7, 3789: 7, 3866: 7, 3912: 7, 3976: 7, 3998: 7, 4011: 7, 4029: 7, 4065: 7, 4070: 7, 4073: 7, 4089: 7, 4097: 7, 4115: 7, 4119: 7, 4193: 7, 4227: 7, 4263: 7, 4267: 7, 4326: 7, 4355: 7, 4393: 7, 4422: 7, 4464: 7, 4469: 7, 4492: 7, 4522: 7, 4533: 7, 4558: 7, 4623: 7, 4659: 7, 4671: 7, 4702: 7, 4757: 7, 4766: 7, 4777: 7, 4820: 7, 4872: 7, 4876: 7, 4908: 7, 4911: 7, 4986: 7, 5055: 7, 5072: 7, 5074: 7, 5115: 7, 5144: 7, 5172: 7, 5200: 7, 5239: 7, 5313: 7, 5454: 7, 5483: 7, 5494: 7, 5498: 7, 5509: 7, 5561: 7, 5651: 7, 5873: 7, 5923: 7, 5947: 7, 6006: 7, 6208: 7, 6662: 7, 6676: 7, 6680: 7, 6715: 7, 7006: 7, 7608: 7, 23: 6, 69: 6, 107: 6, 312: 6, 353: 6, 374: 6, 382: 6, 393: 6, 614: 6, 718: 6, 721: 6, 780: 6, 782: 6, 794: 6, 810: 6, 815: 6, 825: 6, 841: 6, 958: 6, 961: 6, 1006: 6, 1031: 6, 1063: 6, 1121: 6, 1159: 6, 1168: 6, 1218: 6, 1274: 6, 1315: 6, 1336: 6, 1409: 6, 1466: 6, 1511: 6, 1514: 6, 1548: 6, 1564: 6, 1618: 6, 1620: 6, 1622: 6, 1635: 6, 1658: 6, 1690: 6, 1706: 6, 1753: 6, 1784: 6, 1796: 6, 1845: 6, 1861: 6, 1882: 6, 1893: 6, 1936: 6, 1944: 6, 2057: 6, 2091: 6, 2096: 6, 2127: 6, 2136: 6, 2204: 6, 2215: 6, 2231: 6, 2242: 6, 2274: 6, 2317: 6, 2348: 6, 2375: 6, 2415: 6, 2448: 6, 2453: 6, 2486: 6, 2490: 6, 2537: 6, 2540: 6, 2577: 6, 2578: 6, 2629: 6, 2631: 6, 2639: 6, 2654: 6, 2663: 6, 2667: 6, 2681: 6, 2683: 6, 2693: 6, 2699: 6, 2704: 6, 2730: 6, 2755: 6, 2770: 6, 2807: 6, 2847: 6, 2874: 6, 2961: 6, 2998: 6, 3021: 6, 3027: 6, 3035: 6, 3055: 6, 3062: 6, 3082: 6, 3192: 6, 3193: 6, 3233: 6, 3255: 6, 3260: 6, 3266: 6, 3273: 6, 3309: 6, 3327: 6, 3335: 6, 3415: 6, 3459: 6, 3472: 6, 3474: 6, 3645: 6, 3676: 6, 3681: 6, 3697: 6, 3701: 6, 3755: 6, 3759: 6, 3779: 6, 3804: 6, 3805: 6, 3813: 6, 3849: 6, 3854: 6, 3868: 6, 3872: 6, 3895: 6, 3896: 6, 3914: 6, 3922: 6, 3926: 6, 3959: 6, 3961: 6, 3972: 6, 4047: 6, 4069: 6, 4091: 6, 4106: 6, 4138: 6, 4154: 6, 4214: 6, 4258: 6, 4294: 6, 4296: 6, 4324: 6, 4335: 6, 4350: 6, 4417: 6, 4475: 6, 4500: 6, 4633: 6, 4689: 6, 4694: 6, 4787: 6, 4790: 6, 4812: 6, 4842: 6, 4871: 6, 4886: 6, 4920: 6, 4983: 6, 5026: 6, 5034: 6, 5098: 6, 5133: 6, 5155: 6, 5163: 6, 5189: 6, 5191: 6, 5209: 6, 5226: 6, 5260: 6, 5409: 6, 5440: 6, 5449: 6, 5482: 6, 5524: 6, 5529: 6, 5538: 6, 5569: 6, 5625: 6, 5660: 6, 5714: 6, 5723: 6, 5749: 6, 5803: 6, 5834: 6, 5874: 6, 5921: 6, 5926: 6, 5980: 6, 6037: 6, 6059: 6, 6060: 6, 6118: 6, 6135: 6, 6154: 6, 6323: 6, 6468: 6, 6499: 6, 6535: 6, 6611: 6, 6874: 6, 6939: 6, 6975: 6, 7060: 6, 7064: 6, 7270: 6, 7594: 6, 7609: 6, 7755: 6, 7828: 6, 7875: 6, 8029: 6, 8486: 6, 8487: 6, 50: 5, 181: 5, 187: 5, 271: 5, 328: 5, 500: 5, 508: 5, 550: 5, 630: 5, 640: 5, 751: 5, 824: 5, 835: 5, 843: 5, 854: 5, 860: 5, 869: 5, 928: 5, 937: 5, 974: 5, 987: 5, 999: 5, 1033: 5, 1039: 5, 1041: 5, 1075: 5, 1082: 5, 1105: 5, 1130: 5, 1154: 5, 1158: 5, 1227: 5, 1254: 5, 1269: 5, 1284: 5, 1290: 5, 1293: 5, 1298: 5, 1324: 5, 1340: 5, 1343: 5, 1376: 5, 1431: 5, 1453: 5, 1475: 5, 1513: 5, 1522: 5, 1541: 5, 1569: 5, 1589: 5, 1594: 5, 1607: 5, 1629: 5, 1634: 5, 1663: 5, 1692: 5, 1700: 5, 1703: 5, 1721: 5, 1811: 5, 1826: 5, 1832: 5, 1850: 5, 1853: 5, 1865: 5, 1888: 5, 1895: 5, 1898: 5, 1965: 5, 1967: 5, 1969: 5, 1974: 5, 1978: 5, 2000: 5, 2005: 5, 2032: 5, 2113: 5, 2123: 5, 2178: 5, 2182: 5, 2193: 5, 2216: 5, 2275: 5, 2278: 5, 2294: 5, 2301: 5, 2310: 5, 2316: 5, 2318: 5, 2333: 5, 2358: 5, 2363: 5, 2371: 5, 2400: 5, 2401: 5, 2427: 5, 2468: 5, 2558: 5, 2562: 5, 2564: 5, 2588: 5, 2601: 5, 2610: 5, 2611: 5, 2618: 5, 2622: 5, 2630: 5, 2632: 5, 2665: 5, 2753: 5, 2756: 5, 2757: 5, 2760: 5, 2761: 5, 2774: 5, 2778: 5, 2779: 5, 2819: 5, 2838: 5, 2841: 5, 2866: 5, 2871: 5, 2884: 5, 2898: 5, 2922: 5, 2950: 5, 2963: 5, 2968: 5, 2970: 5, 2982: 5, 2983: 5, 2995: 5, 3003: 5, 3015: 5, 3051: 5, 3067: 5, 3111: 5, 3125: 5, 3134: 5, 3138: 5, 3142: 5, 3143: 5, 3145: 5, 3146: 5, 3155: 5, 3158: 5, 3204: 5, 3227: 5, 3228: 5, 3241: 5, 3242: 5, 3246: 5, 3317: 5, 3365: 5, 3385: 5, 3437: 5, 3480: 5, 3503: 5, 3514: 5, 3521: 5, 3531: 5, 3535: 5, 3548: 5, 3568: 5, 3569: 5, 3586: 5, 3626: 5, 3638: 5, 3658: 5, 3659: 5, 3664: 5, 3669: 5, 3674: 5, 3721: 5, 3725: 5, 3771: 5, 3773: 5, 3796: 5, 3856: 5, 3859: 5, 3860: 5, 3903: 5, 3904: 5, 3946: 5, 3949: 5, 3965: 5, 3967: 5, 3993: 5, 4000: 5, 4075: 5, 4081: 5, 4094: 5, 4116: 5, 4120: 5, 4141: 5, 4143: 5, 4147: 5, 4229: 5, 4264: 5, 4323: 5, 4333: 5, 4334: 5, 4342: 5, 4357: 5, 4360: 5, 4361: 5, 4394: 5, 4400: 5, 4407: 5, 4432: 5, 4440: 5, 4454: 5, 4489: 5, 4495: 5, 4503: 5, 4516: 5, 4529: 5, 4557: 5, 4564: 5, 4569: 5, 4574: 5, 4654: 5, 4660: 5, 4668: 5, 4679: 5, 4684: 5, 4685: 5, 4726: 5, 4751: 5, 4768: 5, 4780: 5, 4784: 5, 4802: 5, 4811: 5, 4851: 5, 4857: 5, 4890: 5, 4903: 5, 4958: 5, 4977: 5, 4999: 5, 5021: 5, 5037: 5, 5067: 5, 5094: 5, 5100: 5, 5104: 5, 5118: 5, 5176: 5, 5211: 5, 5216: 5, 5222: 5, 5229: 5, 5245: 5, 5268: 5, 5273: 5, 5275: 5, 5281: 5, 5318: 5, 5322: 5, 5371: 5, 5376: 5, 5391: 5, 5393: 5, 5397: 5, 5411: 5, 5419: 5, 5426: 5, 5428: 5, 5435: 5, 5442: 5, 5512: 5, 5533: 5, 5547: 5, 5585: 5, 5592: 5, 5603: 5, 5622: 5, 5637: 5, 5656: 5, 5679: 5, 5713: 5, 5753: 5, 5810: 5, 5894: 5, 5981: 5, 6007: 5, 6038: 5, 6076: 5, 6088: 5, 6113: 5, 6130: 5, 6138: 5, 6204: 5, 6229: 5, 6260: 5, 6308: 5, 6329: 5, 6350: 5, 6360: 5, 6365: 5, 6370: 5, 6422: 5, 6484: 5, 6494: 5, 6538: 5, 6593: 5, 6621: 5, 6631: 5, 6660: 5, 6667: 5, 6691: 5, 6727: 5, 6784: 5, 6805: 5, 6814: 5, 6815: 5, 6844: 5, 6862: 5, 6863: 5, 6885: 5, 6920: 5, 6960: 5, 7024: 5, 7079: 5, 7132: 5, 7139: 5, 7178: 5, 7179: 5, 7336: 5, 7381: 5, 7404: 5, 7457: 5, 7514: 5, 7531: 5, 7649: 5, 7749: 5, 7862: 5, 8193: 5, 8329: 5, 8337: 5, 8764: 5, 8800: 5, 27: 4, 192: 4, 457: 4, 467: 4, 532: 4, 566: 4, 616: 4, 651: 4, 687: 4, 692: 4, 767: 4, 798: 4, 813: 4, 884: 4, 902: 4, 903: 4, 910: 4, 924: 4, 1027: 4, 1035: 4, 1051: 4, 1091: 4, 1166: 4, 1193: 4, 1210: 4, 1270: 4, 1304: 4, 1330: 4, 1348: 4, 1355: 4, 1378: 4, 1456: 4, 1460: 4, 1540: 4, 1575: 4, 1587: 4, 1593: 4, 1611: 4, 1615: 4, 1628: 4, 1655: 4, 1673: 4, 1739: 4, 1771: 4, 1772: 4, 1775: 4, 1779: 4, 1791: 4, 1818: 4, 1823: 4, 1834: 4, 1837: 4, 1862: 4, 1881: 4, 1886: 4, 1909: 4, 1919: 4, 1931: 4, 1994: 4, 1996: 4, 2004: 4, 2024: 4, 2044: 4, 2085: 4, 2098: 4, 2111: 4, 2131: 4, 2145: 4, 2149: 4, 2157: 4, 2194: 4, 2198: 4, 2205: 4, 2210: 4, 2223: 4, 2241: 4, 2249: 4, 2261: 4, 2285: 4, 2295: 4, 2313: 4, 2334: 4, 2352: 4, 2357: 4, 2378: 4, 2386: 4, 2402: 4, 2420: 4, 2431: 4, 2434: 4, 2435: 4, 2459: 4, 2474: 4, 2497: 4, 2510: 4, 2511: 4, 2514: 4, 2531: 4, 2545: 4, 2547: 4, 2550: 4, 2552: 4, 2585: 4, 2594: 4, 2597: 4, 2598: 4, 2600: 4, 2608: 4, 2633: 4, 2671: 4, 2687: 4, 2695: 4, 2700: 4, 2707: 4, 2723: 4, 2724: 4, 2734: 4, 2744: 4, 2777: 4, 2783: 4, 2797: 4, 2804: 4, 2814: 4, 2823: 4, 2832: 4, 2844: 4, 2893: 4, 2894: 4, 2896: 4, 2901: 4, 2908: 4, 2953: 4, 2965: 4, 2979: 4, 2980: 4, 2986: 4, 2996: 4, 3022: 4, 3026: 4, 3044: 4, 3070: 4, 3094: 4, 3110: 4, 3140: 4, 3147: 4, 3153: 4, 3161: 4, 3207: 4, 3208: 4, 3234: 4, 3237: 4, 3321: 4, 3324: 4, 3328: 4, 3371: 4, 3381: 4, 3397: 4, 3400: 4, 3401: 4, 3402: 4, 3405: 4, 3425: 4, 3436: 4, 3456: 4, 3485: 4, 3487: 4, 3493: 4, 3498: 4, 3519: 4, 3573: 4, 3600: 4, 3601: 4, 3625: 4, 3628: 4, 3629: 4, 3639: 4, 3644: 4, 3708: 4, 3738: 4, 3746: 4, 3760: 4, 3764: 4, 3775: 4, 3792: 4, 3793: 4, 3799: 4, 3832: 4, 3847: 4, 3875: 4, 3885: 4, 3891: 4, 3898: 4, 3910: 4, 3920: 4, 3921: 4, 3929: 4, 3994: 4, 4016: 4, 4018: 4, 4028: 4, 4039: 4, 4044: 4, 4048: 4, 4096: 4, 4107: 4, 4121: 4, 4124: 4, 4126: 4, 4149: 4, 4151: 4, 4153: 4, 4156: 4, 4171: 4, 4176: 4, 4190: 4, 4197: 4, 4209: 4, 4216: 4, 4223: 4, 4224: 4, 4225: 4, 4226: 4, 4250: 4, 4252: 4, 4253: 4, 4269: 4, 4299: 4, 4318: 4, 4375: 4, 4391: 4, 4401: 4, 4421: 4, 4424: 4, 4458: 4, 4483: 4, 4488: 4, 4504: 4, 4505: 4, 4509: 4, 4518: 4, 4546: 4, 4547: 4, 4560: 4, 4562: 4, 4573: 4, 4579: 4, 4581: 4, 4590: 4, 4612: 4, 4635: 4, 4650: 4, 4652: 4, 4661: 4, 4670: 4, 4691: 4, 4709: 4, 4713: 4, 4723: 4, 4730: 4, 4749: 4, 4773: 4, 4782: 4, 4786: 4, 4816: 4, 4827: 4, 4828: 4, 4832: 4, 4839: 4, 4850: 4, 4863: 4, 4874: 4, 4877: 4, 4880: 4, 4884: 4, 4904: 4, 4926: 4, 4931: 4, 4936: 4, 4946: 4, 4951: 4, 4957: 4, 4964: 4, 4969: 4, 4975: 4, 4982: 4, 4985: 4, 5001: 4, 5005: 4, 5006: 4, 5020: 4, 5064: 4, 5071: 4, 5077: 4, 5080: 4, 5089: 4, 5090: 4, 5103: 4, 5124: 4, 5140: 4, 5145: 4, 5164: 4, 5171: 4, 5177: 4, 5182: 4, 5195: 4, 5207: 4, 5218: 4, 5235: 4, 5243: 4, 5246: 4, 5265: 4, 5274: 4, 5276: 4, 5282: 4, 5296: 4, 5303: 4, 5304: 4, 5327: 4, 5329: 4, 5331: 4, 5345: 4, 5350: 4, 5387: 4, 5399: 4, 5436: 4, 5444: 4, 5477: 4, 5484: 4, 5486: 4, 5491: 4, 5513: 4, 5517: 4, 5571: 4, 5593: 4, 5611: 4, 5616: 4, 5665: 4, 5699: 4, 5707: 4, 5717: 4, 5735: 4, 5743: 4, 5759: 4, 5786: 4, 5807: 4, 5839: 4, 5848: 4, 5855: 4, 5856: 4, 5869: 4, 5887: 4, 5893: 4, 5907: 4, 5915: 4, 5949: 4, 5950: 4, 5953: 4, 5969: 4, 5978: 4, 5999: 4, 6012: 4, 6013: 4, 6019: 4, 6023: 4, 6024: 4, 6042: 4, 6044: 4, 6086: 4, 6095: 4, 6097: 4, 6122: 4, 6134: 4, 6144: 4, 6147: 4, 6159: 4, 6176: 4, 6185: 4, 6254: 4, 6270: 4, 6271: 4, 6387: 4, 6392: 4, 6403: 4, 6443: 4, 6444: 4, 6445: 4, 6467: 4, 6479: 4, 6482: 4, 6496: 4, 6511: 4, 6563: 4, 6577: 4, 6578: 4, 6597: 4, 6609: 4, 6610: 4, 6673: 4, 6690: 4, 6696: 4, 6767: 4, 6776: 4, 6792: 4, 6794: 4, 6864: 4, 6884: 4, 6907: 4, 6929: 4, 6977: 4, 6995: 4, 7052: 4, 7054: 4, 7077: 4, 7080: 4, 7086: 4, 7092: 4, 7113: 4, 7123: 4, 7147: 4, 7156: 4, 7180: 4, 7247: 4, 7286: 4, 7293: 4, 7294: 4, 7341: 4, 7439: 4, 7453: 4, 7459: 4, 7481: 4, 7572: 4, 7573: 4, 7678: 4, 7871: 4, 7890: 4, 7948: 4, 7959: 4, 8177: 4, 8215: 4, 8284: 4, 8499: 4, 8586: 4, 8657: 4, 8700: 4, 8753: 4, 68: 3, 149: 3, 260: 3, 315: 3, 344: 3, 554: 3, 643: 3, 660: 3, 686: 3, 707: 3, 736: 3, 773: 3, 776: 3, 802: 3, 820: 3, 846: 3, 872: 3, 936: 3, 1011: 3, 1024: 3, 1080: 3, 1125: 3, 1131: 3, 1222: 3, 1256: 3, 1331: 3, 1334: 3, 1359: 3, 1392: 3, 1398: 3, 1416: 3, 1436: 3, 1441: 3, 1443: 3, 1449: 3, 1503: 3, 1550: 3, 1565: 3, 1577: 3, 1601: 3, 1610: 3, 1617: 3, 1619: 3, 1640: 3, 1653: 3, 1670: 3, 1681: 3, 1696: 3, 1701: 3, 1707: 3, 1717: 3, 1733: 3, 1756: 3, 1801: 3, 1813: 3, 1822: 3, 1828: 3, 1839: 3, 1844: 3, 1854: 3, 1897: 3, 1914: 3, 1915: 3, 1916: 3, 1926: 3, 1929: 3, 1961: 3, 1980: 3, 1989: 3, 2020: 3, 2030: 3, 2031: 3, 2035: 3, 2087: 3, 2104: 3, 2112: 3, 2120: 3, 2138: 3, 2139: 3, 2148: 3, 2187: 3, 2190: 3, 2220: 3, 2232: 3, 2233: 3, 2235: 3, 2253: 3, 2266: 3, 2273: 3, 2289: 3, 2338: 3, 2339: 3, 2342: 3, 2367: 3, 2374: 3, 2396: 3, 2422: 3, 2426: 3, 2428: 3, 2440: 3, 2449: 3, 2455: 3, 2467: 3, 2506: 3, 2507: 3, 2526: 3, 2553: 3, 2559: 3, 2568: 3, 2575: 3, 2586: 3, 2589: 3, 2621: 3, 2626: 3, 2627: 3, 2637: 3, 2660: 3, 2669: 3, 2689: 3, 2692: 3, 2697: 3, 2716: 3, 2720: 3, 2721: 3, 2728: 3, 2742: 3, 2746: 3, 2751: 3, 2775: 3, 2786: 3, 2788: 3, 2806: 3, 2848: 3, 2867: 3, 2869: 3, 2885: 3, 2909: 3, 2929: 3, 2931: 3, 2943: 3, 2948: 3, 2958: 3, 2962: 3, 2997: 3, 3008: 3, 3031: 3, 3045: 3, 3049: 3, 3057: 3, 3058: 3, 3060: 3, 3071: 3, 3075: 3, 3081: 3, 3084: 3, 3086: 3, 3090: 3, 3104: 3, 3105: 3, 3117: 3, 3151: 3, 3166: 3, 3176: 3, 3180: 3, 3196: 3, 3197: 3, 3202: 3, 3213: 3, 3214: 3, 3230: 3, 3258: 3, 3270: 3, 3274: 3, 3297: 3, 3304: 3, 3311: 3, 3313: 3, 3332: 3, 3342: 3, 3343: 3, 3348: 3, 3361: 3, 3370: 3, 3373: 3, 3376: 3, 3386: 3, 3387: 3, 3406: 3, 3423: 3, 3430: 3, 3452: 3, 3502: 3, 3504: 3, 3507: 3, 3511: 3, 3527: 3, 3532: 3, 3545: 3, 3551: 3, 3554: 3, 3559: 3, 3566: 3, 3580: 3, 3588: 3, 3592: 3, 3598: 3, 3599: 3, 3603: 3, 3619: 3, 3627: 3, 3637: 3, 3643: 3, 3652: 3, 3677: 3, 3680: 3, 3688: 3, 3712: 3, 3714: 3, 3742: 3, 3743: 3, 3757: 3, 3761: 3, 3774: 3, 3782: 3, 3786: 3, 3788: 3, 3797: 3, 3798: 3, 3800: 3, 3801: 3, 3802: 3, 3812: 3, 3819: 3, 3829: 3, 3834: 3, 3837: 3, 3850: 3, 3871: 3, 3873: 3, 3889: 3, 3890: 3, 3892: 3, 3900: 3, 3905: 3, 3916: 3, 3917: 3, 3918: 3, 3931: 3, 3968: 3, 3970: 3, 3975: 3, 3983: 3, 3991: 3, 4017: 3, 4025: 3, 4034: 3, 4037: 3, 4038: 3, 4050: 3, 4052: 3, 4053: 3, 4062: 3, 4074: 3, 4086: 3, 4090: 3, 4092: 3, 4098: 3, 4099: 3, 4102: 3, 4111: 3, 4125: 3, 4159: 3, 4182: 3, 4198: 3, 4201: 3, 4206: 3, 4213: 3, 4221: 3, 4238: 3, 4246: 3, 4266: 3, 4268: 3, 4270: 3, 4279: 3, 4287: 3, 4312: 3, 4316: 3, 4331: 3, 4341: 3, 4352: 3, 4364: 3, 4367: 3, 4381: 3, 4383: 3, 4384: 3, 4387: 3, 4389: 3, 4390: 3, 4403: 3, 4408: 3, 4412: 3, 4413: 3, 4420: 3, 4433: 3, 4439: 3, 4462: 3, 4463: 3, 4466: 3, 4493: 3, 4498: 3, 4511: 3, 4520: 3, 4536: 3, 4543: 3, 4551: 3, 4553: 3, 4561: 3, 4586: 3, 4597: 3, 4599: 3, 4608: 3, 4620: 3, 4624: 3, 4626: 3, 4629: 3, 4649: 3, 4662: 3, 4674: 3, 4682: 3, 4683: 3, 4687: 3, 4695: 3, 4698: 3, 4701: 3, 4705: 3, 4706: 3, 4718: 3, 4719: 3, 4721: 3, 4743: 3, 4752: 3, 4753: 3, 4760: 3, 4771: 3, 4801: 3, 4821: 3, 4834: 3, 4847: 3, 4848: 3, 4858: 3, 4864: 3, 4866: 3, 4875: 3, 4887: 3, 4894: 3, 4895: 3, 4900: 3, 4916: 3, 4917: 3, 4918: 3, 4924: 3, 4928: 3, 4932: 3, 4935: 3, 4938: 3, 4945: 3, 4954: 3, 4976: 3, 4998: 3, 5009: 3, 5015: 3, 5019: 3, 5030: 3, 5031: 3, 5066: 3, 5069: 3, 5076: 3, 5085: 3, 5092: 3, 5096: 3, 5116: 3, 5134: 3, 5143: 3, 5157: 3, 5166: 3, 5183: 3, 5186: 3, 5197: 3, 5198: 3, 5201: 3, 5208: 3, 5210: 3, 5213: 3, 5214: 3, 5227: 3, 5234: 3, 5240: 3, 5250: 3, 5251: 3, 5253: 3, 5262: 3, 5269: 3, 5272: 3, 5279: 3, 5283: 3, 5297: 3, 5298: 3, 5307: 3, 5317: 3, 5326: 3, 5332: 3, 5334: 3, 5335: 3, 5338: 3, 5340: 3, 5347: 3, 5361: 3, 5367: 3, 5373: 3, 5377: 3, 5380: 3, 5382: 3, 5416: 3, 5420: 3, 5421: 3, 5427: 3, 5429: 3, 5433: 3, 5438: 3, 5439: 3, 5448: 3, 5459: 3, 5465: 3, 5474: 3, 5489: 3, 5495: 3, 5505: 3, 5516: 3, 5520: 3, 5525: 3, 5535: 3, 5543: 3, 5552: 3, 5558: 3, 5560: 3, 5570: 3, 5575: 3, 5587: 3, 5609: 3, 5621: 3, 5627: 3, 5628: 3, 5634: 3, 5636: 3, 5644: 3, 5646: 3, 5658: 3, 5667: 3, 5675: 3, 5676: 3, 5685: 3, 5686: 3, 5691: 3, 5695: 3, 5700: 3, 5701: 3, 5703: 3, 5719: 3, 5733: 3, 5760: 3, 5765: 3, 5768: 3, 5769: 3, 5780: 3, 5789: 3, 5798: 3, 5808: 3, 5816: 3, 5822: 3, 5823: 3, 5831: 3, 5847: 3, 5867: 3, 5870: 3, 5871: 3, 5881: 3, 5882: 3, 5883: 3, 5885: 3, 5892: 3, 5901: 3, 5909: 3, 5918: 3, 5936: 3, 5942: 3, 5946: 3, 5965: 3, 5974: 3, 5987: 3, 5990: 3, 6008: 3, 6011: 3, 6041: 3, 6050: 3, 6056: 3, 6065: 3, 6082: 3, 6101: 3, 6123: 3, 6131: 3, 6133: 3, 6139: 3, 6140: 3, 6145: 3, 6166: 3, 6169: 3, 6192: 3, 6199: 3, 6206: 3, 6207: 3, 6215: 3, 6216: 3, 6227: 3, 6257: 3, 6272: 3, 6290: 3, 6302: 3, 6315: 3, 6316: 3, 6328: 3, 6331: 3, 6347: 3, 6358: 3, 6362: 3, 6363: 3, 6368: 3, 6371: 3, 6374: 3, 6382: 3, 6390: 3, 6400: 3, 6405: 3, 6416: 3, 6423: 3, 6425: 3, 6430: 3, 6442: 3, 6447: 3, 6490: 3, 6492: 3, 6495: 3, 6537: 3, 6544: 3, 6570: 3, 6605: 3, 6639: 3, 6644: 3, 6652: 3, 6661: 3, 6678: 3, 6692: 3, 6694: 3, 6700: 3, 6705: 3, 6713: 3, 6733: 3, 6756: 3, 6761: 3, 6783: 3, 6798: 3, 6804: 3, 6808: 3, 6811: 3, 6821: 3, 6828: 3, 6853: 3, 6872: 3, 6958: 3, 6963: 3, 6974: 3, 6992: 3, 7004: 3, 7014: 3, 7021: 3, 7023: 3, 7029: 3, 7032: 3, 7083: 3, 7090: 3, 7097: 3, 7103: 3, 7124: 3, 7128: 3, 7144: 3, 7162: 3, 7176: 3, 7200: 3, 7205: 3, 7225: 3, 7262: 3, 7266: 3, 7289: 3, 7291: 3, 7292: 3, 7296: 3, 7317: 3, 7324: 3, 7326: 3, 7388: 3, 7399: 3, 7406: 3, 7458: 3, 7462: 3, 7504: 3, 7506: 3, 7512: 3, 7516: 3, 7532: 3, 7563: 3, 7585: 3, 7589: 3, 7600: 3, 7619: 3, 7670: 3, 7690: 3, 7699: 3, 7700: 3, 7704: 3, 7742: 3, 7765: 3, 7777: 3, 7799: 3, 7829: 3, 7858: 3, 7861: 3, 7968: 3, 8044: 3, 8061: 3, 8068: 3, 8136: 3, 8142: 3, 8168: 3, 8189: 3, 8225: 3, 8230: 3, 8279: 3, 8289: 3, 8296: 3, 8309: 3, 8346: 3, 8419: 3, 8436: 3, 8464: 3, 8540: 3, 8577: 3, 8578: 3, 8712: 3, 8826: 3, 101: 2, 112: 2, 115: 2, 173: 2, 195: 2, 267: 2, 362: 2, 377: 2, 475: 2, 476: 2, 564: 2, 567: 2, 582: 2, 654: 2, 659: 2, 684: 2, 698: 2, 702: 2, 708: 2, 711: 2, 715: 2, 726: 2, 754: 2, 760: 2, 770: 2, 783: 2, 799: 2, 804: 2, 812: 2, 831: 2, 864: 2, 866: 2, 891: 2, 893: 2, 900: 2, 923: 2, 971: 2, 984: 2, 1023: 2, 1032: 2, 1036: 2, 1060: 2, 1061: 2, 1078: 2, 1081: 2, 1136: 2, 1147: 2, 1191: 2, 1214: 2, 1219: 2, 1237: 2, 1258: 2, 1266: 2, 1301: 2, 1303: 2, 1306: 2, 1326: 2, 1346: 2, 1349: 2, 1361: 2, 1366: 2, 1385: 2, 1406: 2, 1419: 2, 1497: 2, 1506: 2, 1515: 2, 1529: 2, 1530: 2, 1559: 2, 1574: 2, 1579: 2, 1588: 2, 1590: 2, 1591: 2, 1592: 2, 1596: 2, 1605: 2, 1624: 2, 1649: 2, 1656: 2, 1659: 2, 1674: 2, 1684: 2, 1691: 2, 1694: 2, 1713: 2, 1724: 2, 1731: 2, 1745: 2, 1761: 2, 1782: 2, 1783: 2, 1804: 2, 1808: 2, 1820: 2, 1825: 2, 1833: 2, 1860: 2, 1863: 2, 1876: 2, 1877: 2, 1904: 2, 1918: 2, 1923: 2, 1943: 2, 1984: 2, 1995: 2, 1998: 2, 2016: 2, 2021: 2, 2025: 2, 2038: 2, 2056: 2, 2060: 2, 2063: 2, 2065: 2, 2086: 2, 2101: 2, 2102: 2, 2107: 2, 2117: 2, 2132: 2, 2159: 2, 2171: 2, 2174: 2, 2177: 2, 2179: 2, 2180: 2, 2185: 2, 2199: 2, 2208: 2, 2214: 2, 2221: 2, 2222: 2, 2226: 2, 2279: 2, 2281: 2, 2287: 2, 2291: 2, 2300: 2, 2305: 2, 2306: 2, 2309: 2, 2321: 2, 2323: 2, 2344: 2, 2346: 2, 2353: 2, 2356: 2, 2362: 2, 2364: 2, 2366: 2, 2370: 2, 2380: 2, 2395: 2, 2397: 2, 2398: 2, 2410: 2, 2412: 2, 2421: 2, 2425: 2, 2429: 2, 2436: 2, 2450: 2, 2452: 2, 2460: 2, 2473: 2, 2476: 2, 2483: 2, 2489: 2, 2513: 2, 2518: 2, 2523: 2, 2524: 2, 2529: 2, 2533: 2, 2574: 2, 2579: 2, 2615: 2, 2646: 2, 2648: 2, 2649: 2, 2656: 2, 2658: 2, 2670: 2, 2676: 2, 2708: 2, 2711: 2, 2715: 2, 2719: 2, 2722: 2, 2733: 2, 2747: 2, 2748: 2, 2758: 2, 2762: 2, 2769: 2, 2772: 2, 2776: 2, 2801: 2, 2812: 2, 2817: 2, 2829: 2, 2831: 2, 2840: 2, 2843: 2, 2850: 2, 2852: 2, 2853: 2, 2856: 2, 2858: 2, 2861: 2, 2875: 2, 2902: 2, 2907: 2, 2918: 2, 2939: 2, 2954: 2, 2957: 2, 2969: 2, 2971: 2, 2973: 2, 2975: 2, 2976: 2, 2977: 2, 2985: 2, 2988: 2, 2999: 2, 3010: 2, 3013: 2, 3020: 2, 3030: 2, 3047: 2, 3054: 2, 3068: 2, 3069: 2, 3073: 2, 3080: 2, 3083: 2, 3085: 2, 3092: 2, 3096: 2, 3103: 2, 3109: 2, 3116: 2, 3126: 2, 3132: 2, 3148: 2, 3160: 2, 3170: 2, 3172: 2, 3181: 2, 3183: 2, 3210: 2, 3223: 2, 3224: 2, 3238: 2, 3243: 2, 3250: 2, 3252: 2, 3254: 2, 3257: 2, 3265: 2, 3269: 2, 3282: 2, 3286: 2, 3305: 2, 3323: 2, 3329: 2, 3337: 2, 3346: 2, 3351: 2, 3359: 2, 3399: 2, 3407: 2, 3409: 2, 3414: 2, 3418: 2, 3419: 2, 3420: 2, 3421: 2, 3422: 2, 3426: 2, 3432: 2, 3435: 2, 3439: 2, 3440: 2, 3441: 2, 3446: 2, 3453: 2, 3457: 2, 3465: 2, 3466: 2, 3468: 2, 3473: 2, 3476: 2, 3479: 2, 3488: 2, 3512: 2, 3515: 2, 3524: 2, 3525: 2, 3537: 2, 3541: 2, 3547: 2, 3549: 2, 3558: 2, 3564: 2, 3567: 2, 3571: 2, 3578: 2, 3579: 2, 3602: 2, 3605: 2, 3606: 2, 3612: 2, 3616: 2, 3618: 2, 3622: 2, 3647: 2, 3649: 2, 3655: 2, 3663: 2, 3687: 2, 3695: 2, 3698: 2, 3699: 2, 3704: 2, 3713: 2, 3735: 2, 3737: 2, 3740: 2, 3749: 2, 3766: 2, 3784: 2, 3806: 2, 3807: 2, 3808: 2, 3817: 2, 3825: 2, 3833: 2, 3838: 2, 3839: 2, 3843: 2, 3845: 2, 3851: 2, 3857: 2, 3858: 2, 3864: 2, 3870: 2, 3874: 2, 3878: 2, 3880: 2, 3883: 2, 3884: 2, 3886: 2, 3899: 2, 3907: 2, 3925: 2, 3932: 2, 3951: 2, 3952: 2, 3992: 2, 4001: 2, 4004: 2, 4008: 2, 4019: 2, 4021: 2, 4030: 2, 4035: 2, 4043: 2, 4045: 2, 4049: 2, 4057: 2, 4064: 2, 4076: 2, 4083: 2, 4084: 2, 4087: 2, 4100: 2, 4101: 2, 4104: 2, 4112: 2, 4131: 2, 4136: 2, 4137: 2, 4142: 2, 4146: 2, 4148: 2, 4150: 2, 4160: 2, 4162: 2, 4169: 2, 4172: 2, 4179: 2, 4185: 2, 4186: 2, 4199: 2, 4203: 2, 4205: 2, 4212: 2, 4219: 2, 4230: 2, 4236: 2, 4237: 2, 4240: 2, 4261: 2, 4262: 2, 4275: 2, 4288: 2, 4291: 2, 4298: 2, 4300: 2, 4315: 2, 4317: 2, 4339: 2, 4343: 2, 4345: 2, 4346: 2, 4347: 2, 4349: 2, 4356: 2, 4362: 2, 4373: 2, 4378: 2, 4379: 2, 4385: 2, 4388: 2, 4404: 2, 4405: 2, 4406: 2, 4414: 2, 4428: 2, 4434: 2, 4436: 2, 4449: 2, 4456: 2, 4457: 2, 4459: 2, 4470: 2, 4471: 2, 4474: 2, 4485: 2, 4486: 2, 4499: 2, 4501: 2, 4512: 2, 4515: 2, 4525: 2, 4528: 2, 4542: 2, 4548: 2, 4555: 2, 4563: 2, 4576: 2, 4578: 2, 4582: 2, 4584: 2, 4587: 2, 4589: 2, 4591: 2, 4592: 2, 4593: 2, 4596: 2, 4600: 2, 4607: 2, 4611: 2, 4615: 2, 4617: 2, 4618: 2, 4631: 2, 4632: 2, 4634: 2, 4636: 2, 4637: 2, 4641: 2, 4645: 2, 4655: 2, 4690: 2, 4703: 2, 4704: 2, 4711: 2, 4715: 2, 4724: 2, 4736: 2, 4742: 2, 4744: 2, 4745: 2, 4746: 2, 4750: 2, 4761: 2, 4762: 2, 4764: 2, 4775: 2, 4776: 2, 4783: 2, 4785: 2, 4788: 2, 4792: 2, 4803: 2, 4806: 2, 4808: 2, 4818: 2, 4824: 2, 4829: 2, 4831: 2, 4837: 2, 4838: 2, 4843: 2, 4844: 2, 4846: 2, 4849: 2, 4854: 2, 4855: 2, 4865: 2, 4868: 2, 4870: 2, 4878: 2, 4879: 2, 4882: 2, 4888: 2, 4889: 2, 4906: 2, 4913: 2, 4929: 2, 4949: 2, 4950: 2, 4955: 2, 4962: 2, 4973: 2, 4979: 2, 4996: 2, 5000: 2, 5003: 2, 5012: 2, 5024: 2, 5025: 2, 5033: 2, 5042: 2, 5050: 2, 5052: 2, 5057: 2, 5058: 2, 5065: 2, 5068: 2, 5084: 2, 5086: 2, 5088: 2, 5095: 2, 5101: 2, 5107: 2, 5109: 2, 5111: 2, 5127: 2, 5139: 2, 5152: 2, 5154: 2, 5156: 2, 5170: 2, 5173: 2, 5190: 2, 5199: 2, 5204: 2, 5205: 2, 5206: 2, 5212: 2, 5217: 2, 5223: 2, 5231: 2, 5252: 2, 5254: 2, 5255: 2, 5256: 2, 5258: 2, 5263: 2, 5277: 2, 5292: 2, 5295: 2, 5300: 2, 5308: 2, 5311: 2, 5315: 2, 5316: 2, 5324: 2, 5330: 2, 5337: 2, 5339: 2, 5343: 2, 5351: 2, 5353: 2, 5356: 2, 5365: 2, 5370: 2, 5384: 2, 5389: 2, 5390: 2, 5400: 2, 5412: 2, 5413: 2, 5414: 2, 5417: 2, 5418: 2, 5422: 2, 5423: 2, 5441: 2, 5450: 2, 5452: 2, 5453: 2, 5467: 2, 5503: 2, 5508: 2, 5514: 2, 5518: 2, 5519: 2, 5521: 2, 5526: 2, 5528: 2, 5534: 2, 5540: 2, 5542: 2, 5546: 2, 5549: 2, 5555: 2, 5559: 2, 5563: 2, 5565: 2, 5567: 2, 5572: 2, 5573: 2, 5583: 2, 5606: 2, 5617: 2, 5626: 2, 5638: 2, 5642: 2, 5643: 2, 5645: 2, 5654: 2, 5659: 2, 5662: 2, 5668: 2, 5670: 2, 5671: 2, 5672: 2, 5680: 2, 5681: 2, 5684: 2, 5704: 2, 5710: 2, 5725: 2, 5726: 2, 5728: 2, 5736: 2, 5739: 2, 5751: 2, 5757: 2, 5761: 2, 5762: 2, 5770: 2, 5771: 2, 5779: 2, 5782: 2, 5783: 2, 5791: 2, 5812: 2, 5817: 2, 5821: 2, 5826: 2, 5829: 2, 5833: 2, 5841: 2, 5842: 2, 5850: 2, 5854: 2, 5857: 2, 5878: 2, 5884: 2, 5888: 2, 5890: 2, 5897: 2, 5911: 2, 5916: 2, 5917: 2, 5928: 2, 5932: 2, 5933: 2, 5948: 2, 5958: 2, 5960: 2, 5961: 2, 5966: 2, 5973: 2, 5977: 2, 5984: 2, 5989: 2, 5992: 2, 5993: 2, 5996: 2, 6014: 2, 6015: 2, 6016: 2, 6021: 2, 6025: 2, 6033: 2, 6045: 2, 6048: 2, 6052: 2, 6057: 2, 6061: 2, 6062: 2, 6068: 2, 6069: 2, 6072: 2, 6112: 2, 6114: 2, 6115: 2, 6125: 2, 6126: 2, 6128: 2, 6129: 2, 6136: 2, 6150: 2, 6155: 2, 6160: 2, 6163: 2, 6167: 2, 6170: 2, 6172: 2, 6187: 2, 6191: 2, 6193: 2, 6195: 2, 6202: 2, 6210: 2, 6212: 2, 6218: 2, 6222: 2, 6228: 2, 6230: 2, 6234: 2, 6238: 2, 6240: 2, 6246: 2, 6261: 2, 6262: 2, 6265: 2, 6274: 2, 6275: 2, 6276: 2, 6278: 2, 6283: 2, 6286: 2, 6289: 2, 6291: 2, 6293: 2, 6299: 2, 6305: 2, 6306: 2, 6307: 2, 6309: 2, 6311: 2, 6312: 2, 6317: 2, 6321: 2, 6324: 2, 6325: 2, 6335: 2, 6340: 2, 6345: 2, 6346: 2, 6352: 2, 6353: 2, 6359: 2, 6361: 2, 6375: 2, 6376: 2, 6381: 2, 6384: 2, 6386: 2, 6397: 2, 6398: 2, 6399: 2, 6435: 2, 6437: 2, 6438: 2, 6441: 2, 6452: 2, 6456: 2, 6460: 2, 6465: 2, 6472: 2, 6473: 2, 6485: 2, 6493: 2, 6497: 2, 6498: 2, 6502: 2, 6504: 2, 6507: 2, 6512: 2, 6513: 2, 6524: 2, 6533: 2, 6542: 2, 6543: 2, 6546: 2, 6550: 2, 6551: 2, 6552: 2, 6556: 2, 6559: 2, 6567: 2, 6573: 2, 6581: 2, 6583: 2, 6588: 2, 6589: 2, 6592: 2, 6603: 2, 6612: 2, 6620: 2, 6622: 2, 6627: 2, 6630: 2, 6632: 2, 6633: 2, 6635: 2, 6651: 2, 6655: 2, 6668: 2, 6671: 2, 6683: 2, 6684: 2, 6687: 2, 6688: 2, 6695: 2, 6698: 2, 6703: 2, 6704: 2, 6707: 2, 6708: 2, 6712: 2, 6721: 2, 6725: 2, 6728: 2, 6731: 2, 6741: 2, 6744: 2, 6750: 2, 6751: 2, 6755: 2, 6757: 2, 6758: 2, 6760: 2, 6774: 2, 6780: 2, 6787: 2, 6789: 2, 6793: 2, 6795: 2, 6803: 2, 6809: 2, 6840: 2, 6848: 2, 6854: 2, 6858: 2, 6860: 2, 6869: 2, 6879: 2, 6901: 2, 6902: 2, 6912: 2, 6922: 2, 6930: 2, 6937: 2, 6942: 2, 6947: 2, 6959: 2, 6983: 2, 6988: 2, 6991: 2, 6993: 2, 7011: 2, 7018: 2, 7019: 2, 7030: 2, 7031: 2, 7037: 2, 7039: 2, 7045: 2, 7049: 2, 7051: 2, 7055: 2, 7069: 2, 7074: 2, 7075: 2, 7099: 2, 7105: 2, 7114: 2, 7116: 2, 7118: 2, 7126: 2, 7143: 2, 7150: 2, 7151: 2, 7168: 2, 7170: 2, 7193: 2, 7198: 2, 7202: 2, 7207: 2, 7209: 2, 7212: 2, 7217: 2, 7218: 2, 7224: 2, 7228: 2, 7230: 2, 7232: 2, 7240: 2, 7241: 2, 7243: 2, 7244: 2, 7248: 2, 7249: 2, 7251: 2, 7257: 2, 7259: 2, 7260: 2, 7267: 2, 7274: 2, 7281: 2, 7295: 2, 7298: 2, 7300: 2, 7301: 2, 7307: 2, 7311: 2, 7320: 2, 7329: 2, 7331: 2, 7343: 2, 7344: 2, 7358: 2, 7360: 2, 7372: 2, 7373: 2, 7374: 2, 7378: 2, 7379: 2, 7380: 2, 7389: 2, 7396: 2, 7398: 2, 7414: 2, 7417: 2, 7422: 2, 7425: 2, 7435: 2, 7437: 2, 7440: 2, 7449: 2, 7452: 2, 7460: 2, 7463: 2, 7464: 2, 7473: 2, 7474: 2, 7479: 2, 7480: 2, 7491: 2, 7502: 2, 7510: 2, 7511: 2, 7523: 2, 7525: 2, 7551: 2, 7565: 2, 7568: 2, 7574: 2, 7577: 2, 7578: 2, 7611: 2, 7613: 2, 7615: 2, 7618: 2, 7624: 2, 7628: 2, 7636: 2, 7637: 2, 7645: 2, 7646: 2, 7652: 2, 7658: 2, 7662: 2, 7664: 2, 7666: 2, 7677: 2, 7688: 2, 7692: 2, 7694: 2, 7706: 2, 7710: 2, 7712: 2, 7726: 2, 7727: 2, 7741: 2, 7743: 2, 7745: 2, 7746: 2, 7758: 2, 7760: 2, 7763: 2, 7766: 2, 7768: 2, 7773: 2, 7790: 2, 7792: 2, 7796: 2, 7801: 2, 7819: 2, 7847: 2, 7848: 2, 7849: 2, 7869: 2, 7877: 2, 7880: 2, 7896: 2, 7900: 2, 7911: 2, 7919: 2, 7942: 2, 7947: 2, 7954: 2, 7956: 2, 7957: 2, 7964: 2, 7979: 2, 7982: 2, 7985: 2, 7991: 2, 7994: 2, 7997: 2, 8001: 2, 8017: 2, 8018: 2, 8019: 2, 8030: 2, 8031: 2, 8038: 2, 8041: 2, 8052: 2, 8054: 2, 8056: 2, 8066: 2, 8083: 2, 8096: 2, 8109: 2, 8118: 2, 8135: 2, 8140: 2, 8154: 2, 8159: 2, 8169: 2, 8170: 2, 8192: 2, 8194: 2, 8198: 2, 8213: 2, 8216: 2, 8229: 2, 8238: 2, 8249: 2, 8264: 2, 8265: 2, 8266: 2, 8272: 2, 8287: 2, 8297: 2, 8301: 2, 8308: 2, 8311: 2, 8319: 2, 8326: 2, 8341: 2, 8349: 2, 8362: 2, 8364: 2, 8375: 2, 8378: 2, 8424: 2, 8425: 2, 8427: 2, 8434: 2, 8437: 2, 8438: 2, 8446: 2, 8451: 2, 8452: 2, 8475: 2, 8495: 2, 8506: 2, 8544: 2, 8558: 2, 8562: 2, 8569: 2, 8570: 2, 8579: 2, 8611: 2, 8633: 2, 8642: 2, 8652: 2, 8673: 2, 8674: 2, 8678: 2, 8699: 2, 8706: 2, 8711: 2, 8734: 2, 8750: 2, 8751: 2, 8762: 2, 8784: 2, 8806: 2, 14: 1, 103: 1, 106: 1, 155: 1, 199: 1, 287: 1, 288: 1, 307: 1, 311: 1, 316: 1, 371: 1, 386: 1, 399: 1, 407: 1, 445: 1, 448: 1, 454: 1, 466: 1, 468: 1, 479: 1, 481: 1, 494: 1, 496: 1, 536: 1, 571: 1, 573: 1, 606: 1, 611: 1, 637: 1, 663: 1, 677: 1, 688: 1, 712: 1, 713: 1, 724: 1, 779: 1, 784: 1, 786: 1, 789: 1, 808: 1, 814: 1, 822: 1, 826: 1, 830: 1, 833: 1, 838: 1, 842: 1, 850: 1, 868: 1, 881: 1, 894: 1, 909: 1, 916: 1, 918: 1, 940: 1, 959: 1, 980: 1, 988: 1, 1001: 1, 1003: 1, 1004: 1, 1022: 1, 1030: 1, 1034: 1, 1053: 1, 1054: 1, 1077: 1, 1079: 1, 1088: 1, 1104: 1, 1123: 1, 1135: 1, 1165: 1, 1171: 1, 1172: 1, 1175: 1, 1181: 1, 1189: 1, 1190: 1, 1192: 1, 1196: 1, 1200: 1, 1204: 1, 1238: 1, 1243: 1, 1251: 1, 1257: 1, 1263: 1, 1285: 1, 1286: 1, 1287: 1, 1288: 1, 1289: 1, 1295: 1, 1296: 1, 1302: 1, 1305: 1, 1308: 1, 1310: 1, 1317: 1, 1318: 1, 1321: 1, 1339: 1, 1354: 1, 1364: 1, 1367: 1, 1394: 1, 1397: 1, 1401: 1, 1402: 1, 1405: 1, 1410: 1, 1411: 1, 1422: 1, 1426: 1, 1428: 1, 1439: 1, 1442: 1, 1462: 1, 1465: 1, 1473: 1, 1481: 1, 1489: 1, 1507: 1, 1510: 1, 1512: 1, 1520: 1, 1521: 1, 1523: 1, 1527: 1, 1528: 1, 1545: 1, 1554: 1, 1558: 1, 1583: 1, 1604: 1, 1608: 1, 1627: 1, 1646: 1, 1652: 1, 1667: 1, 1672: 1, 1675: 1, 1689: 1, 1693: 1, 1702: 1, 1708: 1, 1712: 1, 1726: 1, 1730: 1, 1743: 1, 1744: 1, 1746: 1, 1760: 1, 1763: 1, 1765: 1, 1768: 1, 1776: 1, 1786: 1, 1787: 1, 1789: 1, 1797: 1, 1800: 1, 1806: 1, 1807: 1, 1809: 1, 1812: 1, 1829: 1, 1830: 1, 1835: 1, 1843: 1, 1848: 1, 1855: 1, 1858: 1, 1864: 1, 1866: 1, 1871: 1, 1889: 1, 1891: 1, 1905: 1, 1912: 1, 1930: 1, 1933: 1, 1939: 1, 1941: 1, 1952: 1, 1955: 1, 1960: 1, 1970: 1, 1975: 1, 1976: 1, 1979: 1, 1982: 1, 1986: 1, 2011: 1, 2013: 1, 2018: 1, 2042: 1, 2043: 1, 2049: 1, 2062: 1, 2068: 1, 2071: 1, 2074: 1, 2078: 1, 2082: 1, 2089: 1, 2099: 1, 2103: 1, 2108: 1, 2115: 1, 2116: 1, 2119: 1, 2134: 1, 2137: 1, 2144: 1, 2153: 1, 2156: 1, 2161: 1, 2162: 1, 2165: 1, 2169: 1, 2172: 1, 2201: 1, 2203: 1, 2206: 1, 2209: 1, 2225: 1, 2227: 1, 2238: 1, 2247: 1, 2248: 1, 2255: 1, 2257: 1, 2267: 1, 2269: 1, 2282: 1, 2290: 1, 2296: 1, 2297: 1, 2303: 1, 2308: 1, 2311: 1, 2320: 1, 2326: 1, 2327: 1, 2332: 1, 2340: 1, 2341: 1, 2350: 1, 2351: 1, 2361: 1, 2369: 1, 2372: 1, 2373: 1, 2381: 1, 2382: 1, 2383: 1, 2385: 1, 2388: 1, 2390: 1, 2392: 1, 2393: 1, 2399: 1, 2407: 1, 2408: 1, 2409: 1, 2411: 1, 2414: 1, 2416: 1, 2418: 1, 2423: 1, 2424: 1, 2430: 1, 2433: 1, 2446: 1, 2447: 1, 2456: 1, 2457: 1, 2463: 1, 2487: 1, 2491: 1, 2499: 1, 2500: 1, 2502: 1, 2503: 1, 2504: 1, 2505: 1, 2515: 1, 2516: 1, 2517: 1, 2519: 1, 2520: 1, 2530: 1, 2535: 1, 2548: 1, 2554: 1, 2555: 1, 2561: 1, 2565: 1, 2566: 1, 2567: 1, 2570: 1, 2571: 1, 2581: 1, 2583: 1, 2587: 1, 2592: 1, 2599: 1, 2602: 1, 2603: 1, 2607: 1, 2609: 1, 2614: 1, 2616: 1, 2624: 1, 2635: 1, 2640: 1, 2642: 1, 2644: 1, 2650: 1, 2651: 1, 2655: 1, 2659: 1, 2664: 1, 2678: 1, 2680: 1, 2694: 1, 2705: 1, 2706: 1, 2709: 1, 2714: 1, 2718: 1, 2725: 1, 2726: 1, 2732: 1, 2735: 1, 2740: 1, 2752: 1, 2764: 1, 2765: 1, 2768: 1, 2773: 1, 2781: 1, 2785: 1, 2787: 1, 2793: 1, 2796: 1, 2798: 1, 2811: 1, 2813: 1, 2815: 1, 2822: 1, 2825: 1, 2827: 1, 2830: 1, 2833: 1, 2836: 1, 2837: 1, 2855: 1, 2862: 1, 2863: 1, 2864: 1, 2865: 1, 2870: 1, 2878: 1, 2881: 1, 2882: 1, 2883: 1, 2887: 1, 2889: 1, 2897: 1, 2899: 1, 2903: 1, 2905: 1, 2910: 1, 2912: 1, 2913: 1, 2914: 1, 2915: 1, 2917: 1, 2919: 1, 2924: 1, 2925: 1, 2928: 1, 2930: 1, 2935: 1, 2936: 1, 2938: 1, 2945: 1, 2949: 1, 2960: 1, 2964: 1, 2966: 1, 2972: 1, 2974: 1, 2978: 1, 2984: 1, 2987: 1, 2992: 1, 3000: 1, 3001: 1, 3002: 1, 3005: 1, 3009: 1, 3012: 1, 3019: 1, 3025: 1, 3032: 1, 3033: 1, 3037: 1, 3038: 1, 3040: 1, 3041: 1, 3046: 1, 3048: 1, 3050: 1, 3052: 1, 3063: 1, 3074: 1, 3076: 1, 3077: 1, 3079: 1, 3099: 1, 3107: 1, 3112: 1, 3113: 1, 3119: 1, 3120: 1, 3121: 1, 3122: 1, 3123: 1, 3124: 1, 3128: 1, 3129: 1, 3130: 1, 3133: 1, 3139: 1, 3149: 1, 3150: 1, 3159: 1, 3165: 1, 3167: 1, 3171: 1, 3174: 1, 3175: 1, 3177: 1, 3182: 1, 3184: 1, 3185: 1, 3186: 1, 3187: 1, 3188: 1, 3189: 1, 3190: 1, 3199: 1, 3201: 1, 3205: 1, 3211: 1, 3215: 1, 3217: 1, 3221: 1, 3222: 1, 3225: 1, 3226: 1, 3236: 1, 3239: 1, 3245: 1, 3248: 1, 3249: 1, 3251: 1, 3259: 1, 3267: 1, 3271: 1, 3272: 1, 3276: 1, 3278: 1, 3279: 1, 3280: 1, 3283: 1, 3284: 1, 3288: 1, 3290: 1, 3293: 1, 3296: 1, 3300: 1, 3301: 1, 3302: 1, 3303: 1, 3310: 1, 3312: 1, 3314: 1, 3315: 1, 3319: 1, 3320: 1, 3325: 1, 3326: 1, 3333: 1, 3336: 1, 3339: 1, 3340: 1, 3341: 1, 3347: 1, 3349: 1, 3350: 1, 3352: 1, 3353: 1, 3354: 1, 3356: 1, 3357: 1, 3362: 1, 3366: 1, 3367: 1, 3369: 1, 3374: 1, 3380: 1, 3388: 1, 3389: 1, 3391: 1, 3392: 1, 3395: 1, 3398: 1, 3403: 1, 3404: 1, 3408: 1, 3410: 1, 3411: 1, 3412: 1, 3413: 1, 3416: 1, 3417: 1, 3424: 1, 3427: 1, 3429: 1, 3431: 1, 3433: 1, 3434: 1, 3438: 1, 3442: 1, 3443: 1, 3444: 1, 3445: 1, 3448: 1, 3449: 1, 3458: 1, 3463: 1, 3467: 1, 3470: 1, 3471: 1, 3477: 1, 3481: 1, 3482: 1, 3483: 1, 3484: 1, 3492: 1, 3494: 1, 3496: 1, 3497: 1, 3508: 1, 3510: 1, 3518: 1, 3520: 1, 3523: 1, 3528: 1, 3529: 1, 3538: 1, 3539: 1, 3540: 1, 3542: 1, 3544: 1, 3550: 1, 3552: 1, 3555: 1, 3556: 1, 3560: 1, 3565: 1, 3570: 1, 3576: 1, 3577: 1, 3582: 1, 3583: 1, 3584: 1, 3591: 1, 3595: 1, 3596: 1, 3597: 1, 3607: 1, 3608: 1, 3609: 1, 3610: 1, 3611: 1, 3615: 1, 3620: 1, 3631: 1, 3632: 1, 3633: 1, 3634: 1, 3640: 1, 3641: 1, 3642: 1, 3648: 1, 3651: 1, 3654: 1, 3656: 1, 3657: 1, 3660: 1, 3662: 1, 3668: 1, 3670: 1, 3672: 1, 3673: 1, 3675: 1, 3679: 1, 3683: 1, 3689: 1, 3690: 1, 3691: 1, 3692: 1, 3693: 1, 3694: 1, 3696: 1, 3710: 1, 3716: 1, 3717: 1, 3720: 1, 3722: 1, 3723: 1, 3726: 1, 3727: 1, 3728: 1, 3730: 1, 3734: 1, 3736: 1, 3748: 1, 3751: 1, 3753: 1, 3756: 1, 3758: 1, 3762: 1, 3763: 1, 3765: 1, 3767: 1, 3768: 1, 3769: 1, 3777: 1, 3778: 1, 3781: 1, 3785: 1, 3787: 1, 3790: 1, 3791: 1, 3794: 1, 3809: 1, 3810: 1, 3811: 1, 3814: 1, 3815: 1, 3816: 1, 3818: 1, 3820: 1, 3821: 1, 3826: 1, 3827: 1, 3828: 1, 3830: 1, 3835: 1, 3840: 1, 3842: 1, 3844: 1, 3846: 1, 3848: 1, 3852: 1, 3853: 1, 3855: 1, 3863: 1, 3867: 1, 3869: 1, 3876: 1, 3877: 1, 3881: 1, 3887: 1, 3888: 1, 3893: 1, 3894: 1, 3897: 1, 3901: 1, 3902: 1, 3913: 1, 3915: 1, 3927: 1, 3930: 1, 3933: 1, 3934: 1, 3935: 1, 3937: 1, 3938: 1, 3940: 1, 3941: 1, 3942: 1, 3944: 1, 3945: 1, 3950: 1, 3954: 1, 3955: 1, 3956: 1, 3957: 1, 3960: 1, 3962: 1, 3964: 1, 3966: 1, 3969: 1, 3971: 1, 3981: 1, 3982: 1, 3984: 1, 3985: 1, 3986: 1, 3988: 1, 3989: 1, 3990: 1, 3995: 1, 3996: 1, 3997: 1, 3999: 1, 4003: 1, 4005: 1, 4006: 1, 4007: 1, 4010: 1, 4013: 1, 4014: 1, 4015: 1, 4020: 1, 4022: 1, 4024: 1, 4027: 1, 4032: 1, 4040: 1, 4041: 1, 4042: 1, 4046: 1, 4051: 1, 4054: 1, 4055: 1, 4056: 1, 4066: 1, 4068: 1, 4080: 1, 4088: 1, 4095: 1, 4103: 1, 4105: 1, 4108: 1, 4113: 1, 4117: 1, 4122: 1, 4123: 1, 4127: 1, 4128: 1, 4129: 1, 4130: 1, 4132: 1, 4135: 1, 4139: 1, 4144: 1, 4145: 1, 4155: 1, 4157: 1, 4158: 1, 4161: 1, 4165: 1, 4166: 1, 4168: 1, 4170: 1, 4174: 1, 4175: 1, 4177: 1, 4178: 1, 4180: 1, 4181: 1, 4183: 1, 4184: 1, 4188: 1, 4189: 1, 4191: 1, 4192: 1, 4195: 1, 4200: 1, 4202: 1, 4204: 1, 4207: 1, 4208: 1, 4211: 1, 4215: 1, 4218: 1, 4220: 1, 4222: 1, 4233: 1, 4234: 1, 4241: 1, 4243: 1, 4244: 1, 4247: 1, 4248: 1, 4254: 1, 4255: 1, 4256: 1, 4257: 1, 4260: 1, 4265: 1, 4271: 1, 4272: 1, 4274: 1, 4278: 1, 4281: 1, 4282: 1, 4284: 1, 4285: 1, 4286: 1, 4289: 1, 4293: 1, 4295: 1, 4308: 1, 4310: 1, 4311: 1, 4313: 1, 4314: 1, 4320: 1, 4321: 1, 4327: 1, 4328: 1, 4329: 1, 4330: 1, 4332: 1, 4336: 1, 4337: 1, 4338: 1, 4340: 1, 4344: 1, 4348: 1, 4351: 1, 4354: 1, 4363: 1, 4365: 1, 4366: 1, 4368: 1, 4369: 1, 4371: 1, 4372: 1, 4374: 1, 4376: 1, 4382: 1, 4386: 1, 4392: 1, 4397: 1, 4399: 1, 4402: 1, 4410: 1, 4411: 1, 4415: 1, 4416: 1, 4418: 1, 4419: 1, 4423: 1, 4425: 1, 4426: 1, 4427: 1, 4430: 1, 4435: 1, 4437: 1, 4438: 1, 4441: 1, 4443: 1, 4446: 1, 4448: 1, 4453: 1, 4455: 1, 4460: 1, 4461: 1, 4465: 1, 4467: 1, 4468: 1, 4473: 1, 4476: 1, 4477: 1, 4478: 1, 4479: 1, 4480: 1, 4481: 1, 4482: 1, 4484: 1, 4487: 1, 4491: 1, 4494: 1, 4506: 1, 4507: 1, 4510: 1, 4513: 1, 4514: 1, 4517: 1, 4521: 1, 4523: 1, 4526: 1, 4527: 1, 4530: 1, 4532: 1, 4534: 1, 4535: 1, 4537: 1, 4539: 1, 4540: 1, 4544: 1, 4545: 1, 4552: 1, 4554: 1, 4556: 1, 4559: 1, 4566: 1, 4567: 1, 4568: 1, 4570: 1, 4571: 1, 4572: 1, 4577: 1, 4580: 1, 4583: 1, 4585: 1, 4588: 1, 4594: 1, 4595: 1, 4601: 1, 4602: 1, 4603: 1, 4604: 1, 4605: 1, 4606: 1, 4610: 1, 4613: 1, 4614: 1, 4621: 1, 4622: 1, 4625: 1, 4638: 1, 4639: 1, 4640: 1, 4642: 1, 4643: 1, 4644: 1, 4647: 1, 4648: 1, 4651: 1, 4656: 1, 4657: 1, 4658: 1, 4663: 1, 4665: 1, 4666: 1, 4669: 1, 4672: 1, 4673: 1, 4676: 1, 4677: 1, 4678: 1, 4681: 1, 4693: 1, 4696: 1, 4699: 1, 4700: 1, 4707: 1, 4708: 1, 4710: 1, 4712: 1, 4714: 1, 4716: 1, 4717: 1, 4720: 1, 4722: 1, 4727: 1, 4728: 1, 4729: 1, 4731: 1, 4732: 1, 4733: 1, 4734: 1, 4735: 1, 4737: 1, 4738: 1, 4739: 1, 4741: 1, 4747: 1, 4748: 1, 4754: 1, 4755: 1, 4756: 1, 4758: 1, 4767: 1, 4769: 1, 4770: 1, 4772: 1, 4774: 1, 4779: 1, 4781: 1, 4789: 1, 4791: 1, 4793: 1, 4794: 1, 4795: 1, 4797: 1, 4799: 1, 4804: 1, 4807: 1, 4809: 1, 4810: 1, 4813: 1, 4814: 1, 4815: 1, 4817: 1, 4819: 1, 4822: 1, 4825: 1, 4826: 1, 4830: 1, 4836: 1, 4840: 1, 4841: 1, 4845: 1, 4852: 1, 4853: 1, 4856: 1, 4859: 1, 4860: 1, 4861: 1, 4867: 1, 4869: 1, 4873: 1, 4881: 1, 4883: 1, 4885: 1, 4891: 1, 4892: 1, 4893: 1, 4896: 1, 4897: 1, 4898: 1, 4899: 1, 4905: 1, 4907: 1, 4909: 1, 4910: 1, 4912: 1, 4914: 1, 4919: 1, 4922: 1, 4925: 1, 4927: 1, 4930: 1, 4933: 1, 4934: 1, 4937: 1, 4939: 1, 4940: 1, 4941: 1, 4942: 1, 4943: 1, 4944: 1, 4947: 1, 4948: 1, 4952: 1, 4953: 1, 4956: 1, 4959: 1, 4960: 1, 4961: 1, 4963: 1, 4965: 1, 4966: 1, 4967: 1, 4968: 1, 4970: 1, 4971: 1, 4972: 1, 4980: 1, 4984: 1, 4987: 1, 4988: 1, 4989: 1, 4990: 1, 4991: 1, 4992: 1, 4993: 1, 4997: 1, 5002: 1, 5004: 1, 5007: 1, 5010: 1, 5011: 1, 5013: 1, 5014: 1, 5016: 1, 5017: 1, 5018: 1, 5022: 1, 5027: 1, 5028: 1, 5029: 1, 5032: 1, 5035: 1, 5036: 1, 5038: 1, 5039: 1, 5040: 1, 5041: 1, 5043: 1, 5044: 1, 5045: 1, 5046: 1, 5047: 1, 5048: 1, 5049: 1, 5051: 1, 5054: 1, 5056: 1, 5059: 1, 5060: 1, 5061: 1, 5062: 1, 5063: 1, 5070: 1, 5075: 1, 5078: 1, 5079: 1, 5081: 1, 5082: 1, 5083: 1, 5087: 1, 5091: 1, 5093: 1, 5097: 1, 5102: 1, 5105: 1, 5106: 1, 5108: 1, 5110: 1, 5112: 1, 5113: 1, 5114: 1, 5117: 1, 5119: 1, 5120: 1, 5121: 1, 5122: 1, 5123: 1, 5125: 1, 5126: 1, 5128: 1, 5129: 1, 5130: 1, 5131: 1, 5132: 1, 5135: 1, 5136: 1, 5137: 1, 5138: 1, 5141: 1, 5142: 1, 5146: 1, 5147: 1, 5148: 1, 5149: 1, 5150: 1, 5151: 1, 5153: 1, 5161: 1, 5162: 1, 5167: 1, 5168: 1, 5169: 1, 5175: 1, 5178: 1, 5180: 1, 5181: 1, 5184: 1, 5185: 1, 5187: 1, 5188: 1, 5202: 1, 5203: 1, 5215: 1, 5219: 1, 5220: 1, 5221: 1, 5224: 1, 5225: 1, 5228: 1, 5230: 1, 5232: 1, 5233: 1, 5236: 1, 5237: 1, 5238: 1, 5241: 1, 5242: 1, 5244: 1, 5247: 1, 5248: 1, 5249: 1, 5257: 1, 5259: 1, 5264: 1, 5266: 1, 5267: 1, 5270: 1, 5271: 1, 5278: 1, 5280: 1, 5284: 1, 5285: 1, 5286: 1, 5287: 1, 5288: 1, 5289: 1, 5290: 1, 5291: 1, 5293: 1, 5294: 1, 5299: 1, 5301: 1, 5302: 1, 5305: 1, 5306: 1, 5309: 1, 5310: 1, 5312: 1, 5314: 1, 5319: 1, 5320: 1, 5321: 1, 5323: 1, 5325: 1, 5328: 1, 5336: 1, 5341: 1, 5342: 1, 5344: 1, 5348: 1, 5352: 1, 5354: 1, 5355: 1, 5357: 1, 5358: 1, 5359: 1, 5360: 1, 5363: 1, 5364: 1, 5366: 1, 5368: 1, 5372: 1, 5374: 1, 5375: 1, 5378: 1, 5379: 1, 5381: 1, 5383: 1, 5385: 1, 5392: 1, 5394: 1, 5395: 1, 5396: 1, 5398: 1, 5401: 1, 5402: 1, 5403: 1, 5405: 1, 5407: 1, 5408: 1, 5410: 1, 5424: 1, 5425: 1, 5430: 1, 5431: 1, 5432: 1, 5437: 1, 5443: 1, 5445: 1, 5446: 1, 5447: 1, 5451: 1, 5455: 1, 5456: 1, 5457: 1, 5458: 1, 5460: 1, 5461: 1, 5464: 1, 5466: 1, 5469: 1, 5470: 1, 5471: 1, 5472: 1, 5473: 1, 5475: 1, 5476: 1, 5480: 1, 5481: 1, 5485: 1, 5487: 1, 5488: 1, 5490: 1, 5492: 1, 5493: 1, 5496: 1, 5497: 1, 5499: 1, 5500: 1, 5501: 1, 5502: 1, 5506: 1, 5510: 1, 5511: 1, 5515: 1, 5522: 1, 5523: 1, 5527: 1, 5530: 1, 5531: 1, 5532: 1, 5536: 1, 5537: 1, 5539: 1, 5541: 1, 5544: 1, 5545: 1, 5550: 1, 5551: 1, 5553: 1, 5554: 1, 5556: 1, 5557: 1, 5562: 1, 5564: 1, 5566: 1, 5568: 1, 5574: 1, 5577: 1, 5578: 1, 5579: 1, 5580: 1, 5581: 1, 5582: 1, 5584: 1, 5588: 1, 5589: 1, 5590: 1, 5591: 1, 5594: 1, 5595: 1, 5596: 1, 5597: 1, 5598: 1, 5599: 1, 5600: 1, 5601: 1, 5604: 1, 5605: 1, 5607: 1, 5608: 1, 5610: 1, 5612: 1, 5613: 1, 5614: 1, 5615: 1, 5618: 1, 5619: 1, 5623: 1, 5624: 1, 5629: 1, 5630: 1, 5631: 1, 5632: 1, 5633: 1, 5635: 1, 5640: 1, 5641: 1, 5647: 1, 5648: 1, 5649: 1, 5650: 1, 5652: 1, 5653: 1, 5655: 1, 5663: 1, 5664: 1, 5666: 1, 5669: 1, 5673: 1, 5674: 1, 5677: 1, 5678: 1, 5683: 1, 5687: 1, 5688: 1, 5689: 1, 5690: 1, 5692: 1, 5693: 1, 5694: 1, 5696: 1, 5697: 1, 5698: 1, 5702: 1, 5705: 1, 5706: 1, 5708: 1, 5709: 1, 5711: 1, 5712: 1, 5715: 1, 5716: 1, 5718: 1, 5720: 1, 5721: 1, 5722: 1, 5724: 1, 5727: 1, 5729: 1, 5730: 1, 5731: 1, 5732: 1, 5734: 1, 5738: 1, 5740: 1, 5741: 1, 5742: 1, 5744: 1, 5745: 1, 5746: 1, 5747: 1, 5748: 1, 5750: 1, 5752: 1, 5754: 1, 5755: 1, 5756: 1, 5758: 1, 5763: 1, 5766: 1, 5767: 1, 5772: 1, 5773: 1, 5774: 1, 5775: 1, 5776: 1, 5777: 1, 5778: 1, 5781: 1, 5784: 1, 5785: 1, 5787: 1, 5788: 1, 5790: 1, 5792: 1, 5793: 1, 5794: 1, 5795: 1, 5796: 1, 5797: 1, 5799: 1, 5800: 1, 5801: 1, 5804: 1, 5805: 1, 5806: 1, 5809: 1, 5811: 1, 5813: 1, 5814: 1, 5815: 1, 5818: 1, 5819: 1, 5820: 1, 5824: 1, 5825: 1, 5827: 1, 5828: 1, 5830: 1, 5832: 1, 5835: 1, 5836: 1, 5837: 1, 5838: 1, 5843: 1, 5844: 1, 5845: 1, 5846: 1, 5849: 1, 5851: 1, 5852: 1, 5853: 1, 5858: 1, 5859: 1, 5860: 1, 5861: 1, 5862: 1, 5863: 1, 5864: 1, 5865: 1, 5866: 1, 5868: 1, 5872: 1, 5876: 1, 5877: 1, 5879: 1, 5880: 1, 5886: 1, 5889: 1, 5891: 1, 5895: 1, 5896: 1, 5898: 1, 5899: 1, 5900: 1, 5902: 1, 5903: 1, 5905: 1, 5908: 1, 5910: 1, 5912: 1, 5913: 1, 5914: 1, 5919: 1, 5920: 1, 5922: 1, 5924: 1, 5925: 1, 5927: 1, 5929: 1, 5930: 1, 5931: 1, 5934: 1, 5935: 1, 5937: 1, 5938: 1, 5939: 1, 5940: 1, 5941: 1, 5943: 1, 5944: 1, 5945: 1, 5951: 1, 5952: 1, 5954: 1, 5955: 1, 5956: 1, 5957: 1, 5959: 1, 5962: 1, 5963: 1, 5964: 1, 5967: 1, 5968: 1, 5970: 1, 5971: 1, 5972: 1, 5975: 1, 5976: 1, 5979: 1, 5982: 1, 5983: 1, 5985: 1, 5988: 1, 5991: 1, 5994: 1, 5995: 1, 5997: 1, 5998: 1, 6000: 1, 6001: 1, 6002: 1, 6004: 1, 6005: 1, 6009: 1, 6010: 1, 6017: 1, 6018: 1, 6020: 1, 6022: 1, 6026: 1, 6028: 1, 6029: 1, 6030: 1, 6031: 1, 6032: 1, 6034: 1, 6036: 1, 6039: 1, 6043: 1, 6046: 1, 6047: 1, 6049: 1, 6051: 1, 6053: 1, 6054: 1, 6055: 1, 6063: 1, 6064: 1, 6066: 1, 6067: 1, 6070: 1, 6071: 1, 6073: 1, 6074: 1, 6075: 1, 6077: 1, 6078: 1, 6079: 1, 6080: 1, 6081: 1, 6083: 1, 6084: 1, 6085: 1, 6087: 1, 6089: 1, 6090: 1, 6091: 1, 6092: 1, 6093: 1, 6094: 1, 6096: 1, 6098: 1, 6099: 1, 6100: 1, 6102: 1, 6103: 1, 6104: 1, 6106: 1, 6107: 1, 6108: 1, 6109: 1, 6110: 1, 6111: 1, 6116: 1, 6117: 1, 6119: 1, 6120: 1, 6121: 1, 6124: 1, 6127: 1, 6132: 1, 6137: 1, 6141: 1, 6142: 1, 6143: 1, 6146: 1, 6148: 1, 6149: 1, 6151: 1, 6152: 1, 6153: 1, 6156: 1, 6157: 1, 6158: 1, 6161: 1, 6162: 1, 6164: 1, 6165: 1, 6168: 1, 6171: 1, 6173: 1, 6174: 1, 6175: 1, 6177: 1, 6178: 1, 6180: 1, 6181: 1, 6182: 1, 6183: 1, 6184: 1, 6186: 1, 6188: 1, 6189: 1, 6190: 1, 6194: 1, 6196: 1, 6198: 1, 6200: 1, 6201: 1, 6203: 1, 6205: 1, 6209: 1, 6211: 1, 6213: 1, 6214: 1, 6217: 1, 6219: 1, 6220: 1, 6221: 1, 6223: 1, 6224: 1, 6225: 1, 6226: 1, 6231: 1, 6232: 1, 6233: 1, 6235: 1, 6236: 1, 6237: 1, 6239: 1, 6241: 1, 6242: 1, 6243: 1, 6244: 1, 6245: 1, 6247: 1, 6248: 1, 6249: 1, 6250: 1, 6251: 1, 6252: 1, 6253: 1, 6255: 1, 6256: 1, 6258: 1, 6259: 1, 6263: 1, 6264: 1, 6266: 1, 6267: 1, 6268: 1, 6269: 1, 6273: 1, 6277: 1, 6279: 1, 6280: 1, 6281: 1, 6282: 1, 6284: 1, 6285: 1, 6287: 1, 6288: 1, 6292: 1, 6294: 1, 6295: 1, 6296: 1, 6297: 1, 6298: 1, 6300: 1, 6301: 1, 6304: 1, 6310: 1, 6313: 1, 6314: 1, 6318: 1, 6319: 1, 6320: 1, 6322: 1, 6326: 1, 6327: 1, 6330: 1, 6332: 1, 6333: 1, 6334: 1, 6336: 1, 6337: 1, 6338: 1, 6339: 1, 6341: 1, 6342: 1, 6343: 1, 6344: 1, 6349: 1, 6351: 1, 6354: 1, 6355: 1, 6356: 1, 6357: 1, 6366: 1, 6367: 1, 6369: 1, 6372: 1, 6373: 1, 6377: 1, 6378: 1, 6379: 1, 6383: 1, 6385: 1, 6388: 1, 6389: 1, 6391: 1, 6393: 1, 6394: 1, 6395: 1, 6396: 1, 6401: 1, 6402: 1, 6404: 1, 6406: 1, 6409: 1, 6410: 1, 6411: 1, 6412: 1, 6413: 1, 6414: 1, 6415: 1, 6417: 1, 6418: 1, 6419: 1, 6420: 1, 6421: 1, 6424: 1, 6426: 1, 6427: 1, 6428: 1, 6429: 1, 6431: 1, 6432: 1, 6433: 1, 6434: 1, 6436: 1, 6439: 1, 6440: 1, 6446: 1, 6448: 1, 6449: 1, 6450: 1, 6451: 1, 6453: 1, 6454: 1, 6455: 1, 6457: 1, 6458: 1, 6459: 1, 6461: 1, 6462: 1, 6463: 1, 6464: 1, 6466: 1, 6470: 1, 6471: 1, 6474: 1, 6475: 1, 6476: 1, 6477: 1, 6478: 1, 6480: 1, 6481: 1, 6483: 1, 6486: 1, 6487: 1, 6488: 1, 6489: 1, 6491: 1, 6500: 1, 6501: 1, 6503: 1, 6505: 1, 6506: 1, 6508: 1, 6509: 1, 6510: 1, 6514: 1, 6515: 1, 6516: 1, 6517: 1, 6518: 1, 6519: 1, 6520: 1, 6521: 1, 6522: 1, 6523: 1, 6525: 1, 6526: 1, 6527: 1, 6528: 1, 6529: 1, 6530: 1, 6531: 1, 6532: 1, 6534: 1, 6536: 1, 6539: 1, 6540: 1, 6541: 1, 6545: 1, 6547: 1, 6548: 1, 6549: 1, 6553: 1, 6554: 1, 6555: 1, 6557: 1, 6558: 1, 6560: 1, 6561: 1, 6562: 1, 6564: 1, 6565: 1, 6566: 1, 6568: 1, 6569: 1, 6571: 1, 6572: 1, 6575: 1, 6576: 1, 6579: 1, 6580: 1, 6582: 1, 6584: 1, 6585: 1, 6586: 1, 6587: 1, 6590: 1, 6591: 1, 6594: 1, 6595: 1, 6598: 1, 6599: 1, 6600: 1, 6601: 1, 6602: 1, 6604: 1, 6606: 1, 6607: 1, 6613: 1, 6614: 1, 6615: 1, 6616: 1, 6617: 1, 6618: 1, 6619: 1, 6625: 1, 6626: 1, 6628: 1, 6629: 1, 6634: 1, 6636: 1, 6637: 1, 6638: 1, 6640: 1, 6641: 1, 6642: 1, 6643: 1, 6645: 1, 6647: 1, 6648: 1, 6649: 1, 6650: 1, 6653: 1, 6654: 1, 6656: 1, 6657: 1, 6658: 1, 6659: 1, 6663: 1, 6664: 1, 6665: 1, 6666: 1, 6669: 1, 6670: 1, 6672: 1, 6674: 1, 6675: 1, 6677: 1, 6679: 1, 6681: 1, 6682: 1, 6685: 1, 6686: 1, 6689: 1, 6693: 1, 6697: 1, 6699: 1, 6701: 1, 6702: 1, 6706: 1, 6709: 1, 6710: 1, 6711: 1, 6714: 1, 6716: 1, 6717: 1, 6718: 1, 6719: 1, 6720: 1, 6722: 1, 6723: 1, 6724: 1, 6726: 1, 6729: 1, 6730: 1, 6732: 1, 6734: 1, 6735: 1, 6736: 1, 6737: 1, 6738: 1, 6739: 1, 6740: 1, 6742: 1, 6743: 1, 6745: 1, 6746: 1, 6747: 1, 6748: 1, 6749: 1, 6752: 1, 6753: 1, 6754: 1, 6759: 1, 6762: 1, 6763: 1, 6764: 1, 6765: 1, 6766: 1, 6768: 1, 6769: 1, 6770: 1, 6771: 1, 6772: 1, 6773: 1, 6777: 1, 6778: 1, 6779: 1, 6782: 1, 6785: 1, 6786: 1, 6788: 1, 6790: 1, 6791: 1, 6796: 1, 6797: 1, 6799: 1, 6800: 1, 6801: 1, 6802: 1, 6806: 1, 6807: 1, 6810: 1, 6813: 1, 6816: 1, 6817: 1, 6818: 1, 6819: 1, 6820: 1, 6822: 1, 6823: 1, 6824: 1, 6825: 1, 6826: 1, 6827: 1, 6829: 1, 6830: 1, 6831: 1, 6832: 1, 6833: 1, 6834: 1, 6835: 1, 6836: 1, 6837: 1, 6838: 1, 6839: 1, 6841: 1, 6842: 1, 6843: 1, 6845: 1, 6846: 1, 6847: 1, 6849: 1, 6850: 1, 6851: 1, 6852: 1, 6855: 1, 6856: 1, 6857: 1, 6859: 1, 6861: 1, 6865: 1, 6866: 1, 6867: 1, 6868: 1, 6870: 1, 6873: 1, 6875: 1, 6876: 1, 6877: 1, 6878: 1, 6880: 1, 6881: 1, 6882: 1, 6883: 1, 6886: 1, 6887: 1, 6888: 1, 6889: 1, 6890: 1, 6891: 1, 6892: 1, 6893: 1, 6894: 1, 6895: 1, 6896: 1, 6897: 1, 6898: 1, 6899: 1, 6900: 1, 6903: 1, 6904: 1, 6905: 1, 6906: 1, 6908: 1, 6909: 1, 6910: 1, 6911: 1, 6913: 1, 6914: 1, 6915: 1, 6916: 1, 6917: 1, 6918: 1, 6919: 1, 6921: 1, 6923: 1, 6924: 1, 6925: 1, 6926: 1, 6927: 1, 6928: 1, 6931: 1, 6932: 1, 6934: 1, 6935: 1, 6936: 1, 6938: 1, 6940: 1, 6941: 1, 6943: 1, 6944: 1, 6945: 1, 6946: 1, 6948: 1, 6949: 1, 6950: 1, 6951: 1, 6952: 1, 6953: 1, 6954: 1, 6955: 1, 6956: 1, 6957: 1, 6961: 1, 6962: 1, 6964: 1, 6965: 1, 6966: 1, 6967: 1, 6968: 1, 6969: 1, 6970: 1, 6971: 1, 6972: 1, 6973: 1, 6976: 1, 6978: 1, 6979: 1, 6980: 1, 6981: 1, 6982: 1, 6984: 1, 6985: 1, 6986: 1, 6987: 1, 6989: 1, 6990: 1, 6994: 1, 6996: 1, 6997: 1, 6998: 1, 6999: 1, 7000: 1, 7001: 1, 7002: 1, 7003: 1, 7005: 1, 7007: 1, 7009: 1, 7010: 1, 7012: 1, 7013: 1, 7015: 1, 7016: 1, 7017: 1, 7020: 1, 7022: 1, 7025: 1, 7026: 1, 7027: 1, 7028: 1, 7033: 1, 7034: 1, 7035: 1, 7036: 1, 7038: 1, 7040: 1, 7041: 1, 7042: 1, 7043: 1, 7044: 1, 7046: 1, 7047: 1, 7048: 1, 7050: 1, 7053: 1, 7056: 1, 7057: 1, 7058: 1, 7059: 1, 7061: 1, 7062: 1, 7063: 1, 7065: 1, 7067: 1, 7068: 1, 7070: 1, 7071: 1, 7072: 1, 7073: 1, 7076: 1, 7078: 1, 7081: 1, 7082: 1, 7084: 1, 7085: 1, 7087: 1, 7088: 1, 7089: 1, 7091: 1, 7093: 1, 7094: 1, 7095: 1, 7096: 1, 7098: 1, 7100: 1, 7101: 1, 7102: 1, 7104: 1, 7106: 1, 7107: 1, 7108: 1, 7109: 1, 7110: 1, 7111: 1, 7112: 1, 7115: 1, 7117: 1, 7119: 1, 7120: 1, 7121: 1, 7122: 1, 7125: 1, 7127: 1, 7129: 1, 7130: 1, 7131: 1, 7133: 1, 7134: 1, 7135: 1, 7136: 1, 7137: 1, 7140: 1, 7141: 1, 7142: 1, 7145: 1, 7146: 1, 7148: 1, 7149: 1, 7152: 1, 7153: 1, 7154: 1, 7155: 1, 7157: 1, 7158: 1, 7159: 1, 7160: 1, 7161: 1, 7163: 1, 7164: 1, 7165: 1, 7166: 1, 7167: 1, 7169: 1, 7171: 1, 7172: 1, 7173: 1, 7174: 1, 7175: 1, 7177: 1, 7181: 1, 7182: 1, 7183: 1, 7184: 1, 7185: 1, 7186: 1, 7187: 1, 7188: 1, 7189: 1, 7190: 1, 7191: 1, 7192: 1, 7194: 1, 7195: 1, 7196: 1, 7197: 1, 7199: 1, 7201: 1, 7203: 1, 7204: 1, 7206: 1, 7208: 1, 7210: 1, 7211: 1, 7213: 1, 7214: 1, 7215: 1, 7216: 1, 7219: 1, 7220: 1, 7221: 1, 7222: 1, 7223: 1, 7226: 1, 7227: 1, 7229: 1, 7231: 1, 7233: 1, 7234: 1, 7235: 1, 7236: 1, 7237: 1, 7238: 1, 7239: 1, 7242: 1, 7245: 1, 7246: 1, 7250: 1, 7252: 1, 7253: 1, 7254: 1, 7255: 1, 7256: 1, 7258: 1, 7261: 1, 7263: 1, 7264: 1, 7265: 1, 7268: 1, 7269: 1, 7271: 1, 7272: 1, 7273: 1, 7275: 1, 7276: 1, 7277: 1, 7278: 1, 7279: 1, 7280: 1, 7282: 1, 7283: 1, 7284: 1, 7285: 1, 7287: 1, 7288: 1, 7290: 1, 7297: 1, 7299: 1, 7302: 1, 7303: 1, 7304: 1, 7305: 1, 7308: 1, 7309: 1, 7310: 1, 7312: 1, 7313: 1, 7314: 1, 7315: 1, 7316: 1, 7318: 1, 7319: 1, 7321: 1, 7322: 1, 7323: 1, 7325: 1, 7327: 1, 7328: 1, 7330: 1, 7332: 1, 7333: 1, 7334: 1, 7335: 1, 7337: 1, 7338: 1, 7339: 1, 7340: 1, 7342: 1, 7345: 1, 7346: 1, 7347: 1, 7348: 1, 7349: 1, 7350: 1, 7351: 1, 7352: 1, 7353: 1, 7354: 1, 7355: 1, 7356: 1, 7357: 1, 7359: 1, 7361: 1, 7362: 1, 7363: 1, 7364: 1, 7365: 1, 7366: 1, 7367: 1, 7368: 1, 7369: 1, 7370: 1, 7371: 1, 7375: 1, 7376: 1, 7377: 1, 7382: 1, 7383: 1, 7384: 1, 7385: 1, 7386: 1, 7387: 1, 7390: 1, 7391: 1, 7392: 1, 7393: 1, 7394: 1, 7395: 1, 7397: 1, 7400: 1, 7401: 1, 7402: 1, 7403: 1, 7405: 1, 7407: 1, 7408: 1, 7409: 1, 7410: 1, 7411: 1, 7412: 1, 7413: 1, 7415: 1, 7416: 1, 7418: 1, 7419: 1, 7420: 1, 7421: 1, 7423: 1, 7424: 1, 7426: 1, 7427: 1, 7428: 1, 7429: 1, 7430: 1, 7431: 1, 7432: 1, 7433: 1, 7434: 1, 7436: 1, 7438: 1, 7441: 1, 7442: 1, 7443: 1, 7444: 1, 7445: 1, 7446: 1, 7447: 1, 7448: 1, 7450: 1, 7451: 1, 7454: 1, 7455: 1, 7456: 1, 7461: 1, 7465: 1, 7466: 1, 7467: 1, 7468: 1, 7469: 1, 7470: 1, 7471: 1, 7472: 1, 7475: 1, 7476: 1, 7477: 1, 7478: 1, 7482: 1, 7483: 1, 7484: 1, 7485: 1, 7486: 1, 7487: 1, 7488: 1, 7489: 1, 7490: 1, 7492: 1, 7493: 1, 7495: 1, 7496: 1, 7497: 1, 7498: 1, 7499: 1, 7500: 1, 7501: 1, 7503: 1, 7505: 1, 7507: 1, 7508: 1, 7509: 1, 7513: 1, 7515: 1, 7517: 1, 7518: 1, 7519: 1, 7520: 1, 7521: 1, 7522: 1, 7524: 1, 7526: 1, 7527: 1, 7528: 1, 7529: 1, 7530: 1, 7533: 1, 7534: 1, 7535: 1, 7536: 1, 7537: 1, 7538: 1, 7539: 1, 7540: 1, 7541: 1, 7542: 1, 7543: 1, 7544: 1, 7545: 1, 7546: 1, 7547: 1, 7548: 1, 7549: 1, 7550: 1, 7552: 1, 7553: 1, 7554: 1, 7555: 1, 7556: 1, 7557: 1, 7558: 1, 7559: 1, 7560: 1, 7561: 1, 7562: 1, 7564: 1, 7566: 1, 7567: 1, 7569: 1, 7570: 1, 7571: 1, 7575: 1, 7576: 1, 7579: 1, 7580: 1, 7581: 1, 7582: 1, 7583: 1, 7584: 1, 7586: 1, 7587: 1, 7588: 1, 7590: 1, 7591: 1, 7592: 1, 7593: 1, 7595: 1, 7596: 1, 7597: 1, 7598: 1, 7599: 1, 7601: 1, 7602: 1, 7603: 1, 7604: 1, 7605: 1, 7606: 1, 7607: 1, 7610: 1, 7612: 1, 7614: 1, 7616: 1, 7617: 1, 7620: 1, 7621: 1, 7622: 1, 7623: 1, 7625: 1, 7626: 1, 7627: 1, 7629: 1, 7630: 1, 7631: 1, 7632: 1, 7633: 1, 7634: 1, 7635: 1, 7638: 1, 7639: 1, 7640: 1, 7641: 1, 7642: 1, 7643: 1, 7644: 1, 7647: 1, 7650: 1, 7651: 1, 7653: 1, 7654: 1, 7655: 1, 7656: 1, 7657: 1, 7659: 1, 7660: 1, 7661: 1, 7663: 1, 7665: 1, 7667: 1, 7668: 1, 7669: 1, 7671: 1, 7672: 1, 7673: 1, 7674: 1, 7675: 1, 7679: 1, 7680: 1, 7681: 1, 7682: 1, 7683: 1, 7684: 1, 7685: 1, 7686: 1, 7687: 1, 7689: 1, 7691: 1, 7693: 1, 7695: 1, 7696: 1, 7697: 1, 7698: 1, 7701: 1, 7702: 1, 7703: 1, 7705: 1, 7707: 1, 7708: 1, 7709: 1, 7711: 1, 7713: 1, 7715: 1, 7716: 1, 7717: 1, 7718: 1, 7719: 1, 7720: 1, 7721: 1, 7722: 1, 7723: 1, 7724: 1, 7725: 1, 7728: 1, 7729: 1, 7730: 1, 7731: 1, 7732: 1, 7733: 1, 7734: 1, 7735: 1, 7736: 1, 7737: 1, 7738: 1, 7739: 1, 7740: 1, 7744: 1, 7747: 1, 7748: 1, 7750: 1, 7751: 1, 7752: 1, 7753: 1, 7754: 1, 7756: 1, 7757: 1, 7759: 1, 7761: 1, 7762: 1, 7764: 1, 7767: 1, 7769: 1, 7770: 1, 7771: 1, 7772: 1, 7774: 1, 7775: 1, 7776: 1, 7778: 1, 7779: 1, 7780: 1, 7781: 1, 7782: 1, 7783: 1, 7784: 1, 7785: 1, 7786: 1, 7787: 1, 7788: 1, 7789: 1, 7791: 1, 7793: 1, 7794: 1, 7795: 1, 7797: 1, 7798: 1, 7800: 1, 7802: 1, 7803: 1, 7804: 1, 7805: 1, 7806: 1, 7807: 1, 7808: 1, 7809: 1, 7810: 1, 7811: 1, 7812: 1, 7813: 1, 7814: 1, 7815: 1, 7816: 1, 7817: 1, 7818: 1, 7820: 1, 7821: 1, 7822: 1, 7823: 1, 7824: 1, 7825: 1, 7826: 1, 7827: 1, 7830: 1, 7831: 1, 7832: 1, 7833: 1, 7834: 1, 7835: 1, 7836: 1, 7837: 1, 7838: 1, 7839: 1, 7840: 1, 7841: 1, 7842: 1, 7843: 1, 7844: 1, 7845: 1, 7846: 1, 7850: 1, 7851: 1, 7852: 1, 7853: 1, 7854: 1, 7855: 1, 7856: 1, 7857: 1, 7859: 1, 7860: 1, 7863: 1, 7864: 1, 7865: 1, 7866: 1, 7867: 1, 7868: 1, 7870: 1, 7872: 1, 7873: 1, 7874: 1, 7876: 1, 7878: 1, 7879: 1, 7881: 1, 7882: 1, 7883: 1, 7884: 1, 7885: 1, 7886: 1, 7887: 1, 7888: 1, 7889: 1, 7891: 1, 7892: 1, 7893: 1, 7894: 1, 7895: 1, 7897: 1, 7898: 1, 7899: 1, 7901: 1, 7902: 1, 7903: 1, 7904: 1, 7905: 1, 7906: 1, 7907: 1, 7908: 1, 7909: 1, 7910: 1, 7912: 1, 7913: 1, 7914: 1, 7915: 1, 7916: 1, 7917: 1, 7918: 1, 7920: 1, 7921: 1, 7922: 1, 7923: 1, 7924: 1, 7925: 1, 7926: 1, 7927: 1, 7928: 1, 7929: 1, 7930: 1, 7931: 1, 7932: 1, 7933: 1, 7934: 1, 7935: 1, 7936: 1, 7937: 1, 7938: 1, 7939: 1, 7940: 1, 7941: 1, 7943: 1, 7944: 1, 7945: 1, 7946: 1, 7949: 1, 7950: 1, 7951: 1, 7952: 1, 7953: 1, 7955: 1, 7958: 1, 7960: 1, 7961: 1, 7962: 1, 7963: 1, 7965: 1, 7966: 1, 7967: 1, 7969: 1, 7970: 1, 7971: 1, 7972: 1, 7973: 1, 7974: 1, 7975: 1, 7976: 1, 7977: 1, 7978: 1, 7980: 1, 7981: 1, 7983: 1, 7984: 1, 7986: 1, 7987: 1, 7988: 1, 7989: 1, 7990: 1, 7992: 1, 7993: 1, 7995: 1, 7996: 1, 7998: 1, 7999: 1, 8000: 1, 8002: 1, 8003: 1, 8004: 1, 8005: 1, 8007: 1, 8008: 1, 8009: 1, 8010: 1, 8011: 1, 8012: 1, 8013: 1, 8014: 1, 8015: 1, 8016: 1, 8020: 1, 8021: 1, 8022: 1, 8023: 1, 8024: 1, 8025: 1, 8026: 1, 8027: 1, 8028: 1, 8032: 1, 8033: 1, 8034: 1, 8035: 1, 8036: 1, 8037: 1, 8039: 1, 8040: 1, 8042: 1, 8043: 1, 8045: 1, 8046: 1, 8047: 1, 8048: 1, 8049: 1, 8050: 1, 8051: 1, 8053: 1, 8055: 1, 8057: 1, 8058: 1, 8059: 1, 8060: 1, 8062: 1, 8063: 1, 8064: 1, 8065: 1, 8067: 1, 8069: 1, 8070: 1, 8071: 1, 8072: 1, 8073: 1, 8074: 1, 8075: 1, 8076: 1, 8077: 1, 8078: 1, 8079: 1, 8080: 1, 8081: 1, 8082: 1, 8084: 1, 8085: 1, 8086: 1, 8087: 1, 8088: 1, 8089: 1, 8090: 1, 8091: 1, 8092: 1, 8093: 1, 8094: 1, 8095: 1, 8097: 1, 8098: 1, 8099: 1, 8100: 1, 8101: 1, 8102: 1, 8103: 1, 8104: 1, 8105: 1, 8106: 1, 8107: 1, 8108: 1, 8110: 1, 8111: 1, 8112: 1, 8113: 1, 8114: 1, 8115: 1, 8116: 1, 8117: 1, 8119: 1, 8120: 1, 8121: 1, 8122: 1, 8123: 1, 8124: 1, 8125: 1, 8126: 1, 8127: 1, 8128: 1, 8129: 1, 8130: 1, 8131: 1, 8132: 1, 8133: 1, 8134: 1, 8137: 1, 8138: 1, 8139: 1, 8141: 1, 8143: 1, 8144: 1, 8145: 1, 8146: 1, 8147: 1, 8148: 1, 8149: 1, 8150: 1, 8151: 1, 8152: 1, 8153: 1, 8155: 1, 8156: 1, 8157: 1, 8158: 1, 8160: 1, 8161: 1, 8162: 1, 8163: 1, 8164: 1, 8165: 1, 8166: 1, 8167: 1, 8171: 1, 8172: 1, 8173: 1, 8174: 1, 8175: 1, 8176: 1, 8178: 1, 8179: 1, 8180: 1, 8181: 1, 8182: 1, 8183: 1, 8184: 1, 8185: 1, 8186: 1, 8187: 1, 8188: 1, 8190: 1, 8191: 1, 8195: 1, 8196: 1, 8197: 1, 8199: 1, 8200: 1, 8201: 1, 8202: 1, 8203: 1, 8204: 1, 8205: 1, 8206: 1, 8207: 1, 8208: 1, 8209: 1, 8210: 1, 8211: 1, 8212: 1, 8214: 1, 8217: 1, 8218: 1, 8219: 1, 8220: 1, 8221: 1, 8222: 1, 8223: 1, 8224: 1, 8226: 1, 8227: 1, 8228: 1, 8231: 1, 8232: 1, 8233: 1, 8234: 1, 8235: 1, 8236: 1, 8237: 1, 8239: 1, 8240: 1, 8241: 1, 8242: 1, 8243: 1, 8244: 1, 8245: 1, 8246: 1, 8247: 1, 8248: 1, 8250: 1, 8251: 1, 8252: 1, 8253: 1, 8254: 1, 8255: 1, 8256: 1, 8257: 1, 8258: 1, 8259: 1, 8260: 1, 8261: 1, 8262: 1, 8263: 1, 8267: 1, 8268: 1, 8269: 1, 8270: 1, 8271: 1, 8273: 1, 8274: 1, 8275: 1, 8276: 1, 8277: 1, 8278: 1, 8280: 1, 8281: 1, 8282: 1, 8283: 1, 8285: 1, 8286: 1, 8288: 1, 8290: 1, 8291: 1, 8292: 1, 8293: 1, 8294: 1, 8295: 1, 8298: 1, 8299: 1, 8300: 1, 8302: 1, 8303: 1, 8304: 1, 8305: 1, 8306: 1, 8307: 1, 8310: 1, 8312: 1, 8313: 1, 8314: 1, 8315: 1, 8316: 1, 8317: 1, 8318: 1, 8320: 1, 8321: 1, 8322: 1, 8323: 1, 8324: 1, 8325: 1, 8327: 1, 8328: 1, 8330: 1, 8331: 1, 8332: 1, 8333: 1, 8334: 1, 8335: 1, 8336: 1, 8338: 1, 8339: 1, 8340: 1, 8342: 1, 8343: 1, 8344: 1, 8345: 1, 8347: 1, 8348: 1, 8350: 1, 8351: 1, 8352: 1, 8353: 1, 8354: 1, 8355: 1, 8356: 1, 8357: 1, 8358: 1, 8359: 1, 8360: 1, 8361: 1, 8363: 1, 8365: 1, 8366: 1, 8367: 1, 8368: 1, 8369: 1, 8370: 1, 8371: 1, 8372: 1, 8373: 1, 8374: 1, 8376: 1, 8377: 1, 8379: 1, 8380: 1, 8381: 1, 8382: 1, 8383: 1, 8384: 1, 8385: 1, 8386: 1, 8387: 1, 8388: 1, 8389: 1, 8390: 1, 8391: 1, 8392: 1, 8393: 1, 8394: 1, 8395: 1, 8396: 1, 8397: 1, 8398: 1, 8400: 1, 8401: 1, 8402: 1, 8403: 1, 8404: 1, 8405: 1, 8406: 1, 8407: 1, 8408: 1, 8409: 1, 8410: 1, 8411: 1, 8412: 1, 8413: 1, 8414: 1, 8415: 1, 8416: 1, 8417: 1, 8418: 1, 8420: 1, 8421: 1, 8422: 1, 8423: 1, 8426: 1, 8428: 1, 8429: 1, 8430: 1, 8431: 1, 8432: 1, 8433: 1, 8435: 1, 8439: 1, 8440: 1, 8441: 1, 8442: 1, 8443: 1, 8444: 1, 8445: 1, 8447: 1, 8448: 1, 8449: 1, 8450: 1, 8453: 1, 8454: 1, 8455: 1, 8456: 1, 8457: 1, 8458: 1, 8459: 1, 8460: 1, 8461: 1, 8462: 1, 8463: 1, 8465: 1, 8466: 1, 8467: 1, 8468: 1, 8469: 1, 8470: 1, 8471: 1, 8472: 1, 8473: 1, 8474: 1, 8476: 1, 8477: 1, 8478: 1, 8479: 1, 8480: 1, 8481: 1, 8482: 1, 8483: 1, 8484: 1, 8485: 1, 8488: 1, 8489: 1, 8490: 1, 8491: 1, 8492: 1, 8493: 1, 8494: 1, 8496: 1, 8497: 1, 8498: 1, 8500: 1, 8501: 1, 8502: 1, 8503: 1, 8504: 1, 8505: 1, 8507: 1, 8508: 1, 8509: 1, 8510: 1, 8511: 1, 8512: 1, 8513: 1, 8514: 1, 8515: 1, 8516: 1, 8517: 1, 8518: 1, 8519: 1, 8520: 1, 8521: 1, 8522: 1, 8523: 1, 8524: 1, 8525: 1, 8526: 1, 8527: 1, 8528: 1, 8529: 1, 8530: 1, 8531: 1, 8532: 1, 8533: 1, 8534: 1, 8535: 1, 8536: 1, 8537: 1, 8538: 1, 8539: 1, 8541: 1, 8542: 1, 8543: 1, 8545: 1, 8546: 1, 8547: 1, 8548: 1, 8549: 1, 8550: 1, 8551: 1, 8552: 1, 8553: 1, 8554: 1, 8555: 1, 8556: 1, 8557: 1, 8559: 1, 8560: 1, 8561: 1, 8563: 1, 8564: 1, 8565: 1, 8566: 1, 8567: 1, 8568: 1, 8571: 1, 8572: 1, 8573: 1, 8574: 1, 8575: 1, 8576: 1, 8580: 1, 8581: 1, 8582: 1, 8583: 1, 8584: 1, 8585: 1, 8587: 1, 8588: 1, 8589: 1, 8590: 1, 8591: 1, 8592: 1, 8593: 1, 8594: 1, 8595: 1, 8596: 1, 8597: 1, 8598: 1, 8599: 1, 8600: 1, 8601: 1, 8602: 1, 8603: 1, 8604: 1, 8605: 1, 8606: 1, 8607: 1, 8608: 1, 8609: 1, 8610: 1, 8612: 1, 8613: 1, 8614: 1, 8615: 1, 8616: 1, 8617: 1, 8618: 1, 8619: 1, 8620: 1, 8621: 1, 8622: 1, 8623: 1, 8624: 1, 8625: 1, 8626: 1, 8627: 1, 8628: 1, 8629: 1, 8630: 1, 8631: 1, 8632: 1, 8634: 1, 8635: 1, 8636: 1, 8637: 1, 8638: 1, 8639: 1, 8640: 1, 8641: 1, 8643: 1, 8644: 1, 8645: 1, 8646: 1, 8647: 1, 8648: 1, 8649: 1, 8650: 1, 8651: 1, 8653: 1, 8654: 1, 8655: 1, 8656: 1, 8658: 1, 8659: 1, 8660: 1, 8661: 1, 8662: 1, 8663: 1, 8664: 1, 8665: 1, 8666: 1, 8667: 1, 8668: 1, 8669: 1, 8670: 1, 8671: 1, 8672: 1, 8675: 1, 8676: 1, 8677: 1, 8679: 1, 8680: 1, 8681: 1, 8682: 1, 8683: 1, 8684: 1, 8685: 1, 8686: 1, 8687: 1, 8688: 1, 8689: 1, 8690: 1, 8691: 1, 8692: 1, 8693: 1, 8694: 1, 8695: 1, 8696: 1, 8697: 1, 8698: 1, 8701: 1, 8702: 1, 8703: 1, 8704: 1, 8705: 1, 8707: 1, 8708: 1, 8709: 1, 8710: 1, 8713: 1, 8714: 1, 8715: 1, 8716: 1, 8717: 1, 8718: 1, 8719: 1, 8720: 1, 8721: 1, 8722: 1, 8723: 1, 8724: 1, 8725: 1, 8726: 1, 8727: 1, 8728: 1, 8729: 1, 8730: 1, 8731: 1, 8732: 1, 8733: 1, 8735: 1, 8736: 1, 8737: 1, 8738: 1, 8739: 1, 8740: 1, 8741: 1, 8742: 1, 8743: 1, 8744: 1, 8745: 1, 8746: 1, 8747: 1, 8748: 1, 8749: 1, 8752: 1, 8754: 1, 8755: 1, 8756: 1, 8757: 1, 8758: 1, 8759: 1, 8760: 1, 8761: 1, 8763: 1, 8765: 1, 8766: 1, 8767: 1, 8768: 1, 8769: 1, 8770: 1, 8771: 1, 8772: 1, 8773: 1, 8774: 1, 8775: 1, 8776: 1, 8777: 1, 8778: 1, 8779: 1, 8780: 1, 8781: 1, 8782: 1, 8783: 1, 8785: 1, 8786: 1, 8787: 1, 8788: 1, 8789: 1, 8790: 1, 8791: 1, 8792: 1, 8793: 1, 8794: 1, 8795: 1, 8796: 1, 8797: 1, 8798: 1, 8799: 1, 8801: 1, 8802: 1, 8803: 1, 8804: 1, 8805: 1, 8807: 1, 8808: 1, 8809: 1, 8810: 1, 8811: 1, 8812: 1, 8813: 1, 8814: 1, 8815: 1, 8816: 1, 8817: 1, 8818: 1, 8819: 1, 8820: 1, 8821: 1, 8822: 1, 8823: 1, 8824: 1, 8825: 1, 8827: 1, 8828: 1, 8829: 1, 8830: 1})\n",
            "\n",
            "Counts array after removing banned tokens: [0.0000e+00 0.0000e+00 6.2683e+04 ... 1.0000e+00 1.0000e+00 1.0000e+00]\n",
            "\n",
            "Shape of the counts array: (8831,)\n",
            "\n",
            "Total: 474859.0\n",
            "\n",
            "Probability: [0.00000000e+00 0.00000000e+00 1.32003395e-01 ... 2.10588827e-06\n",
            " 2.10588827e-06 2.10588827e-06]\n",
            "\n",
            "log_p: [  0.           0.          -2.02492764 ... -13.0707732  -13.0707732\n",
            " -13.0707732 ]\n",
            "\n",
            "\n",
            "Uniform entropy: 9.09\n",
            "Marginal entropy: 5.37\n",
            "Bias: [-1.00000000e+09 -1.00000000e+09 -2.02492764e+00 ... -1.30707732e+01\n",
            " -1.30707732e+01 -1.30707732e+01]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Remove the ['START'] Token from the key in train and test dataset.***"
      ],
      "metadata": {
        "id": "EbwxflVCb2r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train['key'] = [a[1:] for a in df_train['key']]\n",
        "# df_test['key'] = [a[1:] for a in df_test['key']]"
      ],
      "metadata": {
        "id": "-ErYP8CabQzC"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_tensor = tokens_to_tensor(vocab_dict=vocab_dict, vocabulary_size=vocabulary_size)\n",
        "#     tgt_ids = test_tensor.preprocessing([[1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 12, 13, 14, 15], [1, 16, 17, 18, 1, 19, 20, 15]])\n",
        "#     out = decoder(tgt_ids)"
      ],
      "metadata": {
        "id": "mpkjASHi_BIQ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, feature_extractor, output_layer, index_to_word, vocab_dict, vocabulary_size, num_layers=1,\n",
        "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    # self.word_to_index = tf.keras.layers.StringLookup(\n",
        "    #     mask_token=\"\",\n",
        "    #     vocabulary=tokenizer.get_vocabulary())\n",
        "    self.index_to_word = index_to_word\n",
        "\n",
        "    self.proj = tf.keras.layers.Dense(256)\n",
        "\n",
        "    self.tensor = tokens_to_tensor(vocab_dict=vocab_dict, vocabulary_size=vocabulary_size)\n",
        "\n",
        "    self.seq_embedding = EmbeddingLayer(\n",
        "        vocab_size=vocabulary_size,\n",
        "        depth=units,\n",
        "        max_length=max_length)\n",
        "\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)]\n",
        "\n",
        "    self.output_layer = output_layer"
      ],
      "metadata": {
        "id": "3MRywMLtwPLv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  @Captioner.add_method\n",
        "  def __call__(self, image, txt):\n",
        "    if image.shape[-1] == 3:\n",
        "      # Apply the feature-extractor, if you get an RGB image.\n",
        "      image = self.feature_extractor(image)\n",
        "\n",
        "    # Flatten the feature map\n",
        "    # image = tf.reshape(image, [image.shape[0], -1])\n",
        "\n",
        "    batch_size = tf.shape(image)[0]\n",
        "    H, W, C = image.shape[1], image.shape[2], image.shape[3]\n",
        "\n",
        "    # 1) flatten spatial dims\n",
        "    image = tf.reshape(image, (batch_size, H*W, C))\n",
        "\n",
        "    image = self.proj(image)\n",
        "\n",
        "    print(f\"Image shape: {image.shape}\")\n",
        "\n",
        "    ## convert the text into the tensor\n",
        "    txt = self.tensor.preprocessing(txt)\n",
        "\n",
        "    txt = self.seq_embedding(txt)\n",
        "\n",
        "    txt = txt.detach().numpy()\n",
        "\n",
        "    print(f\"After embedding: {txt}\")\n",
        "\n",
        "    # Look at the image\n",
        "    for dec_layer in self.decoder_layers:\n",
        "      txt = dec_layer(inputs=(image, txt))\n",
        "\n",
        "    print(f\"After completing decoder layer: {txt}\")\n",
        "    txt = self.output_layer(txt)\n",
        "\n",
        "    return txt"
      ],
      "metadata": {
        "id": "lwtJBeTvcmrg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Captioner(feature_extractor=feature_extractor, output_layer=output_layer, index_to_word=index_to_word, vocab_dict=vocab_dict, vocabulary_size=vocabulary_size,\n",
        "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ],
      "metadata": {
        "id": "ijgTYC5eczPA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[1],[2]], dtype=torch.long).detach().numpy()"
      ],
      "metadata": {
        "id": "Vj8m-FjXB5P_",
        "outputId": "ec05f928-845e-4914-fbfe-85f7975ff306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [2]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.concat([torch.tensor([1], dtype=torch.long), torch.tensor([2], dtype=torch.long)], axis=1)"
      ],
      "metadata": {
        "id": "kaIJ87ZcBGHU"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_token(last_logits, temperature=1.0, top_k=None, top_p=None):\n",
        "    # last_logits: (batch, vocab)\n",
        "    # 1) temperature\n",
        "    logits = last_logits / tf.cast(temperature, last_logits.dtype)\n",
        "\n",
        "    # 2) top_k\n",
        "    if top_k is not None and top_k > 0:\n",
        "        values, _ = tf.math.top_k(logits, k=top_k)\n",
        "        min_val   = values[:, -1, None]\n",
        "        logits    = tf.where(logits < min_val,\n",
        "                             tf.fill(tf.shape(logits), -1e9),\n",
        "                             logits)\n",
        "\n",
        "    # 3) top_p\n",
        "    if top_p is not None and top_p < 1.0:\n",
        "        sorted_logits = tf.sort(logits, direction=\"DESCENDING\", axis=-1)\n",
        "        sorted_probs  = tf.nn.softmax(sorted_logits, axis=-1)\n",
        "        cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
        "        # find first index where cum_probs > top_p:\n",
        "        cutoff_idx = tf.reduce_min(\n",
        "            tf.where(cum_probs > top_p,\n",
        "                     tf.range(tf.shape(cum_probs)[-1]),\n",
        "                     tf.fill(tf.shape(cum_probs), tf.shape(cum_probs)[-1]))\n",
        "        )\n",
        "        cutoff_val = sorted_logits[:, cutoff_idx][:, None]\n",
        "        logits = tf.where(logits < cutoff_val,\n",
        "                                 tf.fill(tf.shape(logits), -1e9),\n",
        "                                 logits)\n",
        "\n",
        "    # 4) sample\n",
        "    return tf.random.categorical(logits, num_samples=1, dtype=tf.int32)"
      ],
      "metadata": {
        "id": "HkRQqJgCxB_V"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1, top_p=0.9, top_k=30):\n",
        "  initial = [[vocab_dict['[START]']]] # (batch, sequence)\n",
        "  img_features = self.feature_extractor(image)\n",
        "  final_preds = []\n",
        "  tokens = initial # (batch, sequence)\n",
        "  print(tokens)\n",
        "  for n in range(50):\n",
        "    preds = self(img_features, tokens).numpy()  # (batch, sequence, vocab)\n",
        "\n",
        "    # print(f\"Predictions: {preds.shape}\")\n",
        "    preds = preds[:,-1, :]  #(batch, vocab)\n",
        "    final_preds.append(preds[0])\n",
        "    print(f\"Predictions: {preds}\")\n",
        "    print(f\"Predictions shape: {preds.shape}\")\n",
        "\n",
        "    if temperature==0:\n",
        "        next = np.argmax(preds, axis=-1)  # (batch, 1)\n",
        "    else:\n",
        "        logits = tf.convert_to_tensor(preds / temperature)\n",
        "        print(f\"Logits: {logits}\")\n",
        "\n",
        "        if top_k is not None and top_k > 0:\n",
        "            values, _ = tf.math.top_k(logits, k=top_k)\n",
        "            print(f\"After getting top_k: {values} and shape is: {values.shape}\")\n",
        "            min_val   = values[:, -1, None]\n",
        "            print(f\"Minimum value: {min_val}\")\n",
        "            logits = tf.where(logits < min_val,\n",
        "                                tf.fill(tf.shape(logits), -1e9),\n",
        "                                logits)\n",
        "            print(f\"After filling the top k value as it is and otherwise fill as infinity: {logits}\")\n",
        "\n",
        "        if top_p is not None and top_p < 1.0:\n",
        "            sorted_logits = tf.sort(logits, direction=\"DESCENDING\", axis=-1)\n",
        "            print(f\"Sorted logits in descending: {sorted_logits}\")\n",
        "            sorted_probs = tf.nn.softmax(sorted_logits, axis=-1)\n",
        "            print(f\"After softmax layer: {sorted_probs}\")\n",
        "            cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
        "            print(f\"Cumulative probability: {cum_probs}\")\n",
        "            # find first index where cum_probs > top_p:\n",
        "            cutoff_idx    = tf.reduce_min(\n",
        "                tf.where(cum_probs > top_p,\n",
        "                        tf.range(tf.shape(cum_probs)[-1]),\n",
        "                        tf.fill(tf.shape(cum_probs), tf.shape(cum_probs)[-1]))\n",
        "            )\n",
        "            cutoff_val    = sorted_logits[:, cutoff_idx][:, None]\n",
        "            logits        = tf.where(logits < cutoff_val,\n",
        "                                    tf.fill(tf.shape(logits), -1e9),\n",
        "                                    logits)\n",
        "\n",
        "        next = tf.random.categorical(logits, num_samples=1)\n",
        "        next = next.numpy().squeeze(-1)\n",
        "        # next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "    print(f\"Next: {next}\")\n",
        "    batch0_id = next[0]\n",
        "    max_logit_val  = preds[0, batch0_id]\n",
        "    print(f\"Next token ID: {batch0_id} with logit {max_logit_val:.3f}\")\n",
        "\n",
        "    next_tokens = [index_to_word[i] for i in next]\n",
        "    print(f\"Next tokens: {next_tokens}\")\n",
        "\n",
        "    tokens = np.concatenate([tokens, next[:, None]], axis=1)\n",
        "    # tokens = tf.concat([tokens, next], axis=1) # (batch, sequence)\n",
        "\n",
        "    if next[0] == vocab_dict['[END]']:\n",
        "      break\n",
        "\n",
        "  print(tokens)\n",
        "  words = [index_to_word[i] for i in tokens[0]]\n",
        "  print(f\"Words: {words}\")\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  print(f\"Result: {result}\\n\")\n",
        "  return result.numpy().decode(), final_preds"
      ],
      "metadata": {
        "id": "7D_6ckf8dBiJ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, final_preds = model.simple_gen(\"215214751_e913b6ff09.jpg\", temperature=1.0)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "AWe8e9z2hX6r",
        "outputId": "ea1b22ec-bf19-4559-ba72-165fbca8488a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [3035]\n",
            "Next token ID: 3035 with logit 0.000\n",
            "Next tokens: ['used']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [739]\n",
            "Next token ID: 739 with logit 0.000\n",
            "Next tokens: ['bathing']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8634]\n",
            "Next token ID: 8634 with logit 0.000\n",
            "Next tokens: ['jumpos']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1055]\n",
            "Next token ID: 1055 with logit 0.000\n",
            "Next tokens: ['hallway']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1575]\n",
            "Next token ID: 1575 with logit 0.000\n",
            "Next tokens: ['lie']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8167]\n",
            "Next token ID: 8167 with logit 0.000\n",
            "Next tokens: ['arrives']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [739]\n",
            "Next token ID: 739 with logit 0.000\n",
            "Next tokens: ['bathing']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1055]\n",
            "Next token ID: 1055 with logit 0.000\n",
            "Next tokens: ['hallway']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [6968]\n",
            "Next token ID: 6968 with logit 0.000\n",
            "Next tokens: ['folks']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [739]\n",
            "Next token ID: 739 with logit 0.000\n",
            "Next tokens: ['bathing']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8634]\n",
            "Next token ID: 8634 with logit 0.000\n",
            "Next tokens: ['jumpos']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8634]\n",
            "Next token ID: 8634 with logit 0.000\n",
            "Next tokens: ['jumpos']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [286]\n",
            "Next token ID: 286 with logit 0.000\n",
            "Next tokens: ['picks']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1055]\n",
            "Next token ID: 1055 with logit 0.000\n",
            "Next tokens: ['hallway']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [6621]\n",
            "Next token ID: 6621 with logit 0.000\n",
            "Next tokens: ['afternoon']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1575]\n",
            "Next token ID: 1575 with logit 0.000\n",
            "Next tokens: ['lie']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1769]\n",
            "Next token ID: 1769 with logit 0.000\n",
            "Next tokens: ['horizontal']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [819]\n",
            "Next token ID: 819 with logit 0.000\n",
            "Next tokens: ['audience']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [7326]\n",
            "Next token ID: 7326 with logit 0.000\n",
            "Next tokens: ['fixing']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [7326]\n",
            "Next token ID: 7326 with logit 0.000\n",
            "Next tokens: ['fixing']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1055]\n",
            "Next token ID: 1055 with logit 0.000\n",
            "Next tokens: ['hallway']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [286]\n",
            "Next token ID: 286 with logit 0.000\n",
            "Next tokens: ['picks']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8167]\n",
            "Next token ID: 8167 with logit 0.000\n",
            "Next tokens: ['arrives']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8568]\n",
            "Next token ID: 8568 with logit 0.000\n",
            "Next tokens: ['berets']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [6388]\n",
            "Next token ID: 6388 with logit 0.000\n",
            "Next tokens: ['belongs']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8036]\n",
            "Next token ID: 8036 with logit 0.000\n",
            "Next tokens: ['bring']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [3682]\n",
            "Next token ID: 3682 with logit 0.000\n",
            "Next tokens: ['wade']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [286]\n",
            "Next token ID: 286 with logit 0.000\n",
            "Next tokens: ['picks']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8167]\n",
            "Next token ID: 8167 with logit 0.000\n",
            "Next tokens: ['arrives']\n",
            "[[   1 1104 6968 6388 8129 8167 2609  286 1974 6388 1104 6968 3682 3035\n",
            "  1769 1575 6388  739 6968  739 7326 8167 3035  739 8634 1055 1575 8167\n",
            "   739 1055 6968  739 8634 8634  286 1055 6621 1575 1769  819 7326 7326\n",
            "  1055  286 8167 8568 6388 8036 3682  286 8167]]\n",
            "Words: ['[START]', 'bloe', 'folks', 'belongs', 'pall', 'arrives', 'waterskiis', 'picks', 'squeezing', 'belongs', 'bloe', 'folks', 'wade', 'used', 'horizontal', 'lie', 'belongs', 'bathing', 'folks', 'bathing', 'fixing', 'arrives', 'used', 'bathing', 'jumpos', 'hallway', 'lie', 'arrives', 'bathing', 'hallway', 'folks', 'bathing', 'jumpos', 'jumpos', 'picks', 'hallway', 'afternoon', 'lie', 'horizontal', 'audience', 'fixing', 'fixing', 'hallway', 'picks', 'arrives', 'berets', 'belongs', 'bring', 'wade', 'picks', 'arrives']\n",
            "Result: b'[START] bloe folks belongs pall arrives waterskiis picks squeezing belongs bloe folks wade used horizontal lie belongs bathing folks bathing fixing arrives used bathing jumpos hallway lie arrives bathing hallway folks bathing jumpos jumpos picks hallway afternoon lie horizontal audience fixing fixing hallway picks arrives berets belongs bring wade picks arrives'\n",
            "\n",
            "[START] bloe folks belongs pall arrives waterskiis picks squeezing belongs bloe folks wade used horizontal lie belongs bathing folks bathing fixing arrives used bathing jumpos hallway lie arrives bathing hallway folks bathing jumpos jumpos picks hallway afternoon lie horizontal audience fixing fixing hallway picks arrives berets belongs bring wade picks arrives\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model.simple_gen(\"215214751_e913b6ff09.jpg\", temperature=t)\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3xsJWaVvd4jN",
        "outputId": "4b19f93f-54b4-4c6f-facf-5e20818ceda1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[-0.5932783   1.6369275  -0.6040073  ...  0.68717843 -1.1401287\n",
            "    1.5306277 ]\n",
            "  [ 0.24819267  1.1772299   0.19795448 ...  0.68717843 -1.1400213\n",
            "    1.5306277 ]\n",
            "  [ 0.31601912  0.22078064  0.35413706 ...  0.68717843 -1.1399138\n",
            "    1.5306277 ]\n",
            "  ...\n",
            "  [-0.46970516 -0.35540795 -0.8469634  ...  0.6871637  -1.1350781\n",
            "    1.5306149 ]\n",
            "  [-1.3615329  -0.00321686  0.02878994 ...  0.68716305 -1.1349707\n",
            "    1.5306144 ]\n",
            "  [-1.5470309   0.93752     0.39498323 ...  0.6871624  -1.1348631\n",
            "    1.5306138 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.9067945   1.6584079  -0.22123197 ... -0.17466277  0.3294808\n",
            "   -0.19656646]\n",
            "  [-0.7896479   1.6076301  -0.11278975 ... -0.17539735  0.31443572\n",
            "   -0.22434144]\n",
            "  [-0.7626799   1.490232   -0.06582599 ... -0.17547143  0.32364777\n",
            "   -0.25089958]\n",
            "  ...\n",
            "  [-0.7966166   1.5324073  -0.18362541 ... -0.23677993  0.28531465\n",
            "   -0.18521596]\n",
            "  [-0.88513166  1.5651239  -0.09058343 ... -0.22177319  0.3053802\n",
            "   -0.19760922]\n",
            "  [-0.9021324   1.6590677  -0.05199967 ... -0.20742065  0.3226013\n",
            "   -0.21194339]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0184638  0.01768939 0.01210012 ... 0.02775562 0.02438767\n",
            "    0.01884801]\n",
            "   [0.01711487 0.01610611 0.0107669  ... 0.0276431  0.02538707\n",
            "    0.02029946]\n",
            "   [0.01572504 0.01480713 0.0103667  ... 0.02694425 0.02588821\n",
            "    0.02037464]\n",
            "   ...\n",
            "   [0.0119618  0.01151788 0.00660523 ... 0.02371843 0.02377235\n",
            "    0.01730073]\n",
            "   [0.01156596 0.01140601 0.00710742 ... 0.02341447 0.02412432\n",
            "    0.01713018]\n",
            "   [0.01122259 0.01143682 0.00767917 ... 0.02291325 0.02367426\n",
            "    0.01729617]]\n",
            "\n",
            "  [[0.04624897 0.04542108 0.01018334 ... 0.02380707 0.01820475\n",
            "    0.01162712]\n",
            "   [0.05004157 0.04750044 0.01028272 ... 0.02457849 0.01900565\n",
            "    0.01185291]\n",
            "   [0.05087774 0.0479013  0.0106146  ... 0.0248753  0.0192788\n",
            "    0.01153069]\n",
            "   ...\n",
            "   [0.0505133  0.05248281 0.01063249 ... 0.03286939 0.02383507\n",
            "    0.01511315]\n",
            "   [0.05208215 0.05491758 0.01173539 ... 0.02985851 0.02171278\n",
            "    0.01421717]\n",
            "   [0.05371742 0.05543495 0.0132377  ... 0.02751573 0.02035466\n",
            "    0.01417975]]]]\n",
            "\n",
            "After feed forward neural network: [[[-2.269839    0.99771434 -0.2790793  ...  0.36422578  1.4501133\n",
            "   -0.18192351]\n",
            "  [-2.1728327   0.95021135 -0.19457021 ...  0.36137223  1.4634006\n",
            "   -0.19224201]\n",
            "  [-2.150813    0.85605013 -0.14900889 ...  0.35525686  1.4821868\n",
            "   -0.19322775]\n",
            "  ...\n",
            "  [-2.110932    0.8096787  -0.3111526  ...  0.42404383  1.4315568\n",
            "   -0.26820898]\n",
            "  [-2.1904826   0.85647154 -0.22112387 ...  0.42545813  1.4276736\n",
            "   -0.27096963]\n",
            "  [-2.2103536   0.951034   -0.17997502 ...  0.42502192  1.4218546\n",
            "   -0.2779089 ]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[ 0.5850805   0.519325    0.30160972 ...  1.3361816   1.6978506\n",
            "   -0.09304351]\n",
            "  [ 0.6121593   0.5131636   0.29900467 ...  1.3479333   1.7080253\n",
            "   -0.07856567]\n",
            "  [ 0.6196961   0.507479    0.29675952 ...  1.3547353   1.7153121\n",
            "   -0.06941034]\n",
            "  ...\n",
            "  [ 0.6173163   0.50056005  0.287077   ...  1.3641914   1.6984779\n",
            "   -0.09020078]\n",
            "  [ 0.6050643   0.5020668   0.30536222 ...  1.359633    1.6953479\n",
            "   -0.09693156]\n",
            "  [ 0.602785    0.50833535  0.31449667 ...  1.3564075   1.6907464\n",
            "   -0.10242835]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01640763 0.02046591 0.01017965 ... 0.02543995 0.02869161\n",
            "    0.03593493]\n",
            "   [0.01650411 0.02050389 0.00992247 ... 0.02596974 0.02923744\n",
            "    0.03683154]\n",
            "   [0.0164571  0.0204013  0.00968942 ... 0.02629972 0.0296518\n",
            "    0.03749715]\n",
            "   ...\n",
            "   [0.01698839 0.02141188 0.01033548 ... 0.02603798 0.02967671\n",
            "    0.03855054]\n",
            "   [0.01699962 0.021398   0.01043298 ... 0.02594563 0.02950211\n",
            "    0.03825148]\n",
            "   [0.01714366 0.02152694 0.01044422 ... 0.02605663 0.02954422\n",
            "    0.03821936]]\n",
            "\n",
            "  [[0.00981621 0.00988069 0.01105271 ... 0.01072059 0.0098459\n",
            "    0.0127796 ]\n",
            "   [0.00999246 0.01008803 0.01115372 ... 0.01095568 0.01007318\n",
            "    0.0131436 ]\n",
            "   [0.00998262 0.01009468 0.01101417 ... 0.01104793 0.01018609\n",
            "    0.01335363]\n",
            "   ...\n",
            "   [0.00969819 0.0097814  0.01066972 ... 0.01095404 0.01005596\n",
            "    0.01279983]\n",
            "   [0.00947873 0.00957891 0.01053973 ... 0.01084308 0.00995347\n",
            "    0.01259151]\n",
            "   [0.0094446  0.00952888 0.01056193 ... 0.01082864 0.009943\n",
            "    0.01255605]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "Successfully enter into the output layer: [[[-0.22246878 -0.2404361   0.6190387  ...  1.233753    1.4143596\n",
            "   -0.6839474 ]\n",
            "  [-0.19915292 -0.23813987  0.6170806  ...  1.2407039   1.422049\n",
            "   -0.6645527 ]\n",
            "  [-0.19389036 -0.23711763  0.61635613 ...  1.2448714   1.4297208\n",
            "   -0.64956504]\n",
            "  ...\n",
            "  [-0.19079667 -0.24013828  0.5964172  ...  1.241504    1.4166023\n",
            "   -0.6820593 ]\n",
            "  [-0.20554873 -0.24088006  0.6108345  ...  1.2359601   1.4142689\n",
            "   -0.69154716]\n",
            "  [-0.21371718 -0.2367396   0.61918044 ...  1.2318476   1.4105332\n",
            "   -0.69716495]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[[-1.0000000e+09 -1.6096454e+00 -5.8044248e+00 ... -1.2945517e+01\n",
            "   -1.3259632e+01 -1.3150390e+01]\n",
            "  [-1.0000000e+09 -1.6074951e+00 -5.8079772e+00 ... -1.2946672e+01\n",
            "   -1.3260161e+01 -1.3150123e+01]\n",
            "  [-1.0000000e+09 -1.6066668e+00 -5.8099942e+00 ... -1.2947758e+01\n",
            "   -1.3261295e+01 -1.3149625e+01]\n",
            "  ...\n",
            "  [-1.0000000e+09 -1.6053690e+00 -5.8106036e+00 ... -1.2945221e+01\n",
            "   -1.3253238e+01 -1.3149155e+01]\n",
            "  [-1.0000000e+09 -1.6071987e+00 -5.8093810e+00 ... -1.2942951e+01\n",
            "   -1.3252829e+01 -1.3149305e+01]\n",
            "  [-1.0000000e+09 -1.6083961e+00 -5.8084054e+00 ... -1.2941546e+01\n",
            "   -1.3252921e+01 -1.3149692e+01]]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Tensor is unhashable. Instead, use tensor.ref() as the key.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-57-3834535615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"215214751_e913b6ff09.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-56-2145700339.py\u001b[0m in \u001b[0;36msimple_gen\u001b[0;34m(self, image, temperature)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#   print(tokens[0, 1:-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Words: {words}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;31m# EagerTensors are never hashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     raise TypeError(\"Tensor is unhashable. \"\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \"Instead, use tensor.ref() as the key.\")\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Tensor is unhashable. Instead, use tensor.ref() as the key."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZOvUsTK6pax",
        "outputId": "757fd4ec-f442-4462-9bf3-835cb5b6712f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = tf.constant([final_preds], dtype=tf.float32)\n",
        "labels = tf.constant([[2, 2542, 11, 47, 275, 4, 2, 433, 39, 2, 1511, 238, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n",
        "print(loss)\n",
        "mask = (labels != 0) & (loss < 1e8)\n",
        "print(f'Mask before cast: {mask}')\n",
        "mask = tf.cast(mask, loss.dtype)\n",
        "print(f'Mask after cast: {mask}')\n",
        "loss = loss*mask\n",
        "print(f\"loss before reduse sum: {loss}\")\n",
        "loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "print(f\"Loss after reduse sum: {loss}\")"
      ],
      "metadata": {
        "id": "ioy-EsRi6L6B",
        "outputId": "0260c572-d774-4e4c-c826-bc11cca06fcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[9.086033  9.086047  9.086034  9.0859995 9.086025  9.086     9.086032\n",
            "  9.086029  9.086017  9.086033  9.086037  9.085996  9.086016  9.086056\n",
            "  9.086056  9.086056  9.086056  9.086056  9.086056  9.086056  9.086055\n",
            "  9.086056  9.086056  9.086056  9.086056  9.086056  9.086056  9.086056\n",
            "  9.086056  9.086056  9.086056  9.086055  9.086056  9.086056  9.086056\n",
            "  9.086056  9.086056  9.086055  9.086056  9.086056  9.086056  9.086056\n",
            "  9.086056  9.086056  9.086056  9.086056  9.086056  9.086056  9.086055\n",
            "  9.086056 ]], shape=(1, 50), dtype=float32)\n",
            "Mask before cast: [[ True  True  True  True  True  True  True  True  True  True  True  True\n",
            "   True False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False False False False False False False False False False False\n",
            "  False False]]\n",
            "Mask after cast: [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0.]]\n",
            "loss before reduse sum: [[9.086033  9.086047  9.086034  9.0859995 9.086025  9.086     9.086032\n",
            "  9.086029  9.086017  9.086033  9.086037  9.085996  9.086016  0.\n",
            "  0.        0.        0.        0.        0.        0.        0.\n",
            "  0.        0.        0.        0.        0.        0.        0.\n",
            "  0.        0.        0.        0.        0.        0.        0.\n",
            "  0.        0.        0.        0.        0.        0.        0.\n",
            "  0.        0.        0.        0.        0.        0.        0.\n",
            "  0.       ]]\n",
            "Loss after reduse sum: 9.086023330688477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def padding_labels_tokens(labels):\n",
        "    for i in range(0, len(labels)):   ## Batch\n",
        "        if len(labels[i]) < 50:\n",
        "            for _ in range(0, 50-len(labels[i])):  ## sequence\n",
        "                labels[i].append(0)\n",
        "\n",
        "def masked_loss(labels, preds):\n",
        "    labels = padding_labels_tokens(labels)\n",
        "    labels = tf.constant(labels, dtype=tf.int32)\n",
        "    logits = tf.constant(preds, dtype=tf.float32)\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "    mask = (labels != 0) & (loss < 1e8)\n",
        "    mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "    loss = loss*mask\n",
        "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "    return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels!=0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "2h25nd6-WFRT"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    self.image = \"215214751_e913b6ff09.jpg\"\n",
        "\n",
        "  @property\n",
        "  def model(self):\n",
        "      return self._model\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result, final_preds = self.model.simple_gen(self.image, temperature=t)\n",
        "      print(result)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "CM2gPVkbWFOZ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = GenerateText()\n",
        "g.set_model(model)\n",
        "g.on_epoch_end(0)"
      ],
      "metadata": {
        "id": "ALgJz0PdWFLa",
        "outputId": "019280a9-9625-48fb-c0f2-226f6a474748",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8634]\n",
            "Next token ID: 8634 with logit 0.000\n",
            "Next tokens: ['jumpos']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [6621]\n",
            "Next token ID: 6621 with logit 0.000\n",
            "Next tokens: ['afternoon']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1104]\n",
            "Next token ID: 1104 with logit 0.000\n",
            "Next tokens: ['bloe']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1238]\n",
            "Next token ID: 1238 with logit 0.000\n",
            "Next tokens: ['colt']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8036]\n",
            "Next token ID: 8036 with logit 0.000\n",
            "Next tokens: ['bring']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [7326]\n",
            "Next token ID: 7326 with logit 0.000\n",
            "Next tokens: ['fixing']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1238]\n",
            "Next token ID: 1238 with logit 0.000\n",
            "Next tokens: ['colt']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [6388]\n",
            "Next token ID: 6388 with logit 0.000\n",
            "Next tokens: ['belongs']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [3682]\n",
            "Next token ID: 3682 with logit 0.000\n",
            "Next tokens: ['wade']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1676]\n",
            "Next token ID: 1676 with logit 0.000\n",
            "Next tokens: ['photos']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [819]\n",
            "Next token ID: 819 with logit 0.000\n",
            "Next tokens: ['audience']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1974]\n",
            "Next token ID: 1974 with logit 0.000\n",
            "Next tokens: ['squeezing']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [337]\n",
            "Next token ID: 337 with logit 0.000\n",
            "Next tokens: ['backpack']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8568]\n",
            "Next token ID: 8568 with logit 0.000\n",
            "Next tokens: ['berets']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [3035]\n",
            "Next token ID: 3035 with logit 0.000\n",
            "Next tokens: ['used']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8612]\n",
            "Next token ID: 8612 with logit 0.000\n",
            "Next tokens: ['sloppy']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8612]\n",
            "Next token ID: 8612 with logit 0.000\n",
            "Next tokens: ['sloppy']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [3682]\n",
            "Next token ID: 3682 with logit 0.000\n",
            "Next tokens: ['wade']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [337]\n",
            "Next token ID: 337 with logit 0.000\n",
            "Next tokens: ['backpack']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [819]\n",
            "Next token ID: 819 with logit 0.000\n",
            "Next tokens: ['audience']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8612]\n",
            "Next token ID: 8612 with logit 0.000\n",
            "Next tokens: ['sloppy']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [1238]\n",
            "Next token ID: 1238 with logit 0.000\n",
            "Next tokens: ['colt']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [2609]\n",
            "Next token ID: 2609 with logit 0.000\n",
            "Next tokens: ['waterskiis']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [8167]\n",
            "Next token ID: 8167 with logit 0.000\n",
            "Next tokens: ['arrives']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [7326]\n",
            "Next token ID: 7326 with logit 0.000\n",
            "Next tokens: ['fixing']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [6621]\n",
            "Next token ID: 6621 with logit 0.000\n",
            "Next tokens: ['afternoon']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [337]\n",
            "Next token ID: 337 with logit 0.000\n",
            "Next tokens: ['backpack']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [4115]\n",
            "Next token ID: 4115 with logit 0.000\n",
            "Next tokens: ['tongues']\n",
            "Image shape: (1, 49, 256)\n",
            "After embedding: [[[ 0.23457581  0.16689068 -0.6497135  ...  0.9586575  -0.7274895\n",
            "   -1.9796209 ]\n",
            "  [ 1.0760467  -0.29280698  0.15224826 ...  0.9586575  -0.727382\n",
            "   -1.9796209 ]\n",
            "  [ 1.1438732  -1.2492561   0.30843085 ...  0.9586575  -0.72727454\n",
            "   -1.9796209 ]\n",
            "  ...\n",
            "  [ 0.35814893 -1.8254447  -0.89266956 ...  0.9586428  -0.7224388\n",
            "   -1.9796337 ]\n",
            "  [-0.5336788  -1.4732537  -0.01691628 ...  0.9586421  -0.7223314\n",
            "   -1.9796343 ]\n",
            "  [-0.7191768  -0.5325168   0.34927702 ...  0.95864147 -0.72222394\n",
            "   -1.9796348 ]]]\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-1.6132935   0.41172507  0.00197956 ...  0.10603718  0.7729844\n",
            "   -0.31639686]\n",
            "  [-1.5002338   0.39165118  0.02945775 ...  0.0672683   0.7492199\n",
            "   -0.32344082]\n",
            "  [-1.3741384   0.368281   -0.03103326 ... -0.00643772  0.6956977\n",
            "   -0.3049171 ]\n",
            "  ...\n",
            "  [-1.1472367   0.4770942  -0.5051184  ... -0.01874429  0.5605203\n",
            "   -0.41173992]\n",
            "  [-1.2577574   0.47124696 -0.3904196  ...  0.01212898  0.58900726\n",
            "   -0.44633606]\n",
            "  [-1.210854    0.56346184 -0.3593973  ... -0.02316878  0.5591758\n",
            "   -0.39814952]]]\n",
            "\n",
            "After completing last attention score: [[[[0.0042891  0.00456003 0.00771201 ... 0.00738758 0.00853587\n",
            "    0.00545333]\n",
            "   [0.00487866 0.00523345 0.00815731 ... 0.00750957 0.00859981\n",
            "    0.00550624]\n",
            "   [0.00546418 0.00592595 0.00960582 ... 0.00770333 0.00884602\n",
            "    0.0055621 ]\n",
            "   ...\n",
            "   [0.01024644 0.01060153 0.01653171 ... 0.01027927 0.01085005\n",
            "    0.00773244]\n",
            "   [0.01147621 0.01198466 0.01931181 ... 0.01000263 0.01064417\n",
            "    0.0078669 ]\n",
            "   [0.0111219  0.01172676 0.02039731 ... 0.00903559 0.00975981\n",
            "    0.00700805]]\n",
            "\n",
            "  [[0.01016296 0.00939943 0.0519362  ... 0.0064977  0.00695903\n",
            "    0.00627427]\n",
            "   [0.01110827 0.01018484 0.05778691 ... 0.00633841 0.00665682\n",
            "    0.00637907]\n",
            "   [0.01313845 0.01202371 0.06736144 ... 0.00655754 0.0067624\n",
            "    0.00699247]\n",
            "   ...\n",
            "   [0.01329702 0.01239372 0.03125777 ... 0.0096506  0.01122754\n",
            "    0.01054772]\n",
            "   [0.01115724 0.01078102 0.0313775  ... 0.00859007 0.01005645\n",
            "    0.0086415 ]\n",
            "   [0.01008681 0.01008035 0.03147598 ... 0.00867844 0.00994488\n",
            "    0.0078154 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-1.7045465   1.5536441   0.18867272 ... -1.0156554   0.9045066\n",
            "    0.08264951]\n",
            "  [-1.5834424   1.5310742   0.20498866 ... -1.0606613   0.89384365\n",
            "    0.08796075]\n",
            "  [-1.4452424   1.504722    0.12882371 ... -1.1514812   0.8376521\n",
            "    0.13773039]\n",
            "  ...\n",
            "  [-1.3222282   1.5691811  -0.40965265 ... -1.1973006   0.54240143\n",
            "    0.12869056]\n",
            "  [-1.4150727   1.5695066  -0.31161082 ... -1.1497053   0.5742272\n",
            "    0.08710885]\n",
            "  [-1.3337219   1.6490024  -0.31904334 ... -1.1967477   0.5299227\n",
            "    0.15239687]]]\n",
            "\n",
            "Sucessfully entering into the decoder layer\n",
            "After completing self attention layer: (1, 50, 256)\n",
            "\n",
            "After completing cross attention layer: [[[-0.68919796 -0.91864544  2.0503206  ... -1.4345435   0.43647224\n",
            "   -2.1194074 ]\n",
            "  [-0.6715086  -0.9226443   2.051566   ... -1.4433508   0.43487978\n",
            "   -2.1189108 ]\n",
            "  [-0.6489206  -0.9310736   2.042386   ... -1.4594929   0.42591077\n",
            "   -2.1127214 ]\n",
            "  ...\n",
            "  [-0.61705476 -0.9536194   1.9573348  ... -1.465019    0.4075875\n",
            "   -2.1010256 ]\n",
            "  [-0.63190866 -0.9498781   1.9677831  ... -1.4593258   0.41296503\n",
            "   -2.104969  ]\n",
            "  [-0.61644787 -0.93955356  1.9675703  ... -1.4687443   0.403465\n",
            "   -2.0985556 ]]]\n",
            "\n",
            "After completing last attention score: [[[[0.01129963 0.0122133  0.02244751 ... 0.02397642 0.02287938\n",
            "    0.02136663]\n",
            "   [0.01128664 0.01215474 0.02271294 ... 0.02398775 0.02287421\n",
            "    0.02111476]\n",
            "   [0.01153851 0.01232742 0.02342265 ... 0.02422742 0.02304935\n",
            "    0.02081095]\n",
            "   ...\n",
            "   [0.01392678 0.01415851 0.02926718 ... 0.02436988 0.02313784\n",
            "    0.02212787]\n",
            "   [0.0137849  0.0140126  0.02878466 ... 0.02418546 0.02302941\n",
            "    0.0224055 ]\n",
            "   [0.01411257 0.01432562 0.02861554 ... 0.02428985 0.0230332\n",
            "    0.02211553]]\n",
            "\n",
            "  [[0.01860057 0.01913681 0.01351762 ... 0.02984843 0.02895263\n",
            "    0.0161274 ]\n",
            "   [0.01854925 0.01909649 0.01331238 ... 0.02994132 0.02904156\n",
            "    0.01635115]\n",
            "   [0.0186279  0.01920371 0.01319323 ... 0.02983093 0.02887015\n",
            "    0.01676741]\n",
            "   ...\n",
            "   [0.02007924 0.02060959 0.01371066 ... 0.02871926 0.0263673\n",
            "    0.01507026]\n",
            "   [0.02002818 0.02058946 0.01374019 ... 0.02914778 0.02657223\n",
            "    0.01485475]\n",
            "   [0.02004971 0.0205098  0.01342439 ... 0.02898352 0.02636665\n",
            "    0.0152855 ]]]]\n",
            "\n",
            "After feed forward neural network: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "\n",
            "After completing decoder layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "Successfully enter into the output layer: [[[-0.7597674  -0.2231643   1.9487793  ... -1.1106708  -0.323035\n",
            "   -2.1152873 ]\n",
            "  [-0.7440365  -0.22865024  1.9473172  ... -1.1174315  -0.32530645\n",
            "   -2.1125863 ]\n",
            "  [-0.7258175  -0.23544972  1.9358296  ... -1.1326829  -0.3347131\n",
            "   -2.1029918 ]\n",
            "  ...\n",
            "  [-0.7146154  -0.23394752  1.8518412  ... -1.133284   -0.37036213\n",
            "   -2.060091  ]\n",
            "  [-0.73135734 -0.23158976  1.8602761  ... -1.122239   -0.36564416\n",
            "   -2.0637698 ]\n",
            "  [-0.7219125  -0.22416025  1.8589286  ... -1.1307348  -0.3726763\n",
            "   -2.0599725 ]]]\n",
            "input shape of the output layer: (1, 50, 256)\n",
            "Predictions: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "Predictions shape: (1, 8831)\n",
            "Logits: [[8.1744169e-05 1.2818482e-04 1.0405887e-04 ... 9.4543226e-05\n",
            "  1.1412699e-04 8.1980827e-05]]\n",
            "After getting top_k: [[0.00033323 0.0002525  0.00024954 0.00023378 0.00023252 0.00023036\n",
            "  0.00022929 0.0002257  0.00022479 0.00022272 0.00021805 0.00021633\n",
            "  0.00021623 0.00021613 0.00021575 0.00021535 0.00021478 0.00021473\n",
            "  0.00021459 0.00021453 0.00021446 0.00021418 0.0002134  0.00021268\n",
            "  0.00021035 0.0002101  0.00020951 0.0002081  0.00020808 0.00020747]] and shape is: (1, 30)\n",
            "Minimum value: [[0.00020747]]\n",
            "After filling the top k value as it is and otherwise fill as infinity: [[-1.e+09 -1.e+09 -1.e+09 ... -1.e+09 -1.e+09 -1.e+09]]\n",
            "Sorted logits in descending: [[ 3.3323152e-04  2.5250277e-04  2.4954413e-04 ... -1.0000000e+09\n",
            "  -1.0000000e+09 -1.0000000e+09]]\n",
            "After softmax layer: [[0.033337   0.0333343  0.03333421 ... 0.         0.         0.        ]]\n",
            "Cumulative probability: [[0.033337   0.0666713  0.10000551 ... 0.99999994 0.99999994 0.99999994]]\n",
            "Next: [4115]\n",
            "Next token ID: 4115 with logit 0.000\n",
            "Next tokens: ['tongues']\n",
            "[[   1 1104 7326 1238 3851 8612 1974 8612 8568 1676 8612 7326 1055  819\n",
            "  3035 3851 6388 6388 1676 8167  337 1055 8634 6621 1104 1238 8036 7326\n",
            "  1238 6388 3682 1676  819 1974  337 8568 3035 8612 8612 3682  337  819\n",
            "  8612 1238 2609 8167 7326 6621  337 4115 4115]]\n",
            "Words: ['[START]', 'bloe', 'fixing', 'colt', 'accross', 'sloppy', 'squeezing', 'sloppy', 'berets', 'photos', 'sloppy', 'fixing', 'hallway', 'audience', 'used', 'accross', 'belongs', 'belongs', 'photos', 'arrives', 'backpack', 'hallway', 'jumpos', 'afternoon', 'bloe', 'colt', 'bring', 'fixing', 'colt', 'belongs', 'wade', 'photos', 'audience', 'squeezing', 'backpack', 'berets', 'used', 'sloppy', 'sloppy', 'wade', 'backpack', 'audience', 'sloppy', 'colt', 'waterskiis', 'arrives', 'fixing', 'afternoon', 'backpack', 'tongues', 'tongues']\n",
            "Result: b'[START] bloe fixing colt accross sloppy squeezing sloppy berets photos sloppy fixing hallway audience used accross belongs belongs photos arrives backpack hallway jumpos afternoon bloe colt bring fixing colt belongs wade photos audience squeezing backpack berets used sloppy sloppy wade backpack audience sloppy colt waterskiis arrives fixing afternoon backpack tongues tongues'\n",
            "\n",
            "[START] bloe fixing colt accross sloppy squeezing sloppy berets photos sloppy fixing hallway audience used accross belongs belongs photos arrives backpack hallway jumpos afternoon bloe colt bring fixing colt belongs wade photos audience squeezing backpack berets used sloppy sloppy wade backpack audience sloppy colt waterskiis arrives fixing afternoon backpack tongues tongues\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    GenerateText(),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True)]"
      ],
      "metadata": {
        "id": "AwbDdQd2WFIz"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])"
      ],
      "metadata": {
        "id": "tla6VJCTWFFv"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Splitting the dataset***"
      ],
      "metadata": {
        "id": "Dw17pSk4_Y1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_txt(imgs, tokens):\n",
        "#   tokens = tokenizer(txts)\n",
        "#   tokens = torch.tensor(tokens)\n",
        "  input_tokens = tokens[..., :-1]\n",
        "  label_tokens = tokens[..., 1:]\n",
        "  return (imgs, input_tokens), label_tokens"
      ],
      "metadata": {
        "id": "x3XZ7apqDG2b"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepare_txt([101, 102, 103, 104, 105])"
      ],
      "metadata": {
        "id": "pD-cg4HOGEhL",
        "outputId": "467d7baa-e1fe-49bf-b793-5bc0179c7bc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([101, 102, 103, 104])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([101, 102, 103, 104]), tensor([102, 103, 104, 105]))"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# paths = df['image_path'].values           # e.g. array of strings\n",
        "# caps  = df['caption'].tolist()            # list of lists of ints\n",
        "\n",
        "# # Turn the list-of-lists into a RaggedTensor\n",
        "# caps_rt = tf.ragged.constant(caps, dtype=tf.int32)\n",
        "\n",
        "# # Now from_tensor_slices will accept it\n",
        "# ds = tf.data.Dataset.from_tensor_slices((paths, caps_rt))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "all_caps = np.stack(df['caption'].tolist(), axis=0)  # shape (N, L)\n",
        "all_paths = df['image_path'].values                 # shape (N,)\n",
        "\n",
        "raw_ds = tf.data.Dataset.from_tensor_slices((all_paths, all_caps))\n",
        "# now raw_ds yields (string, [L]-int32) pairs and you can .shuffle/.batch\n"
      ],
      "metadata": {
        "id": "ZlD_y2ZfYR17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(ds, batch_size=32, shuffle_buffer=1000):\n",
        "  all_caps = np.stack(df['caption'].tolist(), axis=0)  # shape (N, L)\n",
        "  all_paths = df['image'].values                 # shape (N,)\n",
        "\n",
        "  ds = tf.data.Dataset.from_tensor_slices((all_paths, all_caps))\n",
        "\n",
        "  print(ds)\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .shuffle(10000)\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds\n",
        "          .unbatch()\n",
        "          .shuffle(shuffle_buffer)\n",
        "          .batch(batch_size)\n",
        "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "          .map(to_tensor, tf.data.AUTOTUNE)\n",
        "          )"
      ],
      "metadata": {
        "id": "RK4xE7c6DNpv"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df.sample(frac=0.8, random_state=1)\n",
        "df_test = df.drop(df_train.index)"
      ],
      "metadata": {
        "id": "-FXf1BNG_mbH"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.reset_index(drop=True)\n",
        "df_test = df_test.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Z8ymd2OA_rm8"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJZMpkYfZv--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = prepare_dataset(df_train)\n",
        "train_ds.element_spec"
      ],
      "metadata": {
        "id": "w3C0QifyDyVb",
        "outputId": "c7e25509-5526-4ec1-cd60-71694d2b04e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(50,), dtype=tf.int64, name=None))>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "in user code:\n\n    File \"/tmp/ipython-input-2032970670.py\", line 17, in to_tensor  *\n        return (images, in_tok.to_tensor()), out_tok.to_tensor()\n\n    AttributeError: 'SymbolicTensor' object has no attribute 'to_tensor'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2017906600.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2032970670.py\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(ds, batch_size, shuffle_buffer)\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m           \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m           )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       )\n\u001b[0;32m---> 57\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1254\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1224\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Note: wrapper_helper will apply autograph based on context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filepsbuvyyq.py\u001b[0m in \u001b[0;36mtf__to_tensor\u001b[0;34m(inputs, labels)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_enable_numpy_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m       \"\"\")\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/tmp/ipython-input-2032970670.py\", line 17, in to_tensor  *\n        return (images, in_tok.to_tensor()), out_tok.to_tensor()\n\n    AttributeError: 'SymbolicTensor' object has no attribute 'to_tensor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = prepare_dataset(df_test)\n",
        "test_ds.element_spec"
      ],
      "metadata": {
        "id": "GtKzbGoZWRaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN = 0\n",
        "df['caption'] = df['caption'].apply(\n",
        "    lambda seq: seq + [PAD_TOKEN] * (50 - len(seq))\n",
        ")"
      ],
      "metadata": {
        "id": "SBbg7tH6aiWG"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNkdc6QpaeN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "58YJbdOBS_AC",
        "outputId": "8fb3f7ed-ff54-4cc5-c5c0-d331e38c6491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 caption  \\\n",
              "0      [1, 146, 77, 2508, 187, 4, 25, 1425, 46, 2, 20...   \n",
              "1      [1, 2, 30, 271, 2212, 2, 65, 460, 4, 2, 180, 1...   \n",
              "2      [1, 25, 404, 899, 18, 103, 25, 1738, 6, 7, 148...   \n",
              "3      [1, 2, 3, 31, 13, 1010, 52, 46, 38, 39, 16, 17...   \n",
              "4      [1, 2, 1021, 661, 4, 2, 1009, 1404, 518, 26, 1...   \n",
              "...                                                  ...   \n",
              "32129  [1, 48, 49, 242, 4, 2, 491, 16, 17, 0, 0, 0, 0...   \n",
              "32130  [1, 2, 1009, 1232, 1163, 452, 2, 1174, 46, 2, ...   \n",
              "32131  [1, 2, 30, 359, 2, 319, 16, 17, 0, 0, 0, 0, 0,...   \n",
              "32132  [1, 48, 199, 33, 1435, 76, 11, 25, 811, 955, 1...   \n",
              "32133  [1, 2, 189, 4, 2, 29, 291, 31, 501, 164, 292, ...   \n",
              "\n",
              "                           image  \n",
              "0      3727740053_3baa94ffcb.jpg  \n",
              "1      1600208439_e94527b80f.jpg  \n",
              "2      3670918456_68631d362a.jpg  \n",
              "3      3057862887_135c61816a.jpg  \n",
              "4      3516299821_8f0375d221.jpg  \n",
              "...                          ...  \n",
              "32129  2599444370_9e40103027.jpg  \n",
              "32130  3631344685_ed0f3e091b.jpg  \n",
              "32131   436393371_822ee70952.jpg  \n",
              "32132  3359636318_39267812a0.jpg  \n",
              "32133  3328397409_092de2bd32.jpg  \n",
              "\n",
              "[32134 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4341ac5e-b8e2-4e8b-888b-1c88648002d6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1, 146, 77, 2508, 187, 4, 25, 1425, 46, 2, 20...</td>\n",
              "      <td>3727740053_3baa94ffcb.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1, 2, 30, 271, 2212, 2, 65, 460, 4, 2, 180, 1...</td>\n",
              "      <td>1600208439_e94527b80f.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1, 25, 404, 899, 18, 103, 25, 1738, 6, 7, 148...</td>\n",
              "      <td>3670918456_68631d362a.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1, 2, 3, 31, 13, 1010, 52, 46, 38, 39, 16, 17...</td>\n",
              "      <td>3057862887_135c61816a.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1, 2, 1021, 661, 4, 2, 1009, 1404, 518, 26, 1...</td>\n",
              "      <td>3516299821_8f0375d221.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32129</th>\n",
              "      <td>[1, 48, 49, 242, 4, 2, 491, 16, 17, 0, 0, 0, 0...</td>\n",
              "      <td>2599444370_9e40103027.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32130</th>\n",
              "      <td>[1, 2, 1009, 1232, 1163, 452, 2, 1174, 46, 2, ...</td>\n",
              "      <td>3631344685_ed0f3e091b.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32131</th>\n",
              "      <td>[1, 2, 30, 359, 2, 319, 16, 17, 0, 0, 0, 0, 0,...</td>\n",
              "      <td>436393371_822ee70952.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32132</th>\n",
              "      <td>[1, 48, 199, 33, 1435, 76, 11, 25, 811, 955, 1...</td>\n",
              "      <td>3359636318_39267812a0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32133</th>\n",
              "      <td>[1, 2, 189, 4, 2, 29, 291, 31, 501, 164, 292, ...</td>\n",
              "      <td>3328397409_092de2bd32.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32134 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4341ac5e-b8e2-4e8b-888b-1c88648002d6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4341ac5e-b8e2-4e8b-888b-1c88648002d6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4341ac5e-b8e2-4e8b-888b-1c88648002d6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8b8d1e99-5052-4ce4-8a38-d18cb57f916f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8b8d1e99-5052-4ce4-8a38-d18cb57f916f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8b8d1e99-5052-4ce4-8a38-d18cb57f916f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_4b7119a9-f6f4-4796-8d3b-c998fd89df1c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4b7119a9-f6f4-4796-8d3b-c998fd89df1c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 32134,\n  \"fields\": [\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8087,\n        \"samples\": [\n          \"3282634762_2650d0088a.jpg\",\n          \"2394857899_76bfdf720b.jpg\",\n          \"2652155912_8ba5426790.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_ds = tf.data.Dataset.from_tensor_slices((df_train['image'], df_train['caption']))\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((df_test['image'], df_test['caption']))"
      ],
      "metadata": {
        "id": "qAxjJMgj_RB2"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    df_train,\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=df_test,\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "hkNdFcIyWFCr",
        "outputId": "167f9148-fb99-4736-aeff-ee502563ddce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid dtype: object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1168928455.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optree/ops.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_is_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid dtype: object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "f3MSEbCWWWSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "pzln-aByWWPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "msD56LvhWWM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKMyTnHeWWJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RuCD5Fh7WWHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D0jzhM93WWD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class temp_convert_output_to_tensor:\n",
        "    def __init__(self, vocab_dict, vocabulary_size, max_len=50):\n",
        "      self.vocab_dict = vocab_dict\n",
        "      self.vocabulary_size = vocabulary_size\n",
        "      self.max_len = max_len\n",
        "      self.preprocess_text = preprocess_text()\n",
        "\n",
        "    def preprocessing(self, caption_batch):\n",
        "      all_captions_tokenIDs = []\n",
        "\n",
        "      for s_tokenID in caption_batch:\n",
        "    #     ## split the caption\n",
        "    #     s_tokenID = []\n",
        "\n",
        "    #     ## find the token ID for the each token from vocab_dict.\n",
        "    #     for word in caption:\n",
        "    #       if word in self.vocab_dict:\n",
        "    #         s_tokenID.append(self.vocab_dict[word])\n",
        "\n",
        "    #       else:\n",
        "    #         s_tokenID.append(0)\n",
        "\n",
        "        ## padding the length\n",
        "        for _ in range(0, self.max_len-len(s_tokenID)):\n",
        "          s_tokenID.append(0)\n",
        "\n",
        "        all_captions_tokenIDs.append(s_tokenID)\n",
        "\n",
        "      tgt_ids = torch.tensor(all_captions_tokenIDs, dtype=torch.long)\n",
        "      return tgt_ids"
      ],
      "metadata": {
        "id": "wR-dgjGtr8Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oY2nFaoHd_y_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1c5f6b-f8de-40ac-bc61-c1af12037b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4055, -0.4849, -0.2018,  ...,  0.6381, -1.3429,  1.1430],\n",
            "         [ 0.6739,  1.2321,  0.5574,  ...,  1.3237,  1.2483,  0.9730],\n",
            "         [ 2.5945, -0.8670,  0.5196,  ...,  1.4123, -0.1160,  1.1735],\n",
            "         ...,\n",
            "         [ 0.4736, -1.4479,  2.0100,  ...,  2.1350,  2.7324,  0.1760],\n",
            "         [-0.4182, -1.0957,  1.7640,  ...,  2.1350,  2.7325,  0.1760],\n",
            "         [-0.6037, -0.1550,  0.8888,  ...,  2.1350,  2.7326,  0.1760]],\n",
            "\n",
            "        [[-0.4055, -0.4849, -0.2018,  ...,  0.6381, -1.3429,  1.1430],\n",
            "         [ 0.6016,  0.6899,  0.1445,  ...,  0.5265,  1.0655, -0.1205],\n",
            "         [ 1.1073,  1.7684,  1.8862,  ...,  2.5377,  0.4864,  0.0394],\n",
            "         ...,\n",
            "         [ 0.4736, -1.4479,  2.0100,  ...,  2.1350,  2.7324,  0.1760],\n",
            "         [-0.4182, -1.0957,  1.7640,  ...,  2.1350,  2.7325,  0.1760],\n",
            "         [-0.6037, -0.1550,  0.8888,  ...,  2.1350,  2.7326,  0.1760]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([2, 50, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['image']==\"215214751_e913b6ff09.jpg\"]"
      ],
      "metadata": {
        "id": "SQI-aY537_aA",
        "outputId": "5120101a-54ac-4594-d26e-0d80d71b809d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                caption  \\\n",
              "5501  [2, 433, 36, 47, 3432, 57, 3433, 39, 24, 238, 16]   \n",
              "5502  [2, 2542, 11, 47, 275, 4, 2, 433, 39, 2, 1511,...   \n",
              "5503  [47, 275, 4, 2, 276, 433, 39, 2, 429, 2946, 2,...   \n",
              "5504  [47, 198, 32, 4, 2, 306, 39, 2, 429, 4, 1511, ...   \n",
              "5505  [47, 198, 3436, 4, 2, 65, 433, 39, 2, 2617, 23...   \n",
              "\n",
              "                         image  \n",
              "5501  215214751_e913b6ff09.jpg  \n",
              "5502  215214751_e913b6ff09.jpg  \n",
              "5503  215214751_e913b6ff09.jpg  \n",
              "5504  215214751_e913b6ff09.jpg  \n",
              "5505  215214751_e913b6ff09.jpg  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b905f062-4f87-4dc2-ba4a-06a11dc47184\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5501</th>\n",
              "      <td>[2, 433, 36, 47, 3432, 57, 3433, 39, 24, 238, 16]</td>\n",
              "      <td>215214751_e913b6ff09.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5502</th>\n",
              "      <td>[2, 2542, 11, 47, 275, 4, 2, 433, 39, 2, 1511,...</td>\n",
              "      <td>215214751_e913b6ff09.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5503</th>\n",
              "      <td>[47, 275, 4, 2, 276, 433, 39, 2, 429, 2946, 2,...</td>\n",
              "      <td>215214751_e913b6ff09.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5504</th>\n",
              "      <td>[47, 198, 32, 4, 2, 306, 39, 2, 429, 4, 1511, ...</td>\n",
              "      <td>215214751_e913b6ff09.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5505</th>\n",
              "      <td>[47, 198, 3436, 4, 2, 65, 433, 39, 2, 2617, 23...</td>\n",
              "      <td>215214751_e913b6ff09.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b905f062-4f87-4dc2-ba4a-06a11dc47184')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b905f062-4f87-4dc2-ba4a-06a11dc47184 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b905f062-4f87-4dc2-ba4a-06a11dc47184');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-784c30d8-028b-41bd-b7b8-9c491ca038a5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-784c30d8-028b-41bd-b7b8-9c491ca038a5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-784c30d8-028b-41bd-b7b8-9c491ca038a5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df[df['image']==\\\"215214751_e913b6ff09\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"215214751_e913b6ff09.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['caption'][5502]"
      ],
      "metadata": {
        "id": "ttQNkysv8RKX",
        "outputId": "879d617f-b235-4abf-f99f-475802709eef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 2542, 11, 47, 275, 4, 2, 433, 39, 2, 1511, 238, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "_l9NLDI1BXsd",
        "outputId": "1be9d882-3b94-444c-d826-41b1f5c58916"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 caption  \\\n",
              "0      [2, 3, 4, 2, 5, 6, 7, 8, 9, 2, 10, 11, 12, 4, ...   \n",
              "1                         [2, 17, 18, 19, 2, 20, 21, 16]   \n",
              "2                      [2, 22, 17, 8, 19, 2, 20, 23, 16]   \n",
              "3                 [2, 22, 17, 8, 24, 12, 25, 26, 23, 16]   \n",
              "4         [2, 22, 17, 4, 2, 5, 6, 18, 19, 2, 20, 27, 16]   \n",
              "...                                                  ...   \n",
              "40163  [235, 1259, 39, 2, 4026, 4, 562, 36, 1203, 306...   \n",
              "40164        [2, 76, 4, 2, 5, 162, 113, 2, 165, 203, 16]   \n",
              "40165            [2, 76, 7, 165, 8, 489, 4, 24, 228, 16]   \n",
              "40166  [2, 188, 4, 2, 111, 162, 8, 9, 2, 165, 203, 55...   \n",
              "40167                  [2, 165, 289, 4, 2, 111, 162, 16]   \n",
              "\n",
              "                           image  \n",
              "0      1000268201_693b08cb0e.jpg  \n",
              "1      1000268201_693b08cb0e.jpg  \n",
              "2      1000268201_693b08cb0e.jpg  \n",
              "3      1000268201_693b08cb0e.jpg  \n",
              "4      1000268201_693b08cb0e.jpg  \n",
              "...                          ...  \n",
              "40163   997338199_7343367d7f.jpg  \n",
              "40164   997722733_0cb5439472.jpg  \n",
              "40165   997722733_0cb5439472.jpg  \n",
              "40166   997722733_0cb5439472.jpg  \n",
              "40167   997722733_0cb5439472.jpg  \n",
              "\n",
              "[40168 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf7eb5ab-a443-4896-a5dc-df93f3c51fb0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[2, 3, 4, 2, 5, 6, 7, 8, 9, 2, 10, 11, 12, 4, ...</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[2, 17, 18, 19, 2, 20, 21, 16]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[2, 22, 17, 8, 19, 2, 20, 23, 16]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[2, 22, 17, 8, 24, 12, 25, 26, 23, 16]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[2, 22, 17, 4, 2, 5, 6, 18, 19, 2, 20, 27, 16]</td>\n",
              "      <td>1000268201_693b08cb0e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40163</th>\n",
              "      <td>[235, 1259, 39, 2, 4026, 4, 562, 36, 1203, 306...</td>\n",
              "      <td>997338199_7343367d7f.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40164</th>\n",
              "      <td>[2, 76, 4, 2, 5, 162, 113, 2, 165, 203, 16]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40165</th>\n",
              "      <td>[2, 76, 7, 165, 8, 489, 4, 24, 228, 16]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40166</th>\n",
              "      <td>[2, 188, 4, 2, 111, 162, 8, 9, 2, 165, 203, 55...</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40167</th>\n",
              "      <td>[2, 165, 289, 4, 2, 111, 162, 16]</td>\n",
              "      <td>997722733_0cb5439472.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>40168 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf7eb5ab-a443-4896-a5dc-df93f3c51fb0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cf7eb5ab-a443-4896-a5dc-df93f3c51fb0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cf7eb5ab-a443-4896-a5dc-df93f3c51fb0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0a5276e7-8a63-4a32-b2d8-8fb2fcdb28d9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a5276e7-8a63-4a32-b2d8-8fb2fcdb28d9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0a5276e7-8a63-4a32-b2d8-8fb2fcdb28d9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ebfe837f-9d63-4457-89e6-98a3afd45b7e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ebfe837f-9d63-4457-89e6-98a3afd45b7e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 40168,\n  \"fields\": [\n    {\n      \"column\": \"caption\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8091,\n        \"samples\": [\n          \"3115174046_9e96b9ce47.jpg\",\n          \"3107592525_0bcd00777e.jpg\",\n          \"2426724282_237bca30b5.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['caption'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdAud5cVAAIz",
        "outputId": "01374a61-80b1-45a5-ca1f-1e1d10a689bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10, 11, 3, 12, 13, 14, 15]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9mYqxECBVzR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}